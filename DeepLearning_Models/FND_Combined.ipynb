{"cells":[{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"KQVP5KQC8hyh","executionInfo":{"status":"ok","timestamp":1681237972642,"user_tz":-330,"elapsed":25931,"user":{"displayName":"Ramacharan Reddy Kasireddy","userId":"17080325997413282440"}},"outputId":"72cda539-4689-478d-9454-234ca32c74e5"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"code","source":["!pip install Keras-Preprocessing"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"6Dd1ZD6RFl82","executionInfo":{"status":"ok","timestamp":1681237975228,"user_tz":-330,"elapsed":2590,"user":{"displayName":"Ramacharan Reddy Kasireddy","userId":"17080325997413282440"}},"outputId":"e42cd088-c3ef-420a-bffe-cf06515c9704"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting Keras-Preprocessing\n","  Downloading Keras_Preprocessing-1.1.2-py2.py3-none-any.whl (42 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.6/42.6 kB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.9/dist-packages (from Keras-Preprocessing) (1.16.0)\n","Requirement already satisfied: numpy>=1.9.1 in /usr/local/lib/python3.9/dist-packages (from Keras-Preprocessing) (1.22.4)\n","Installing collected packages: Keras-Preprocessing\n","Successfully installed Keras-Preprocessing-1.1.2\n"]}]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"OVUe9CXD5b8g","executionInfo":{"status":"ok","timestamp":1681237979544,"user_tz":-330,"elapsed":4320,"user":{"displayName":"Ramacharan Reddy Kasireddy","userId":"17080325997413282440"}},"outputId":"cab8a355-bea3-472e-f4e2-3bc6d043b1b4"},"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Requirement already satisfied: tensorflow in /usr/local/lib/python3.9/dist-packages (2.12.0)\n","Requirement already satisfied: tensorboard<2.13,>=2.12 in /usr/local/lib/python3.9/dist-packages (from tensorflow) (2.12.1)\n","Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.9/dist-packages (from tensorflow) (1.53.0)\n","Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.9/dist-packages (from tensorflow) (4.5.0)\n","Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.9/dist-packages (from tensorflow) (3.3.0)\n","Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.9/dist-packages (from tensorflow) (1.4.0)\n","Requirement already satisfied: numpy<1.24,>=1.22 in /usr/local/lib/python3.9/dist-packages (from tensorflow) (1.22.4)\n","Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.9/dist-packages (from tensorflow) (3.8.0)\n","Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.9/dist-packages (from tensorflow) (1.6.3)\n","Requirement already satisfied: jax>=0.3.15 in /usr/local/lib/python3.9/dist-packages (from tensorflow) (0.4.7)\n","Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.9/dist-packages (from tensorflow) (0.2.0)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.9/dist-packages (from tensorflow) (23.0)\n","Requirement already satisfied: flatbuffers>=2.0 in /usr/local/lib/python3.9/dist-packages (from tensorflow) (23.3.3)\n","Requirement already satisfied: tensorflow-estimator<2.13,>=2.12.0 in /usr/local/lib/python3.9/dist-packages (from tensorflow) (2.12.0)\n","Requirement already satisfied: gast<=0.4.0,>=0.2.1 in /usr/local/lib/python3.9/dist-packages (from tensorflow) (0.4.0)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.9/dist-packages (from tensorflow) (67.6.1)\n","Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.9/dist-packages (from tensorflow) (1.16.0)\n","Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.9/dist-packages (from tensorflow) (0.32.0)\n","Requirement already satisfied: keras<2.13,>=2.12.0 in /usr/local/lib/python3.9/dist-packages (from tensorflow) (2.12.0)\n","Requirement already satisfied: wrapt<1.15,>=1.11.0 in /usr/local/lib/python3.9/dist-packages (from tensorflow) (1.14.1)\n","Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.9/dist-packages (from tensorflow) (2.2.0)\n","Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.9/dist-packages (from tensorflow) (16.0.0)\n","Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /usr/local/lib/python3.9/dist-packages (from tensorflow) (3.20.3)\n","Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.9/dist-packages (from astunparse>=1.6.0->tensorflow) (0.40.0)\n","Requirement already satisfied: scipy>=1.7 in /usr/local/lib/python3.9/dist-packages (from jax>=0.3.15->tensorflow) (1.10.1)\n","Requirement already satisfied: ml-dtypes>=0.0.3 in /usr/local/lib/python3.9/dist-packages (from jax>=0.3.15->tensorflow) (0.0.4)\n","Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.9/dist-packages (from tensorboard<2.13,>=2.12->tensorflow) (0.7.0)\n","Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.9/dist-packages (from tensorboard<2.13,>=2.12->tensorflow) (2.2.3)\n","Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.9/dist-packages (from tensorboard<2.13,>=2.12->tensorflow) (3.4.3)\n","Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.9/dist-packages (from tensorboard<2.13,>=2.12->tensorflow) (2.27.1)\n","Requirement already satisfied: google-auth-oauthlib<1.1,>=0.5 in /usr/local/lib/python3.9/dist-packages (from tensorboard<2.13,>=2.12->tensorflow) (1.0.0)\n","Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.9/dist-packages (from tensorboard<2.13,>=2.12->tensorflow) (2.17.2)\n","Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.9/dist-packages (from tensorboard<2.13,>=2.12->tensorflow) (1.8.1)\n","Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.9/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow) (0.2.8)\n","Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.9/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow) (4.9)\n","Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.9/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow) (5.3.0)\n","Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.9/dist-packages (from google-auth-oauthlib<1.1,>=0.5->tensorboard<2.13,>=2.12->tensorflow) (1.3.1)\n","Requirement already satisfied: importlib-metadata>=4.4 in /usr/local/lib/python3.9/dist-packages (from markdown>=2.6.8->tensorboard<2.13,>=2.12->tensorflow) (6.2.0)\n","Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.9/dist-packages (from requests<3,>=2.21.0->tensorboard<2.13,>=2.12->tensorflow) (2.0.12)\n","Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests<3,>=2.21.0->tensorboard<2.13,>=2.12->tensorflow) (1.26.15)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.9/dist-packages (from requests<3,>=2.21.0->tensorboard<2.13,>=2.12->tensorflow) (3.4)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.9/dist-packages (from requests<3,>=2.21.0->tensorboard<2.13,>=2.12->tensorflow) (2022.12.7)\n","Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.9/dist-packages (from werkzeug>=1.0.1->tensorboard<2.13,>=2.12->tensorflow) (2.1.2)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.9/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<2.13,>=2.12->tensorflow) (3.15.0)\n","Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.9/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow) (0.4.8)\n","Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.9/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<1.1,>=0.5->tensorboard<2.13,>=2.12->tensorflow) (3.2.2)\n"]}],"source":["!pip install tensorflow"]},{"cell_type":"markdown","source":["# Linear SVM"],"metadata":{"id":"7FvCmVVbLFmc"}},{"cell_type":"code","source":["# Importing required libraries\n","import pandas as pd\n","from sklearn.feature_extraction.text import TfidfVectorizer\n","from sklearn.model_selection import cross_val_score, cross_val_predict\n","from sklearn.metrics import accuracy_score, recall_score, precision_score, f1_score\n","\n","# Reading data into a pandas DataFrame\n","data = pd.read_csv (r'/content/drive/MyDrive/New/Clean/Combined_clean.csv')"],"metadata":{"id":"fC-AWFlbLDd_"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from sklearn.feature_extraction.text import TfidfVectorizer\n","tfidf_v = TfidfVectorizer(max_features=5000, ngram_range=(1,3))\n","X = tfidf_v.fit_transform(data['statement'].values.astype('U'))\n","y = data['label'].values"],"metadata":{"id":"ipMclUd7LJvA"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import joblib\n","from sklearn.svm import LinearSVC\n","\n","# Creating a Linear SVM classifier\n","svm = LinearSVC()\n","\n","# Performing five-fold cross-validation\n","scores = cross_val_score(svm, X, y, cv=5)\n","\n","# Printing the cross-validation scores\n","print(\"Cross-validation scores:\", scores)\n","\n","# Computing and printing accuracy, recall, precision, and F1 score\n","y_pred = cross_val_predict(svm, X, y, cv=5)\n","accuracy = accuracy_score(y, y_pred)\n","recall = recall_score(y, y_pred)\n","precision = precision_score(y, y_pred)\n","f1 = f1_score(y, y_pred)\n","print(\"Accuracy:\", accuracy)\n","print(\"Recall:\", recall)\n","print(\"Precision:\", precision)\n","print(\"F1 score:\", f1)\n","\n","# Save the trained model to a file\n","joblib.dump(svm, '/content/drive/MyDrive/Colab Notebooks/weights/Combined/linearsvc_combined.pkl')\n"," "],"metadata":{"id":"_zPKKaEwLLB0","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1680545481851,"user_tz":-330,"elapsed":13648,"user":{"displayName":"Ramacharan Reddy Kasireddy","userId":"17080325997413282440"}},"outputId":"4df1699b-16f6-4338-ce33-3b4ec61b3894"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Cross-validation scores: [0.94795068 0.94815062 0.86711096 0.73600373 0.62709944]\n","Accuracy: 0.8252669181451022\n","Recall: 0.8284745966048828\n","Precision: 0.7889355382286579\n","F1 score: 0.8082217833369907\n"]},{"output_type":"execute_result","data":{"text/plain":["['/content/drive/MyDrive/Colab Notebooks/weights/Combined/linearsvc_combined.pkl']"]},"metadata":{},"execution_count":6}]},{"cell_type":"markdown","source":["# LSTM"],"metadata":{"id":"LaFNMN9FHTdr"}},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"irUXCM2q5b8g","executionInfo":{"status":"ok","timestamp":1680545753190,"user_tz":-330,"elapsed":254476,"user":{"displayName":"Ramacharan Reddy Kasireddy","userId":"17080325997413282440"}},"outputId":"d3a1b32b-abac-49ef-feeb-19c6f8d7b58f"},"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 1/10\n","1876/1876 - 88s - loss: 0.3487 - accuracy: 0.8509 - val_loss: 0.3039 - val_accuracy: 0.8751 - 88s/epoch - 47ms/step\n","Epoch 2/10\n","1876/1876 - 26s - loss: 0.2689 - accuracy: 0.8895 - val_loss: 0.2972 - val_accuracy: 0.8754 - 26s/epoch - 14ms/step\n","Epoch 3/10\n","1876/1876 - 15s - loss: 0.2361 - accuracy: 0.9039 - val_loss: 0.3049 - val_accuracy: 0.8805 - 15s/epoch - 8ms/step\n","Epoch 4/10\n","1876/1876 - 15s - loss: 0.2088 - accuracy: 0.9149 - val_loss: 0.3154 - val_accuracy: 0.8800 - 15s/epoch - 8ms/step\n","Epoch 5/10\n","1876/1876 - 14s - loss: 0.1856 - accuracy: 0.9244 - val_loss: 0.3299 - val_accuracy: 0.8684 - 14s/epoch - 7ms/step\n","Epoch 6/10\n","1876/1876 - 15s - loss: 0.1639 - accuracy: 0.9337 - val_loss: 0.3710 - val_accuracy: 0.8740 - 15s/epoch - 8ms/step\n","Epoch 7/10\n","1876/1876 - 13s - loss: 0.1463 - accuracy: 0.9407 - val_loss: 0.4059 - val_accuracy: 0.8586 - 13s/epoch - 7ms/step\n","Epoch 8/10\n","1876/1876 - 13s - loss: 0.1276 - accuracy: 0.9486 - val_loss: 0.3996 - val_accuracy: 0.8634 - 13s/epoch - 7ms/step\n","Epoch 9/10\n","1876/1876 - 14s - loss: 0.1136 - accuracy: 0.9549 - val_loss: 0.4427 - val_accuracy: 0.8697 - 14s/epoch - 7ms/step\n","Epoch 10/10\n","1876/1876 - 14s - loss: 0.0996 - accuracy: 0.9604 - val_loss: 0.5137 - val_accuracy: 0.8706 - 14s/epoch - 7ms/step\n","469/469 [==============================] - 2s 3ms/step\n","Accuracy: 0.8706431189603465\n","Precision: 0.8819885641677255\n","Recall: 0.8225448081765664\n","F1-Score: 0.8512301678546792\n"]}],"source":["import numpy as np\n","import pandas as pd\n","import tensorflow as tf\n","from sklearn.model_selection import train_test_split\n","from tensorflow.keras.preprocessing.text import Tokenizer\n","from tensorflow.keras.preprocessing.sequence import pad_sequences\n","from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n","\n","# Load the data from CSV file\n","data = pd.read_csv('/content/drive/MyDrive/New/Clean/Combined_clean.csv')\n","\n","# Split data into statements and labels\n","statements = data['statement']\n","labels = data['label']\n","\n","# Tokenize statements\n","tokenizer = Tokenizer()\n","tokenizer.fit_on_texts(statements)\n","word_index = tokenizer.word_index\n","sequences = tokenizer.texts_to_sequences(statements)\n","\n","# Pad sequences\n","max_len = 100\n","padded_sequences = pad_sequences(sequences, maxlen=max_len)\n","\n","# Split data into training and testing sets\n","X_train, X_test, y_train, y_test = train_test_split(padded_sequences, labels, test_size=0.2)\n","\n","# Build the model\n","model = tf.keras.Sequential([\n","    tf.keras.layers.Embedding(input_dim=5000, output_dim=32, input_length=100),\n","    tf.keras.layers.LSTM(32),\n","    tf.keras.layers.Dense(1, activation='sigmoid')\n","])\n","\n","# Compile the model\n","model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n","\n","# Train the model\n","history = model.fit(X_train, y_train, epochs=10, validation_data=(X_test, y_test), verbose=2)\n","\n","# Evaluate the model\n","y_pred = model.predict(X_test)\n","y_pred = (y_pred > 0.5).astype(int)\n","\n","# Calculate the evaluation metrics\n","accuracy = accuracy_score(y_test, y_pred)\n","precision = precision_score(y_test, y_pred)\n","recall = recall_score(y_test, y_pred)\n","f1 = f1_score(y_test, y_pred)\n","\n","print(\"Accuracy:\", accuracy)\n","print(\"Precision:\", precision)\n","print(\"Recall:\", recall)\n","print(\"F1-Score:\", f1)\n","\n","# Save the model\n","model.save('/content/drive/MyDrive/Colab Notebooks/weights/Combined/lstm_combined.h5')\n","\n","# Load the saved model\n","# loaded_model = tf.keras.models.load_model('lstm_fn.h5')\n"]},{"cell_type":"markdown","source":["# BI LSTM\n"],"metadata":{"id":"SB2BWfsxFOPo"}},{"cell_type":"code","source":["import numpy as np\n","import pandas as pd\n","from keras.preprocessing.text import Tokenizer\n","from keras_preprocessing.sequence import pad_sequences\n","from keras.models import Sequential\n","from keras.layers import Dense, Embedding, LSTM, Bidirectional\n","from sklearn.model_selection import train_test_split\n","from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n","\n","# Load the data from CSV file\n","data = pd.read_csv('/content/drive/MyDrive/New/Clean/Combined_clean.csv')\n","\n","# Split data into statements and labels\n","statements = data['statement']\n","labels = data['label']\n","\n","# Tokenize statements\n","tokenizer = Tokenizer()\n","tokenizer.fit_on_texts(statements)\n","word_index = tokenizer.word_index\n","print('Found %s unique tokens.' % len(word_index))\n","sequences = tokenizer.texts_to_sequences(statements)\n","\n","# Pad sequences\n","max_len = 100\n","padded_sequences = pad_sequences(sequences, maxlen=max_len)\n","\n","# Split data into training and testing sets\n","x_train, x_test, y_train, y_test = train_test_split(padded_sequences, labels, test_size=0.2)\n","\n","# Define model architecture\n","model = Sequential()\n","model.add(Embedding(len(word_index) + 1, 128, input_length=max_len))\n","model.add(Bidirectional(LSTM(64, return_sequences=True)))\n","model.add(Bidirectional(LSTM(32)))\n","model.add(Dense(1, activation='sigmoid'))\n","\n","# Compile model\n","model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n","print(model.summary())\n","\n","# Train model\n","model.fit(x_train, y_train, epochs=10, batch_size=32, validation_data=(x_test, y_test))\n","\n","# Evaluate the model\n","y_pred = model.predict(x_test)\n","y_pred = (y_pred > 0.5).astype(int)\n","\n","# Calculate the evaluation metrics\n","accuracy = accuracy_score(y_test, y_pred)\n","precision = precision_score(y_test, y_pred)\n","recall = recall_score(y_test, y_pred)\n","f1 = f1_score(y_test, y_pred)\n","\n","print(\"Accuracy:\", accuracy)\n","print(\"Precision:\", precision)\n","print(\"Recall:\", recall)\n","print(\"F1-Score:\", f1)\n","\n","# Save the model\n","model.save('/content/drive/MyDrive/Colab Notebooks/weights/Combined/bilstm_combined.h5')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"FLFOmQOGC9BG","executionInfo":{"status":"ok","timestamp":1680542184859,"user_tz":-330,"elapsed":471761,"user":{"displayName":"Ramacharan Reddy Kasireddy","userId":"17080325997413282440"}},"outputId":"b75f2398-117a-46bc-8f3b-02b09bf7846f"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Found 34902 unique tokens.\n","Model: \"sequential_1\"\n","_________________________________________________________________\n"," Layer (type)                Output Shape              Param #   \n","=================================================================\n"," embedding_1 (Embedding)     (None, 100, 128)          4467584   \n","                                                                 \n"," bidirectional (Bidirectiona  (None, 100, 128)         98816     \n"," l)                                                              \n","                                                                 \n"," bidirectional_1 (Bidirectio  (None, 64)               41216     \n"," nal)                                                            \n","                                                                 \n"," dense_1 (Dense)             (None, 1)                 65        \n","                                                                 \n","=================================================================\n","Total params: 4,607,681\n","Trainable params: 4,607,681\n","Non-trainable params: 0\n","_________________________________________________________________\n","None\n","Epoch 1/10\n","1876/1876 [==============================] - 118s 58ms/step - loss: 0.3422 - accuracy: 0.8562 - val_loss: 0.2981 - val_accuracy: 0.8770\n","Epoch 2/10\n","1876/1876 [==============================] - 43s 23ms/step - loss: 0.2336 - accuracy: 0.9067 - val_loss: 0.2984 - val_accuracy: 0.8824\n","Epoch 3/10\n","1876/1876 [==============================] - 39s 21ms/step - loss: 0.1755 - accuracy: 0.9293 - val_loss: 0.3327 - val_accuracy: 0.8759\n","Epoch 4/10\n","1876/1876 [==============================] - 39s 21ms/step - loss: 0.1334 - accuracy: 0.9452 - val_loss: 0.3750 - val_accuracy: 0.8796\n","Epoch 5/10\n","1876/1876 [==============================] - 37s 20ms/step - loss: 0.1001 - accuracy: 0.9591 - val_loss: 0.4372 - val_accuracy: 0.8739\n","Epoch 6/10\n","1876/1876 [==============================] - 36s 19ms/step - loss: 0.0795 - accuracy: 0.9685 - val_loss: 0.4753 - val_accuracy: 0.8747\n","Epoch 7/10\n","1876/1876 [==============================] - 35s 19ms/step - loss: 0.0616 - accuracy: 0.9751 - val_loss: 0.4945 - val_accuracy: 0.8751\n","Epoch 8/10\n","1876/1876 [==============================] - 34s 18ms/step - loss: 0.0473 - accuracy: 0.9815 - val_loss: 0.5623 - val_accuracy: 0.8735\n","Epoch 9/10\n","1876/1876 [==============================] - 34s 18ms/step - loss: 0.0394 - accuracy: 0.9845 - val_loss: 0.6311 - val_accuracy: 0.8692\n","Epoch 10/10\n","1876/1876 [==============================] - 35s 19ms/step - loss: 0.0319 - accuracy: 0.9870 - val_loss: 0.6969 - val_accuracy: 0.8599\n","469/469 [==============================] - 4s 6ms/step\n","Accuracy: 0.8599133622125958\n","Precision: 0.8254371122391427\n","Recall: 0.8713902947305746\n","F1-Score: 0.8477914554670529\n"]}]},{"cell_type":"markdown","source":["# HYBRID\n"],"metadata":{"id":"U01_EgTCJRRD"}},{"cell_type":"code","source":["import numpy as np\n","import pandas as pd\n","from keras.preprocessing.text import Tokenizer\n","from keras_preprocessing.sequence import pad_sequences\n","from keras.layers import Embedding, Conv1D, MaxPooling1D, Bidirectional, LSTM, Dense, Dropout\n","from keras.models import Sequential\n","from sklearn.model_selection import train_test_split\n","from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n","\n","df = pd.read_csv (r'/content/drive/MyDrive/New/Clean/Combined_clean.csv')\n","\n","# Preprocessing\n","MAX_NB_WORDS = 50000  # Maximum number of words to be used in the tokenizer\n","MAX_SEQUENCE_LENGTH = 300  # Maximum length of each news statement\n","EMBEDDING_DIM = 100  # Dimension of the word embedding\n","VALIDATION_SPLIT = 0.2  # Percentage of data to use for validation\n","BATCH_SIZE = 128\n","EPOCHS = 10\n","\n","# Tokenize the news statements\n","tokenizer = Tokenizer(num_words=MAX_NB_WORDS)\n","tokenizer.fit_on_texts(df['statement'])\n","word_index = tokenizer.word_index\n","print('Found %s unique tokens.' % len(word_index))\n","\n","# Convert the news statements to sequences of integers\n","sequences = tokenizer.texts_to_sequences(df['statement'])\n","\n","# Pad the sequences\n","Max_Len = max([len(x) for x in df['statement']])\n","padded = pad_sequences(sequences, maxlen=Max_Len)\n","\n","print('Shape of data tensor:', padded.shape)\n","\n","# Define the maximum number of words to keep in the vocabulary\n","MAX_NUM_WORDS = 20000\n","\n","# Convert the labels to one-hot encoding\n","y = df['label']\n","print('Shape of label tensor:', y.shape)\n","\n","# Split data into training and testing sets\n","X_train, X_test, y_train, y_test = train_test_split(padded, df['label'].values, test_size=0.2)\n","\n","# Build the model\n","model = Sequential()\n","model.add(Embedding(input_dim=MAX_NB_WORDS, output_dim=EMBEDDING_DIM, input_length=Max_Len))\n","model.add(Conv1D(128, 5, activation='relu'))\n","model.add(Dropout(0.3))\n","model.add(MaxPooling1D(2))\n","model.add(Conv1D(128, 5, activation='relu'))\n","model.add(MaxPooling1D(2))\n","model.add(Bidirectional(LSTM(32)))\n","model.add(Dense(1, activation='sigmoid'))\n","\n","# Compile the model\n","model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n","print(model.summary())\n","\n","# Train the model\n","model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=10, batch_size=128)\n","\n","# Evaluate the model\n","y_pred = model.predict(X_test)\n","y_pred = (y_pred > 0.5).astype(int)\n","\n","# Calculate the evaluation metrics\n","accuracy = accuracy_score(y_test, y_pred)\n","precision = precision_score(y_test, y_pred)\n","recall = recall_score(y_test, y_pred)\n","f1 = f1_score(y_test, y_pred)\n","\n","print(\"Accuracy:\", accuracy)\n","print(\"Precision:\", precision)\n","print(\"Recall:\", recall)\n","print(\"F1-Score:\", f1)\n","\n","# Save the model\n","model.save('/content/drive/MyDrive/Colab Notebooks/weights/Combined/hybrid_combined.h5')\n","\n","# Load the saved model\n","# loaded_model = tf.keras.models.load_model('lstm_fn.h5')\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"GtD8_BqxGMvD","outputId":"d7f87f28-b08c-4f99-f4c2-3b07685f1466","executionInfo":{"status":"ok","timestamp":1681240735616,"user_tz":-330,"elapsed":2741883,"user":{"displayName":"Ramacharan Reddy Kasireddy","userId":"17080325997413282440"}}},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Found 34902 unique tokens.\n","Shape of data tensor: (75023, 29569)\n","Shape of label tensor: (75023,)\n","Model: \"sequential\"\n","_________________________________________________________________\n"," Layer (type)                Output Shape              Param #   \n","=================================================================\n"," embedding (Embedding)       (None, 29569, 100)        5000000   \n","                                                                 \n"," conv1d (Conv1D)             (None, 29565, 128)        64128     \n","                                                                 \n"," dropout (Dropout)           (None, 29565, 128)        0         \n","                                                                 \n"," max_pooling1d (MaxPooling1D  (None, 14782, 128)       0         \n"," )                                                               \n","                                                                 \n"," conv1d_1 (Conv1D)           (None, 14778, 128)        82048     \n","                                                                 \n"," max_pooling1d_1 (MaxPooling  (None, 7389, 128)        0         \n"," 1D)                                                             \n","                                                                 \n"," bidirectional (Bidirectiona  (None, 64)               41216     \n"," l)                                                              \n","                                                                 \n"," dense (Dense)               (None, 1)                 65        \n","                                                                 \n","=================================================================\n","Total params: 5,187,457\n","Trainable params: 5,187,457\n","Non-trainable params: 0\n","_________________________________________________________________\n","None\n","Epoch 1/10\n","469/469 [==============================] - 316s 632ms/step - loss: 0.3581 - accuracy: 0.8425 - val_loss: 0.2930 - val_accuracy: 0.8812\n","Epoch 2/10\n","469/469 [==============================] - 269s 573ms/step - loss: 0.2204 - accuracy: 0.9146 - val_loss: 0.2500 - val_accuracy: 0.9012\n","Epoch 3/10\n","469/469 [==============================] - 265s 564ms/step - loss: 0.1634 - accuracy: 0.9381 - val_loss: 0.2635 - val_accuracy: 0.8998\n","Epoch 4/10\n","469/469 [==============================] - 260s 554ms/step - loss: 0.1192 - accuracy: 0.9541 - val_loss: 0.2862 - val_accuracy: 0.9010\n","Epoch 5/10\n","469/469 [==============================] - 256s 546ms/step - loss: 0.0877 - accuracy: 0.9659 - val_loss: 0.3351 - val_accuracy: 0.8961\n","Epoch 6/10\n","469/469 [==============================] - 256s 547ms/step - loss: 0.0713 - accuracy: 0.9722 - val_loss: 0.3592 - val_accuracy: 0.8920\n","Epoch 7/10\n","469/469 [==============================] - 255s 543ms/step - loss: 0.0581 - accuracy: 0.9774 - val_loss: 0.3956 - val_accuracy: 0.8940\n","Epoch 8/10\n","469/469 [==============================] - 254s 542ms/step - loss: 0.0505 - accuracy: 0.9801 - val_loss: 0.4139 - val_accuracy: 0.8947\n","Epoch 9/10\n","469/469 [==============================] - 253s 540ms/step - loss: 0.0385 - accuracy: 0.9851 - val_loss: 0.4611 - val_accuracy: 0.8866\n","Epoch 10/10\n","469/469 [==============================] - 253s 540ms/step - loss: 0.0333 - accuracy: 0.9866 - val_loss: 0.4770 - val_accuracy: 0.8910\n","469/469 [==============================] - 66s 137ms/step\n","Accuracy: 0.8910363212262579\n","Precision: 0.8911447758863033\n","Recall: 0.8588199879590608\n","F1-Score: 0.8746838353644515\n"]}]},{"cell_type":"markdown","source":["# GLOVE HYBRID\n"],"metadata":{"id":"uYnJCHPuE-pd"}},{"cell_type":"code","source":["import numpy as np\n","import pandas as pd\n","from keras.preprocessing.text import Tokenizer\n","from keras_preprocessing.sequence import pad_sequences\n","from keras.layers import Embedding, Conv1D, MaxPooling1D, Bidirectional, LSTM, Dense, Dropout\n","from keras.models import Sequential\n","from sklearn.model_selection import train_test_split\n","from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n","\n","df = pd.read_csv (r'/content/drive/MyDrive/New/Clean/Combined_clean.csv')\n","\n","MAX_NB_WORDS = 50000  # Maximum number of words to be used in the tokenizer\n","VALIDATION_SPLIT = 0.2\n","\n","# Tokenize the news statements\n","tokenizer = Tokenizer(num_words=MAX_NB_WORDS)\n","tokenizer.fit_on_texts(df['statement'])\n","word_index = tokenizer.word_index\n","print('Found %s unique tokens.' % len(word_index))\n","\n","# Convert the news statements to sequences of integers\n","sequences = tokenizer.texts_to_sequences(df['statement'])\n","\n","# Pad the sequences\n","Max_Len = max([len(x) for x in df['statement']])\n","padded = pad_sequences(sequences, maxlen=Max_Len)\n","\n","print('Shape of data tensor:', padded.shape)\n","\n","# Define the maximum number of words to keep in the vocabulary\n","MAX_NUM_WORDS = 20000\n","\n","# Convert the labels to one-hot encoding\n","y = df['label']\n","print('Shape of label tensor:', y.shape)\n","\n","# Split data into training and testing sets\n","X_train, X_test, y_train, y_test = train_test_split(padded, df['label'].values, test_size=0.2)\n","\n","\n","# Load the GloVe embeddings\n","embeddings_index = {}\n","f = open('/content/drive/MyDrive/Colab Notebooks/weights/glove.6B.300d.txt', encoding='utf8')\n","for line in f:\n","    values = line.split()\n","    word = values[0]\n","    coefs = np.asarray(values[1:], dtype='float32')\n","    embeddings_index[word] = coefs\n","f.close()\n","\n","# Create an embedding matrix\n","embedding_dim = 300\n","word_index = tokenizer.word_index\n","num_words = min(MAX_NUM_WORDS, len(word_index) + 1)\n","embedding_matrix = np.zeros((num_words, embedding_dim))\n","for word, i in word_index.items():\n","    if i >= MAX_NUM_WORDS:\n","        continue\n","    embedding_vector = embeddings_index.get(word)\n","    if embedding_vector is not None:\n","        embedding_matrix[i] = embedding_vector\n","\n","# Build the model\n","model = Sequential()\n","model.add(Embedding(input_dim=num_words, output_dim=embedding_dim, input_length=Max_Len, weights=[embedding_matrix], trainable=False))\n","model.add(Conv1D(128, 5, activation='relu'))\n","model.add(Dropout(0.3))\n","model.add(MaxPooling1D(2))\n","model.add(Conv1D(128, 5, activation='relu'))\n","model.add(MaxPooling1D(2))\n","model.add(Bidirectional(LSTM(32)))\n","\n","model.add(Dense(1, activation='sigmoid'))\n","\n","# Compile the model\n","model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n","print(model.summary())\n","\n","# Train the model\n","history = model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=10, batch_size=128)\n","\n","# Evaluate the model\n","y_pred = model.predict(X_test)\n","y_pred = (y_pred > 0.5).astype(int)\n","\n","# Calculate the evaluation metrics\n","accuracy = accuracy_score(y_test, y_pred)\n","precision = precision_score(y_test, y_pred)\n","recall = recall_score(y_test, y_pred)\n","f1 = f1_score(y_test, y_pred)\n","\n","print(\"Accuracy:\", accuracy)\n","print(\"Precision:\", precision)\n","print(\"Recall:\", recall)\n","print(\"F1-Score:\", f1)\n","\n","# Save the model\n","model.save('/content/drive/MyDrive/Colab Notebooks/weights/Combined/glove6b300_combined.h5')\n","\n","# Load the saved model\n","# loaded_model = tf.keras.models.load_model('lstm_fn.h5')\n"],"metadata":{"id":"iV4m-WrSfZ-0","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1681243331151,"user_tz":-330,"elapsed":2595540,"user":{"displayName":"Ramacharan Reddy Kasireddy","userId":"17080325997413282440"}},"outputId":"cbeeb17f-1299-4068-e337-47638f919ac5"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Found 34902 unique tokens.\n","Shape of data tensor: (75023, 29569)\n","Shape of label tensor: (75023,)\n","Model: \"sequential_1\"\n","_________________________________________________________________\n"," Layer (type)                Output Shape              Param #   \n","=================================================================\n"," embedding_1 (Embedding)     (None, 29569, 300)        6000000   \n","                                                                 \n"," conv1d_2 (Conv1D)           (None, 29565, 128)        192128    \n","                                                                 \n"," dropout_1 (Dropout)         (None, 29565, 128)        0         \n","                                                                 \n"," max_pooling1d_2 (MaxPooling  (None, 14782, 128)       0         \n"," 1D)                                                             \n","                                                                 \n"," conv1d_3 (Conv1D)           (None, 14778, 128)        82048     \n","                                                                 \n"," max_pooling1d_3 (MaxPooling  (None, 7389, 128)        0         \n"," 1D)                                                             \n","                                                                 \n"," bidirectional_1 (Bidirectio  (None, 64)               41216     \n"," nal)                                                            \n","                                                                 \n"," dense_1 (Dense)             (None, 1)                 65        \n","                                                                 \n","=================================================================\n","Total params: 6,315,457\n","Trainable params: 315,457\n","Non-trainable params: 6,000,000\n","_________________________________________________________________\n","None\n","Epoch 1/10\n","469/469 [==============================] - 266s 548ms/step - loss: 0.4135 - accuracy: 0.8139 - val_loss: 0.2965 - val_accuracy: 0.8814\n","Epoch 2/10\n","469/469 [==============================] - 246s 524ms/step - loss: 0.2709 - accuracy: 0.8909 - val_loss: 0.2708 - val_accuracy: 0.8892\n","Epoch 3/10\n","469/469 [==============================] - 246s 524ms/step - loss: 0.2120 - accuracy: 0.9162 - val_loss: 0.2394 - val_accuracy: 0.9031\n","Epoch 4/10\n","469/469 [==============================] - 246s 524ms/step - loss: 0.1763 - accuracy: 0.9303 - val_loss: 0.2404 - val_accuracy: 0.9038\n","Epoch 5/10\n","469/469 [==============================] - 246s 524ms/step - loss: 0.1509 - accuracy: 0.9401 - val_loss: 0.2481 - val_accuracy: 0.9046\n","Epoch 6/10\n","469/469 [==============================] - 246s 524ms/step - loss: 0.1280 - accuracy: 0.9488 - val_loss: 0.2556 - val_accuracy: 0.9080\n","Epoch 7/10\n","469/469 [==============================] - 246s 524ms/step - loss: 0.1141 - accuracy: 0.9549 - val_loss: 0.2620 - val_accuracy: 0.9060\n","Epoch 8/10\n","469/469 [==============================] - 246s 524ms/step - loss: 0.1023 - accuracy: 0.9588 - val_loss: 0.2788 - val_accuracy: 0.9048\n","Epoch 9/10\n","469/469 [==============================] - 246s 524ms/step - loss: 0.0927 - accuracy: 0.9640 - val_loss: 0.3073 - val_accuracy: 0.8999\n","Epoch 10/10\n","469/469 [==============================] - 246s 524ms/step - loss: 0.0825 - accuracy: 0.9674 - val_loss: 0.3033 - val_accuracy: 0.9064\n","469/469 [==============================] - 68s 142ms/step\n","Accuracy: 0.9064311896034655\n","Precision: 0.9347357959251283\n","Recall: 0.8481887870133774\n","F1-Score: 0.8893617021276595\n"]}]},{"cell_type":"code","source":["import numpy as np\n","import pandas as pd\n","from keras.preprocessing.text import Tokenizer\n","from keras_preprocessing.sequence import pad_sequences\n","from keras.layers import Embedding, Conv1D, MaxPooling1D, Bidirectional, LSTM, Dense, Dropout\n","from keras.models import Sequential\n","from sklearn.model_selection import train_test_split\n","from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n","\n","df = pd.read_csv (r'/content/drive/MyDrive/New/Clean/Combined_clean.csv')\n","\n","MAX_NB_WORDS = 50000  # Maximum number of words to be used in the tokenizer\n","VALIDATION_SPLIT = 0.2\n","\n","# Tokenize the news statements\n","tokenizer = Tokenizer(num_words=MAX_NB_WORDS)\n","tokenizer.fit_on_texts(df['statement'])\n","word_index = tokenizer.word_index\n","print('Found %s unique tokens.' % len(word_index))\n","\n","# Convert the news statements to sequences of integers\n","sequences = tokenizer.texts_to_sequences(df['statement'])\n","\n","# Pad the sequences\n","Max_Len = max([len(x) for x in df['statement']])\n","padded = pad_sequences(sequences, maxlen=Max_Len)\n","\n","print('Shape of data tensor:', padded.shape)\n","\n","# Define the maximum number of words to keep in the vocabulary\n","MAX_NUM_WORDS = 20000\n","\n","# Convert the labels to one-hot encoding\n","y = df['label']\n","print('Shape of label tensor:', y.shape)\n","\n","# Split data into training and testing sets\n","X_train, X_test, y_train, y_test = train_test_split(padded, df['label'].values, test_size=0.2)\n","\n","\n","# Load the GloVe embeddings\n","embeddings_index = {}\n","f = open('/content/drive/MyDrive/Colab Notebooks/weights/glove.42B.300d.txt', encoding='utf8')\n","for line in f:\n","    values = line.split()\n","    word = values[0]\n","    coefs = np.asarray(values[1:], dtype='float32')\n","    embeddings_index[word] = coefs\n","f.close()\n","\n","# Create an embedding matrix\n","embedding_dim = 300\n","word_index = tokenizer.word_index\n","num_words = min(MAX_NUM_WORDS, len(word_index) + 1)\n","embedding_matrix = np.zeros((num_words, embedding_dim))\n","for word, i in word_index.items():\n","    if i >= MAX_NUM_WORDS:\n","        continue\n","    embedding_vector = embeddings_index.get(word)\n","    if embedding_vector is not None:\n","        embedding_matrix[i] = embedding_vector\n","\n","# Build the model\n","model = Sequential()\n","model.add(Embedding(input_dim=num_words, output_dim=embedding_dim, input_length=Max_Len, weights=[embedding_matrix], trainable=False))\n","model.add(Conv1D(128, 5, activation='relu'))\n","model.add(Dropout(0.3))\n","model.add(MaxPooling1D(2))\n","model.add(Conv1D(128, 5, activation='relu'))\n","model.add(MaxPooling1D(2))\n","model.add(Bidirectional(LSTM(32)))\n","\n","model.add(Dense(1, activation='sigmoid'))\n","\n","# Compile the model\n","model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n","print(model.summary())\n","\n","# Train the model\n","history = model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=10, batch_size=128)\n","\n","# Evaluate the model\n","y_pred = model.predict(X_test)\n","y_pred = (y_pred > 0.5).astype(int)\n","\n","# Calculate the evaluation metrics\n","accuracy = accuracy_score(y_test, y_pred)\n","precision = precision_score(y_test, y_pred)\n","recall = recall_score(y_test, y_pred)\n","f1 = f1_score(y_test, y_pred)\n","\n","print(\"Accuracy:\", accuracy)\n","print(\"Precision:\", precision)\n","print(\"Recall:\", recall)\n","print(\"F1-Score:\", f1)\n","\n","# Save the model\n","model.save('/content/drive/MyDrive/Colab Notebooks/weights/Combined/glove42b_combined.h5')\n","\n","# Load the saved model\n","# loaded_model = tf.keras.models.load_model('lstm_fn.h5')\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"FVtCtuGu12wL","executionInfo":{"status":"ok","timestamp":1681248922546,"user_tz":-330,"elapsed":2663883,"user":{"displayName":"Ramacharan Reddy Kasireddy","userId":"17080325997413282440"}},"outputId":"41590cd3-5cab-48b9-eacb-4c7200fb2901"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Found 34902 unique tokens.\n","Shape of data tensor: (75023, 29569)\n","Shape of label tensor: (75023,)\n","Model: \"sequential_3\"\n","_________________________________________________________________\n"," Layer (type)                Output Shape              Param #   \n","=================================================================\n"," embedding_3 (Embedding)     (None, 29569, 300)        6000000   \n","                                                                 \n"," conv1d_6 (Conv1D)           (None, 29565, 128)        192128    \n","                                                                 \n"," dropout_3 (Dropout)         (None, 29565, 128)        0         \n","                                                                 \n"," max_pooling1d_6 (MaxPooling  (None, 14782, 128)       0         \n"," 1D)                                                             \n","                                                                 \n"," conv1d_7 (Conv1D)           (None, 14778, 128)        82048     \n","                                                                 \n"," max_pooling1d_7 (MaxPooling  (None, 7389, 128)        0         \n"," 1D)                                                             \n","                                                                 \n"," bidirectional_3 (Bidirectio  (None, 64)               41216     \n"," nal)                                                            \n","                                                                 \n"," dense_3 (Dense)             (None, 1)                 65        \n","                                                                 \n","=================================================================\n","Total params: 6,315,457\n","Trainable params: 315,457\n","Non-trainable params: 6,000,000\n","_________________________________________________________________\n","None\n","Epoch 1/10\n","469/469 [==============================] - 251s 526ms/step - loss: 0.3990 - accuracy: 0.8227 - val_loss: 0.2829 - val_accuracy: 0.8864\n","Epoch 2/10\n","469/469 [==============================] - 246s 525ms/step - loss: 0.2533 - accuracy: 0.8991 - val_loss: 0.2563 - val_accuracy: 0.8988\n","Epoch 3/10\n","469/469 [==============================] - 246s 524ms/step - loss: 0.2060 - accuracy: 0.9192 - val_loss: 0.2438 - val_accuracy: 0.9012\n","Epoch 4/10\n","469/469 [==============================] - 246s 525ms/step - loss: 0.1707 - accuracy: 0.9329 - val_loss: 0.2371 - val_accuracy: 0.9078\n","Epoch 5/10\n","469/469 [==============================] - 246s 525ms/step - loss: 0.1416 - accuracy: 0.9437 - val_loss: 0.2644 - val_accuracy: 0.9008\n","Epoch 6/10\n","469/469 [==============================] - 246s 524ms/step - loss: 0.1218 - accuracy: 0.9508 - val_loss: 0.2741 - val_accuracy: 0.9010\n","Epoch 7/10\n","469/469 [==============================] - 246s 525ms/step - loss: 0.1089 - accuracy: 0.9559 - val_loss: 0.2754 - val_accuracy: 0.9008\n","Epoch 8/10\n","469/469 [==============================] - 246s 524ms/step - loss: 0.0949 - accuracy: 0.9621 - val_loss: 0.2842 - val_accuracy: 0.9074\n","Epoch 9/10\n","469/469 [==============================] - 246s 524ms/step - loss: 0.0868 - accuracy: 0.9660 - val_loss: 0.3022 - val_accuracy: 0.9032\n","Epoch 10/10\n","469/469 [==============================] - 246s 524ms/step - loss: 0.0793 - accuracy: 0.9687 - val_loss: 0.3069 - val_accuracy: 0.9058\n","469/469 [==============================] - 67s 142ms/step\n","Accuracy: 0.9058313895368211\n","Precision: 0.9234312946783161\n","Recall: 0.8619513641755635\n","F1-Score: 0.8916327939259145\n"]}]},{"cell_type":"markdown","source":["# Word2Vec"],"metadata":{"id":"bp1ue4ounMez"}},{"cell_type":"code","source":["import numpy as np\n","import pandas as pd\n","from tensorflow.keras.preprocessing.text import Tokenizer\n","from tensorflow.keras.preprocessing.sequence import pad_sequences\n","from keras.layers import Embedding, Conv1D, MaxPooling1D, Bidirectional, LSTM, Dense, Dropout\n","from keras.models import Sequential\n","from sklearn.model_selection import train_test_split\n","from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n","\n","\n","df = pd.read_csv (r'/content/drive/MyDrive/New/Clean/Combined_clean.csv')\n","\n","MAX_NB_WORDS = 50000  # Maximum number of words to be used in the tokenizer\n","VALIDATION_SPLIT = 0.2\n","\n","# Tokenize the news statements\n","tokenizer = Tokenizer(num_words=MAX_NB_WORDS)\n","tokenizer.fit_on_texts(df['statement'])\n","word_index = tokenizer.word_index\n","print('Found %s unique tokens.' % len(word_index))\n","\n","# Convert the news statements to sequences of integers\n","sequences = tokenizer.texts_to_sequences(df['statement'])\n","\n","# Pad the sequences\n","Max_Len = max([len(x) for x in df['statement']])\n","padded = pad_sequences(sequences, maxlen=Max_Len)\n","\n","print('Shape of data tensor:', padded.shape)\n","\n","# Define the maximum number of words to keep in the vocabulary\n","MAX_NUM_WORDS = 20000\n","\n","# Convert the labels to one-hot encoding\n","y = df['label']\n","print('Shape of label tensor:', y.shape)\n","\n","# Split data into training and testing sets\n","X_train, X_test, y_train, y_test = train_test_split(padded, df['label'].values, test_size=0.2)\n","\n","# Load the Word2Vec embeddings\n","from gensim.models.word2vec import Word2Vec\n","import gensim.downloader as api\n","\n","# Load the pre-trained Word2Vec model\n","w2v_model = api.load('word2vec-google-news-300')\n","\n","# Create an embedding matrix\n","embedding_dim = 300\n","word_index = tokenizer.word_index\n","num_words = min(MAX_NUM_WORDS, len(word_index) + 1)\n","embedding_matrix = np.zeros((num_words, embedding_dim))\n","for word, i in word_index.items():\n","    if i >= MAX_NUM_WORDS:\n","        continue\n","    if word in w2v_model:\n","        embedding_vector = w2v_model[word]\n","        embedding_matrix[i] = embedding_vector\n","\n","# Build the model\n","model = Sequential()\n","model.add(Embedding(input_dim=num_words, output_dim=embedding_dim, input_length=Max_Len, weights=[embedding_matrix], trainable=False))\n","model.add(Conv1D(128, 5, activation='relu'))\n","model.add(Dropout(0.3))\n","model.add(MaxPooling1D(2))\n","model.add(Conv1D(128, 5, activation='relu'))\n","model.add(MaxPooling1D(2))\n","model.add(Bidirectional(LSTM(32)))\n","\n","model.add(Dense(1, activation='sigmoid'))\n","\n","# Compile the model\n","model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n","print(model.summary())\n","\n","# Train the model\n","history = model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=10, batch_size=128)\n","\n","# Evaluate the model\n","y_pred = model.predict(X_test)\n","y_pred = (y_pred > 0.5).astype(int)\n","\n","# Calculate the evaluation metrics\n","accuracy = accuracy_score(y_test, y_pred)\n","precision = precision_score(y_test, y_pred)\n","recall = recall_score(y_test, y_pred)\n","f1 = f1_score(y_test, y_pred)\n","\n","print(\"Accuracy:\", accuracy)\n","print(\"Precision:\", precision)\n","print(\"Recall:\", recall)\n","print(\"F1-Score:\", f1)\n","\n","# Save the model\n","model.save('/content/drive/MyDrive/Colab Notebooks/weights/Combined/hybrid_word2vec_combined.h5')\n","\n","# Load the saved model\n","# loaded_model = tf.keras.models.load_model('lstm_fn.h5')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"I9SXTMnhmSB7","executionInfo":{"status":"ok","timestamp":1681251833623,"user_tz":-330,"elapsed":2885772,"user":{"displayName":"Ramacharan Reddy Kasireddy","userId":"17080325997413282440"}},"outputId":"8e89d557-ce37-4574-9bc3-69c068e7d904"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Found 34902 unique tokens.\n","Shape of data tensor: (75023, 29569)\n","Shape of label tensor: (75023,)\n","[==================================================] 100.0% 1662.8/1662.8MB downloaded\n","Model: \"sequential_4\"\n","_________________________________________________________________\n"," Layer (type)                Output Shape              Param #   \n","=================================================================\n"," embedding_4 (Embedding)     (None, 29569, 300)        6000000   \n","                                                                 \n"," conv1d_8 (Conv1D)           (None, 29565, 128)        192128    \n","                                                                 \n"," dropout_4 (Dropout)         (None, 29565, 128)        0         \n","                                                                 \n"," max_pooling1d_8 (MaxPooling  (None, 14782, 128)       0         \n"," 1D)                                                             \n","                                                                 \n"," conv1d_9 (Conv1D)           (None, 14778, 128)        82048     \n","                                                                 \n"," max_pooling1d_9 (MaxPooling  (None, 7389, 128)        0         \n"," 1D)                                                             \n","                                                                 \n"," bidirectional_4 (Bidirectio  (None, 64)               41216     \n"," nal)                                                            \n","                                                                 \n"," dense_4 (Dense)             (None, 1)                 65        \n","                                                                 \n","=================================================================\n","Total params: 6,315,457\n","Trainable params: 315,457\n","Non-trainable params: 6,000,000\n","_________________________________________________________________\n","None\n","Epoch 1/10\n","469/469 [==============================] - 251s 526ms/step - loss: 0.4246 - accuracy: 0.8074 - val_loss: 0.3170 - val_accuracy: 0.8709\n","Epoch 2/10\n","469/469 [==============================] - 246s 524ms/step - loss: 0.2705 - accuracy: 0.8920 - val_loss: 0.2436 - val_accuracy: 0.9040\n","Epoch 3/10\n","469/469 [==============================] - 246s 524ms/step - loss: 0.2145 - accuracy: 0.9165 - val_loss: 0.2477 - val_accuracy: 0.9011\n","Epoch 4/10\n","469/469 [==============================] - 246s 524ms/step - loss: 0.1750 - accuracy: 0.9320 - val_loss: 0.2360 - val_accuracy: 0.9061\n","Epoch 5/10\n","469/469 [==============================] - 246s 524ms/step - loss: 0.1484 - accuracy: 0.9411 - val_loss: 0.2502 - val_accuracy: 0.8993\n","Epoch 6/10\n","469/469 [==============================] - 246s 524ms/step - loss: 0.1226 - accuracy: 0.9515 - val_loss: 0.2511 - val_accuracy: 0.9086\n","Epoch 7/10\n","469/469 [==============================] - 245s 523ms/step - loss: 0.1077 - accuracy: 0.9572 - val_loss: 0.2580 - val_accuracy: 0.9122\n","Epoch 8/10\n","469/469 [==============================] - 246s 524ms/step - loss: 0.0932 - accuracy: 0.9637 - val_loss: 0.2772 - val_accuracy: 0.9074\n","Epoch 9/10\n","469/469 [==============================] - 246s 524ms/step - loss: 0.0835 - accuracy: 0.9671 - val_loss: 0.2725 - val_accuracy: 0.9120\n","Epoch 10/10\n","469/469 [==============================] - 246s 524ms/step - loss: 0.0758 - accuracy: 0.9705 - val_loss: 0.2845 - val_accuracy: 0.9106\n","469/469 [==============================] - 67s 142ms/step\n","Accuracy: 0.9105631456181273\n","Precision: 0.9205444761000317\n","Recall: 0.8737980769230769\n","F1-Score: 0.8965623554801911\n"]}]},{"cell_type":"markdown","source":["# FastText"],"metadata":{"id":"L2aH41JqnHhF"}},{"cell_type":"code","source":["import numpy as np\n","import pandas as pd\n","from tensorflow.keras.preprocessing.text import Tokenizer\n","from tensorflow.keras.preprocessing.sequence import pad_sequences\n","from keras.layers import Embedding, Conv1D, MaxPooling1D, Bidirectional, LSTM, Dense, Dropout\n","from keras.models import Sequential\n","from sklearn.model_selection import train_test_split\n","from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n","\n","# Load the data from CSV file\n","df = pd.read_csv('/content/drive/MyDrive/New/Clean/Combined_clean.csv')\n","\n","MAX_NB_WORDS = 50000  # Maximum number of words to be used in the tokenizer\n","VALIDATION_SPLIT = 0.2\n","\n","# Split data into statements and labels\n","statements = df['statement'].astype(str)\n","labels = df['label']\n","\n","# Tokenize statements\n","tokenizer = Tokenizer()\n","tokenizer.fit_on_texts(statements)\n","word_index = tokenizer.word_index\n","sequences = tokenizer.texts_to_sequences(statements)\n","\n","# Pad sequences\n","Max_Len = max([len(x) for x in statements])\n","padded_sequences = pad_sequences(sequences, maxlen=Max_Len)\n","\n","print('Shape of data tensor:', padded_sequences.shape)\n","\n","# Define the maximum number of words to keep in the vocabulary\n","MAX_NUM_WORDS = 20000\n","\n","# Convert the labels to one-hot encoding\n","y = df['label']\n","print('Shape of label tensor:', y.shape)\n","\n","# Split data into training and testing sets\n","X_train, X_test, y_train, y_test = train_test_split(padded_sequences, df['label'].values, test_size=0.2)\n","\n","# Load the Word2Vec embeddings\n","from gensim.models.word2vec import Word2Vec\n","import gensim.downloader as api\n","\n","# Load the pre-trained Word2Vec model\n","ft_model = api.load('fasttext-wiki-news-subwords-300')\n","\n","# Create an embedding matrix\n","embedding_dim = 300\n","word_index = tokenizer.word_index\n","num_words = min(MAX_NUM_WORDS, len(word_index) + 1)\n","embedding_matrix = np.zeros((num_words, embedding_dim))\n","for word, i in word_index.items():\n","    if i >= MAX_NUM_WORDS:\n","        continue\n","    if word in ft_model:\n","        embedding_vector = ft_model[word]\n","        embedding_matrix[i] = embedding_vector\n","\n","# Build the model\n","model = Sequential()\n","model.add(Embedding(input_dim=num_words, output_dim=embedding_dim, input_length=Max_Len, weights=[embedding_matrix], trainable=False))\n","model.add(Conv1D(128, 5, activation='relu'))\n","model.add(Dropout(0.3))\n","model.add(MaxPooling1D(2))\n","model.add(Conv1D(128, 5, activation='relu'))\n","model.add(MaxPooling1D(2))\n","model.add(Bidirectional(LSTM(32)))\n","model.add(Dense(1, activation='sigmoid'))\n","\n","# Compile the model\n","model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n","print(model.summary())\n","\n","# Train the model\n","history = model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=10, batch_size=128)\n","\n","# Evaluate the model\n","y_pred = model.predict(X_test)\n","y_pred = (y_pred > 0.5).astype(int)\n","\n","# Calculate the evaluation metrics\n","accuracy = accuracy_score(y_test, y_pred)\n","precision = precision_score(y_test, y_pred)\n","recall = recall_score(y_test, y_pred)\n","f1 = f1_score(y_test, y_pred)\n","\n","print(\"Accuracy:\", accuracy)\n","print(\"Precision:\", precision)\n","print(\"Recall:\", recall)\n","print(\"F1-Score:\", f1)\n","\n","# Save the model\n","model.save('/content/drive/MyDrive/Colab Notebooks/weights/Combined/ft_combined.h5')\n","\n","# Load the saved model\n","# loaded_model = tf.keras.models.load_model('lstm_fn.h5')\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"K6nVQqiNm9Kk","executionInfo":{"status":"ok","timestamp":1681246214524,"user_tz":-330,"elapsed":2883376,"user":{"displayName":"Ramacharan Reddy Kasireddy","userId":"17080325997413282440"}},"outputId":"e62463a1-c88e-4b56-8295-6e7eb3e0834e"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Shape of data tensor: (75023, 29569)\n","Shape of label tensor: (75023,)\n","[==================================================] 100.0% 958.5/958.4MB downloaded\n","Model: \"sequential_2\"\n","_________________________________________________________________\n"," Layer (type)                Output Shape              Param #   \n","=================================================================\n"," embedding_2 (Embedding)     (None, 29569, 300)        6000000   \n","                                                                 \n"," conv1d_4 (Conv1D)           (None, 29565, 128)        192128    \n","                                                                 \n"," dropout_2 (Dropout)         (None, 29565, 128)        0         \n","                                                                 \n"," max_pooling1d_4 (MaxPooling  (None, 14782, 128)       0         \n"," 1D)                                                             \n","                                                                 \n"," conv1d_5 (Conv1D)           (None, 14778, 128)        82048     \n","                                                                 \n"," max_pooling1d_5 (MaxPooling  (None, 7389, 128)        0         \n"," 1D)                                                             \n","                                                                 \n"," bidirectional_2 (Bidirectio  (None, 64)               41216     \n"," nal)                                                            \n","                                                                 \n"," dense_2 (Dense)             (None, 1)                 65        \n","                                                                 \n","=================================================================\n","Total params: 6,315,457\n","Trainable params: 315,457\n","Non-trainable params: 6,000,000\n","_________________________________________________________________\n","None\n","Epoch 1/10\n","469/469 [==============================] - 251s 526ms/step - loss: 0.4328 - accuracy: 0.8036 - val_loss: 0.3379 - val_accuracy: 0.8597\n","Epoch 2/10\n","469/469 [==============================] - 246s 524ms/step - loss: 0.2968 - accuracy: 0.8795 - val_loss: 0.2978 - val_accuracy: 0.8774\n","Epoch 3/10\n","469/469 [==============================] - 246s 524ms/step - loss: 0.2444 - accuracy: 0.9031 - val_loss: 0.2416 - val_accuracy: 0.9048\n","Epoch 4/10\n","469/469 [==============================] - 246s 524ms/step - loss: 0.2108 - accuracy: 0.9170 - val_loss: 0.2423 - val_accuracy: 0.9026\n","Epoch 5/10\n","469/469 [==============================] - 246s 524ms/step - loss: 0.1849 - accuracy: 0.9272 - val_loss: 0.2309 - val_accuracy: 0.9095\n","Epoch 6/10\n","469/469 [==============================] - 246s 524ms/step - loss: 0.1638 - accuracy: 0.9357 - val_loss: 0.2347 - val_accuracy: 0.9092\n","Epoch 7/10\n","469/469 [==============================] - 246s 524ms/step - loss: 0.1419 - accuracy: 0.9435 - val_loss: 0.2408 - val_accuracy: 0.9068\n","Epoch 8/10\n","469/469 [==============================] - 246s 524ms/step - loss: 0.1290 - accuracy: 0.9481 - val_loss: 0.2722 - val_accuracy: 0.9004\n","Epoch 9/10\n","469/469 [==============================] - 246s 524ms/step - loss: 0.1178 - accuracy: 0.9522 - val_loss: 0.2518 - val_accuracy: 0.9086\n","Epoch 10/10\n","469/469 [==============================] - 245s 523ms/step - loss: 0.1079 - accuracy: 0.9558 - val_loss: 0.2716 - val_accuracy: 0.9087\n","469/469 [==============================] - 67s 142ms/step\n","Accuracy: 0.9086971009663446\n","Precision: 0.9306784660766961\n","Recall: 0.8570781768789617\n","F1-Score: 0.8923632935260841\n"]}]},{"cell_type":"code","source":[],"metadata":{"id":"fADmyXxnreyN"},"execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"},"vscode":{"interpreter":{"hash":"aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"}},"colab":{"provenance":[{"file_id":"195VpL2hxfaVbGUg5vKzJWOjcrZszboc6","timestamp":1676873879368}],"collapsed_sections":["PmF1iUhDEey3","LaFNMN9FHTdr"],"machine_shape":"hm"},"gpuClass":"premium","accelerator":"GPU"},"nbformat":4,"nbformat_minor":0}