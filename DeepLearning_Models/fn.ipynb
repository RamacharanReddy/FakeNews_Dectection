{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":4677,"status":"ok","timestamp":1680531846834,"user":{"displayName":"Ramacharan Reddy Kasireddy","userId":"17080325997413282440"},"user_tz":-330},"id":"6Dd1ZD6RFl82","outputId":"a638f737-6a5b-4477-a378-25c53738557b"},"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting Keras-Preprocessing\n","  Downloading Keras_Preprocessing-1.1.2-py2.py3-none-any.whl (42 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.6/42.6 KB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.9/dist-packages (from Keras-Preprocessing) (1.16.0)\n","Requirement already satisfied: numpy>=1.9.1 in /usr/local/lib/python3.9/dist-packages (from Keras-Preprocessing) (1.22.4)\n","Installing collected packages: Keras-Preprocessing\n","Successfully installed Keras-Preprocessing-1.1.2\n"]}],"source":["!pip install Keras-Preprocessing"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":20484,"status":"ok","timestamp":1680531867315,"user":{"displayName":"Ramacharan Reddy Kasireddy","userId":"17080325997413282440"},"user_tz":-330},"id":"KQVP5KQC8hyh","outputId":"ebca4589-3727-4d25-b9f1-d206f7f30847"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","source":["!pip install tensorflow"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"xBZfGEfh2KGk","executionInfo":{"status":"ok","timestamp":1680531870219,"user_tz":-330,"elapsed":2908,"user":{"displayName":"Ramacharan Reddy Kasireddy","userId":"17080325997413282440"}},"outputId":"f9b77122-5f53-4cb6-9325-ee25c1b55f8a"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Requirement already satisfied: tensorflow in /usr/local/lib/python3.9/dist-packages (2.12.0)\n","Requirement already satisfied: gast<=0.4.0,>=0.2.1 in /usr/local/lib/python3.9/dist-packages (from tensorflow) (0.4.0)\n","Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.9/dist-packages (from tensorflow) (1.16.0)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.9/dist-packages (from tensorflow) (67.6.1)\n","Requirement already satisfied: jax>=0.3.15 in /usr/local/lib/python3.9/dist-packages (from tensorflow) (0.4.7)\n","Requirement already satisfied: tensorflow-estimator<2.13,>=2.12.0 in /usr/local/lib/python3.9/dist-packages (from tensorflow) (2.12.0)\n","Requirement already satisfied: flatbuffers>=2.0 in /usr/local/lib/python3.9/dist-packages (from tensorflow) (23.3.3)\n","Requirement already satisfied: keras<2.13,>=2.12.0 in /usr/local/lib/python3.9/dist-packages (from tensorflow) (2.12.0)\n","Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.9/dist-packages (from tensorflow) (1.6.3)\n","Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.9/dist-packages (from tensorflow) (1.53.0)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.9/dist-packages (from tensorflow) (23.0)\n","Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.9/dist-packages (from tensorflow) (1.4.0)\n","Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.9/dist-packages (from tensorflow) (3.3.0)\n","Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /usr/local/lib/python3.9/dist-packages (from tensorflow) (3.20.3)\n","Requirement already satisfied: tensorboard<2.13,>=2.12 in /usr/local/lib/python3.9/dist-packages (from tensorflow) (2.12.0)\n","Requirement already satisfied: numpy<1.24,>=1.22 in /usr/local/lib/python3.9/dist-packages (from tensorflow) (1.22.4)\n","Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.9/dist-packages (from tensorflow) (2.2.0)\n","Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.9/dist-packages (from tensorflow) (16.0.0)\n","Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.9/dist-packages (from tensorflow) (4.5.0)\n","Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.9/dist-packages (from tensorflow) (0.2.0)\n","Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.9/dist-packages (from tensorflow) (0.32.0)\n","Requirement already satisfied: wrapt<1.15,>=1.11.0 in /usr/local/lib/python3.9/dist-packages (from tensorflow) (1.14.1)\n","Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.9/dist-packages (from tensorflow) (3.8.0)\n","Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.9/dist-packages (from astunparse>=1.6.0->tensorflow) (0.40.0)\n","Requirement already satisfied: ml-dtypes>=0.0.3 in /usr/local/lib/python3.9/dist-packages (from jax>=0.3.15->tensorflow) (0.0.4)\n","Requirement already satisfied: scipy>=1.7 in /usr/local/lib/python3.9/dist-packages (from jax>=0.3.15->tensorflow) (1.10.1)\n","Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.9/dist-packages (from tensorboard<2.13,>=2.12->tensorflow) (0.7.0)\n","Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.9/dist-packages (from tensorboard<2.13,>=2.12->tensorflow) (1.8.1)\n","Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.9/dist-packages (from tensorboard<2.13,>=2.12->tensorflow) (2.2.3)\n","Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.9/dist-packages (from tensorboard<2.13,>=2.12->tensorflow) (2.27.1)\n","Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.9/dist-packages (from tensorboard<2.13,>=2.12->tensorflow) (0.4.6)\n","Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.9/dist-packages (from tensorboard<2.13,>=2.12->tensorflow) (3.4.3)\n","Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.9/dist-packages (from tensorboard<2.13,>=2.12->tensorflow) (2.17.0)\n","Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.9/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow) (5.3.0)\n","Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.9/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow) (0.2.8)\n","Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.9/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow) (4.9)\n","Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.9/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.13,>=2.12->tensorflow) (1.3.1)\n","Requirement already satisfied: importlib-metadata>=4.4 in /usr/local/lib/python3.9/dist-packages (from markdown>=2.6.8->tensorboard<2.13,>=2.12->tensorflow) (6.1.0)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.9/dist-packages (from requests<3,>=2.21.0->tensorboard<2.13,>=2.12->tensorflow) (2022.12.7)\n","Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.9/dist-packages (from requests<3,>=2.21.0->tensorboard<2.13,>=2.12->tensorflow) (2.0.12)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.9/dist-packages (from requests<3,>=2.21.0->tensorboard<2.13,>=2.12->tensorflow) (3.4)\n","Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests<3,>=2.21.0->tensorboard<2.13,>=2.12->tensorflow) (1.26.15)\n","Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.9/dist-packages (from werkzeug>=1.0.1->tensorboard<2.13,>=2.12->tensorflow) (2.1.2)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.9/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<2.13,>=2.12->tensorflow) (3.15.0)\n","Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.9/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow) (0.4.8)\n","Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.9/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.13,>=2.12->tensorflow) (3.2.2)\n"]}]},{"cell_type":"markdown","metadata":{"id":"G-GAEKWh41AQ"},"source":["# Linear SVM\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"VGXraLrT4kNx"},"outputs":[],"source":["# Importing required libraries\n","import pandas as pd\n","from sklearn.feature_extraction.text import TfidfVectorizer\n","from sklearn.model_selection import cross_val_score, cross_val_predict\n","from sklearn.metrics import accuracy_score, recall_score, precision_score, f1_score\n","from sklearn.svm import LinearSVC\n","\n","# Reading data into a pandas DataFrame\n","data = pd.read_csv (r'/content/drive/MyDrive/New/Clean/FakeNews_clean.csv')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"BZenuafz4lTU"},"outputs":[],"source":["from sklearn.feature_extraction.text import TfidfVectorizer\n","tfidf_v = TfidfVectorizer(max_features=5000, ngram_range=(1,3))\n","X = tfidf_v.fit_transform(data['statement'].values.astype('U'))\n","y = data['label'].values"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2158,"status":"ok","timestamp":1680532984372,"user":{"displayName":"Ramacharan Reddy Kasireddy","userId":"17080325997413282440"},"user_tz":-330},"id":"tyzkNKhU4ovo","outputId":"d7cf1666-5d21-47bf-d303-96ae63e73b1a"},"outputs":[{"output_type":"stream","name":"stdout","text":["Cross-validation scores: [0.93160445 0.91935024 0.92903961 0.92163009 0.92645382]\n","Accuracy: 0.9256155950752394\n","Recall: 0.9040697674418605\n","Precision: 0.9143217135657287\n","F1 score: 0.9091668406765505\n"]},{"output_type":"execute_result","data":{"text/plain":["['/content/drive/MyDrive/Colab Notebooks/weights/FN/linearsvc_fn.pkl']"]},"metadata":{},"execution_count":9}],"source":["import joblib\n","\n","# Creating a Linear SVM classifier\n","svm = LinearSVC()\n","\n","# Performing five-fold cross-validation\n","scores = cross_val_score(svm, X, y, cv=5)\n","\n","# Printing the cross-validation scores\n","print(\"Cross-validation scores:\", scores)\n","\n","# Computing and printing accuracy, recall, precision, and F1 score\n","y_pred = cross_val_predict(svm, X, y, cv=5)\n","accuracy = accuracy_score(y, y_pred)\n","recall = recall_score(y, y_pred)\n","precision = precision_score(y, y_pred)\n","f1 = f1_score(y, y_pred)\n","print(\"Accuracy:\", accuracy)\n","print(\"Recall:\", recall)\n","print(\"Precision:\", precision)\n","print(\"F1 score:\", f1)\n","\n","# Save the trained model to a file\n","joblib.dump(svm, '/content/drive/MyDrive/Colab Notebooks/weights/FN/linearsvc_fn.pkl')\n"," "]},{"cell_type":"markdown","metadata":{"id":"RrPTHD0tEkw6"},"source":["# LSTM\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":88441,"status":"ok","timestamp":1680533088980,"user":{"displayName":"Ramacharan Reddy Kasireddy","userId":"17080325997413282440"},"user_tz":-330},"id":"irUXCM2q5b8g","outputId":"5f281150-e897-4b9c-80d0-038c5152e977"},"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 1/10\n","439/439 - 38s - loss: 0.3963 - accuracy: 0.8117 - val_loss: 0.2987 - val_accuracy: 0.8803 - 38s/epoch - 86ms/step\n","Epoch 2/10\n","439/439 - 9s - loss: 0.2255 - accuracy: 0.9146 - val_loss: 0.2979 - val_accuracy: 0.8814 - 9s/epoch - 21ms/step\n","Epoch 3/10\n","439/439 - 7s - loss: 0.1766 - accuracy: 0.9367 - val_loss: 0.3241 - val_accuracy: 0.8749 - 7s/epoch - 15ms/step\n","Epoch 4/10\n","439/439 - 6s - loss: 0.1470 - accuracy: 0.9486 - val_loss: 0.3410 - val_accuracy: 0.8726 - 6s/epoch - 13ms/step\n","Epoch 5/10\n","439/439 - 4s - loss: 0.1162 - accuracy: 0.9612 - val_loss: 0.3975 - val_accuracy: 0.8723 - 4s/epoch - 9ms/step\n","Epoch 6/10\n","439/439 - 4s - loss: 0.0874 - accuracy: 0.9714 - val_loss: 0.4596 - val_accuracy: 0.8686 - 4s/epoch - 9ms/step\n","Epoch 7/10\n","439/439 - 4s - loss: 0.0657 - accuracy: 0.9783 - val_loss: 0.5784 - val_accuracy: 0.8618 - 4s/epoch - 10ms/step\n","Epoch 8/10\n","439/439 - 4s - loss: 0.0555 - accuracy: 0.9810 - val_loss: 0.5249 - val_accuracy: 0.8629 - 4s/epoch - 8ms/step\n","Epoch 9/10\n","439/439 - 4s - loss: 0.0389 - accuracy: 0.9876 - val_loss: 0.5662 - val_accuracy: 0.8638 - 4s/epoch - 9ms/step\n","Epoch 10/10\n","439/439 - 3s - loss: 0.0284 - accuracy: 0.9910 - val_loss: 0.7144 - val_accuracy: 0.8641 - 3s/epoch - 8ms/step\n","110/110 [==============================] - 1s 3ms/step\n","Accuracy: 0.8640638358506697\n","Precision: 0.8274247491638796\n","Recall: 0.8495879120879121\n","F1-Score: 0.8383598780074552\n"]}],"source":["import numpy as np\n","import pandas as pd\n","import tensorflow as tf\n","from sklearn.model_selection import train_test_split\n","from keras.preprocessing.text import Tokenizer\n","from keras_preprocessing.sequence import pad_sequences\n","from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n","\n","df = pd.read_csv (r'/content/drive/MyDrive/New/Clean/FakeNews_clean.csv')\n","\n","# Tokenize the text\n","# Split data into statements and labels\n","statements = df['statement'].astype(str)\n","labels = df['label']\n","\n","# Tokenize statements\n","tokenizer = Tokenizer()\n","tokenizer.fit_on_texts(statements)\n","word_index = tokenizer.word_index\n","sequences = tokenizer.texts_to_sequences(statements)\n","\n","# Pad sequences\n","Max_Len = 100\n","padded_sequences = pad_sequences(sequences, maxlen=Max_Len)\n","\n","\n","# Split the data into training and testing sets\n","X_train, X_test, y_train, y_test = train_test_split(padded_sequences, df['label'].values, test_size=0.2, random_state=0)\n","\n","# Build the model\n","model = tf.keras.Sequential([\n","    tf.keras.layers.Embedding(input_dim=5000, output_dim=32, input_length=Max_Len),\n","    tf.keras.layers.LSTM(32),\n","    tf.keras.layers.Dense(1, activation='sigmoid')\n","])\n","\n","# Compile the model\n","model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n","\n","# Train the model\n","history = model.fit(X_train, y_train, epochs=10, validation_data=(X_test, y_test), verbose=2)\n","\n","# Evaluate the model\n","y_pred = model.predict(X_test)\n","y_pred = (y_pred > 0.5).astype(int)\n","\n","# Calculate the evaluation metrics\n","accuracy = accuracy_score(y_test, y_pred)\n","precision = precision_score(y_test, y_pred)\n","recall = recall_score(y_test, y_pred)\n","f1 = f1_score(y_test, y_pred)\n","\n","print(\"Accuracy:\", accuracy)\n","print(\"Precision:\", precision)\n","print(\"Recall:\", recall)\n","print(\"F1-Score:\", f1)\n","\n","# Save the model\n","model.save('/content/drive/MyDrive/Colab Notebooks/weights/FN/lstm_fn.h5')\n","\n","# Load the saved model\n","# loaded_model = tf.keras.models.load_model('lstm_fn.h5')\n"]},{"cell_type":"markdown","metadata":{"id":"SB2BWfsxFOPo"},"source":["# BI LSTM\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":155719,"status":"ok","timestamp":1680533272331,"user":{"displayName":"Ramacharan Reddy Kasireddy","userId":"17080325997413282440"},"user_tz":-330},"id":"FLFOmQOGC9BG","outputId":"578147d7-41f2-4a06-9419-02780a5b7fa3"},"outputs":[{"output_type":"stream","name":"stdout","text":["Model: \"sequential_4\"\n","_________________________________________________________________\n"," Layer (type)                Output Shape              Param #   \n","=================================================================\n"," embedding_4 (Embedding)     (None, 100, 128)          3925376   \n","                                                                 \n"," bidirectional_3 (Bidirectio  (None, 100, 128)         98816     \n"," nal)                                                            \n","                                                                 \n"," bidirectional_4 (Bidirectio  (None, 64)               41216     \n"," nal)                                                            \n","                                                                 \n"," dense_4 (Dense)             (None, 1)                 65        \n","                                                                 \n","=================================================================\n","Total params: 4,065,473\n","Trainable params: 4,065,473\n","Non-trainable params: 0\n","_________________________________________________________________\n","None\n","Epoch 1/10\n","439/439 [==============================] - 52s 104ms/step - loss: 0.3527 - accuracy: 0.8452 - val_loss: 0.2612 - val_accuracy: 0.8940\n","Epoch 2/10\n","439/439 [==============================] - 15s 35ms/step - loss: 0.1306 - accuracy: 0.9543 - val_loss: 0.3089 - val_accuracy: 0.8849\n","Epoch 3/10\n","439/439 [==============================] - 9s 22ms/step - loss: 0.0444 - accuracy: 0.9854 - val_loss: 0.4165 - val_accuracy: 0.8763\n","Epoch 4/10\n","439/439 [==============================] - 10s 23ms/step - loss: 0.0180 - accuracy: 0.9949 - val_loss: 0.4765 - val_accuracy: 0.8769\n","Epoch 5/10\n","439/439 [==============================] - 10s 23ms/step - loss: 0.0145 - accuracy: 0.9951 - val_loss: 0.5752 - val_accuracy: 0.8735\n","Epoch 6/10\n","439/439 [==============================] - 10s 22ms/step - loss: 0.0119 - accuracy: 0.9967 - val_loss: 0.5761 - val_accuracy: 0.8772\n","Epoch 7/10\n","439/439 [==============================] - 9s 21ms/step - loss: 0.0105 - accuracy: 0.9970 - val_loss: 0.5901 - val_accuracy: 0.8732\n","Epoch 8/10\n","439/439 [==============================] - 9s 20ms/step - loss: 0.0074 - accuracy: 0.9981 - val_loss: 0.6518 - val_accuracy: 0.8777\n","Epoch 9/10\n","439/439 [==============================] - 9s 20ms/step - loss: 0.0011 - accuracy: 0.9999 - val_loss: 0.8127 - val_accuracy: 0.8743\n","Epoch 10/10\n","439/439 [==============================] - 8s 18ms/step - loss: 1.7057e-04 - accuracy: 1.0000 - val_loss: 0.8417 - val_accuracy: 0.8740\n","110/110 [==============================] - 2s 7ms/step\n","Accuracy: 0.8740381875178114\n","Precision: 0.8388928317955997\n","Recall: 0.8460987831066571\n","F1-Score: 0.84248039914469\n"]}],"source":["import numpy as np\n","import pandas as pd\n","from keras.preprocessing.text import Tokenizer\n","from keras_preprocessing.sequence import pad_sequences\n","from keras.models import Sequential\n","from keras.layers import Dense, Embedding, LSTM, Bidirectional\n","from sklearn.model_selection import train_test_split\n","from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n","\n","# Load the data from CSV file\n","data = pd.read_csv('/content/drive/MyDrive/New/Clean/FakeNews_clean.csv')\n","\n","# Split data into statements and labels\n","statements = data['statement']\n","labels = data['label']\n","\n","# Tokenize statements\n","tokenizer = Tokenizer()\n","tokenizer.fit_on_texts(statements)\n","word_index = tokenizer.word_index\n","sequences = tokenizer.texts_to_sequences(statements)\n","\n","# Pad sequences\n","max_len = 100\n","padded_sequences = pad_sequences(sequences, maxlen=max_len)\n","\n","# Split data into training and testing sets\n","x_train, x_test, y_train, y_test = train_test_split(padded_sequences, labels, test_size=0.2)\n","\n","# Define model architecture\n","model = Sequential()\n","model.add(Embedding(len(word_index) + 1, 128, input_length=max_len))\n","model.add(Bidirectional(LSTM(64, return_sequences=True)))\n","model.add(Bidirectional(LSTM(32)))\n","model.add(Dense(1, activation='sigmoid'))\n","\n","# Compile model\n","model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n","print(model.summary())\n","\n","# Train model\n","model.fit(x_train, y_train, epochs=10, batch_size=32, validation_data=(x_test, y_test))\n","\n","# Evaluate the model\n","y_pred = model.predict(x_test)\n","y_pred = (y_pred > 0.5).astype(int)\n","\n","# Calculate the evaluation metrics\n","accuracy = accuracy_score(y_test, y_pred)\n","precision = precision_score(y_test, y_pred)\n","recall = recall_score(y_test, y_pred)\n","f1 = f1_score(y_test, y_pred)\n","\n","print(\"Accuracy:\", accuracy)\n","print(\"Precision:\", precision)\n","print(\"Recall:\", recall)\n","print(\"F1-Score:\", f1)\n","\n","\n","# Save the model\n","model.save('/content/drive/MyDrive/Colab Notebooks/weights/FN/bilstm_fn.h5')\n","\n","# Load the saved model\n","# loaded_model = tf.keras.models.load_model('lstm_fn.h5')\n"]},{"cell_type":"markdown","metadata":{"id":"U01_EgTCJRRD"},"source":["# HYBRID\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"xZ8pr8CiH7zq","executionInfo":{"status":"ok","timestamp":1680533633383,"user_tz":-330,"elapsed":334125,"user":{"displayName":"Ramacharan Reddy Kasireddy","userId":"17080325997413282440"}},"outputId":"473260f5-11a4-4f71-e6fa-11602f7e5f00"},"outputs":[{"output_type":"stream","name":"stdout","text":["Found 30666 unique tokens.\n","Shape of data tensor: (17544, 3498)\n","Shape of label tensor: (17544,)\n","Model: \"sequential_5\"\n","_________________________________________________________________\n"," Layer (type)                Output Shape              Param #   \n","=================================================================\n"," embedding_5 (Embedding)     (None, 3498, 100)         5000000   \n","                                                                 \n"," conv1d_6 (Conv1D)           (None, 3494, 128)         64128     \n","                                                                 \n"," dropout_3 (Dropout)         (None, 3494, 128)         0         \n","                                                                 \n"," max_pooling1d_6 (MaxPooling  (None, 1747, 128)        0         \n"," 1D)                                                             \n","                                                                 \n"," conv1d_7 (Conv1D)           (None, 1743, 128)         82048     \n","                                                                 \n"," max_pooling1d_7 (MaxPooling  (None, 871, 128)         0         \n"," 1D)                                                             \n","                                                                 \n"," bidirectional_5 (Bidirectio  (None, 64)               41216     \n"," nal)                                                            \n","                                                                 \n"," dense_5 (Dense)             (None, 1)                 65        \n","                                                                 \n","=================================================================\n","Total params: 5,187,457\n","Trainable params: 5,187,457\n","Non-trainable params: 0\n","_________________________________________________________________\n","None\n","Epoch 1/10\n","110/110 [==============================] - 35s 284ms/step - loss: 0.4246 - accuracy: 0.7954 - val_loss: 0.2382 - val_accuracy: 0.9082\n","Epoch 2/10\n","110/110 [==============================] - 28s 259ms/step - loss: 0.1680 - accuracy: 0.9399 - val_loss: 0.2513 - val_accuracy: 0.9040\n","Epoch 3/10\n","110/110 [==============================] - 27s 244ms/step - loss: 0.0862 - accuracy: 0.9739 - val_loss: 0.2527 - val_accuracy: 0.9122\n","Epoch 4/10\n","110/110 [==============================] - 27s 243ms/step - loss: 0.0557 - accuracy: 0.9845 - val_loss: 0.2712 - val_accuracy: 0.9099\n","Epoch 5/10\n","110/110 [==============================] - 26s 235ms/step - loss: 0.0341 - accuracy: 0.9907 - val_loss: 0.3391 - val_accuracy: 0.9048\n","Epoch 6/10\n","110/110 [==============================] - 26s 240ms/step - loss: 0.0261 - accuracy: 0.9934 - val_loss: 0.3407 - val_accuracy: 0.9065\n","Epoch 7/10\n","110/110 [==============================] - 25s 226ms/step - loss: 0.0198 - accuracy: 0.9953 - val_loss: 0.3328 - val_accuracy: 0.9142\n","Epoch 8/10\n","110/110 [==============================] - 25s 230ms/step - loss: 0.0176 - accuracy: 0.9952 - val_loss: 0.4353 - val_accuracy: 0.8974\n","Epoch 9/10\n","110/110 [==============================] - 25s 229ms/step - loss: 0.0154 - accuracy: 0.9956 - val_loss: 0.3646 - val_accuracy: 0.9074\n","Epoch 10/10\n","110/110 [==============================] - 25s 227ms/step - loss: 0.0117 - accuracy: 0.9969 - val_loss: 0.3841 - val_accuracy: 0.9137\n","110/110 [==============================] - 4s 26ms/step\n","Accuracy: 0.9136506127101739\n","Precision: 0.9175257731958762\n","Recall: 0.8670842032011135\n","F1-Score: 0.8915921288014312\n"]}],"source":["import numpy as np\n","import pandas as pd\n","from keras.preprocessing.text import Tokenizer\n","from keras_preprocessing.sequence import pad_sequences\n","from keras.layers import Embedding, Conv1D, MaxPooling1D, Bidirectional, LSTM, Dense, Dropout\n","from keras.models import Sequential\n","from sklearn.model_selection import train_test_split\n","from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n","\n","# Preprocessing\n","MAX_NB_WORDS = 50000  # Maximum number of words to be used in the tokenizer\n","MAX_SEQUENCE_LENGTH = 300  # Maximum length of each news statement\n","EMBEDDING_DIM = 100  # Dimension of the word embedding\n","VALIDATION_SPLIT = 0.2  # Percentage of data to use for validation\n","BATCH_SIZE = 128\n","EPOCHS = 10\n","\n","# Load the data from CSV file\n","df = pd.read_csv('/content/drive/MyDrive/New/Clean/FakeNews_clean.csv')\n","\n","# Split data into statements and labels\n","statements = df['statement'].astype(str)\n","labels = df['label']\n","\n","# Tokenize statements\n","tokenizer = Tokenizer()\n","tokenizer.fit_on_texts(statements)\n","word_index = tokenizer.word_index\n","sequences = tokenizer.texts_to_sequences(statements)\n","print('Found %s unique tokens.' % len(word_index))\n","\n","# Pad sequences\n","Max_Len = max([len(x) for x in statements])\n","padded_sequences = pad_sequences(sequences, maxlen=Max_Len)\n","\n","print('Shape of data tensor:', padded_sequences.shape)\n","\n","# Define the maximum number of words to keep in the vocabulary\n","MAX_NUM_WORDS = 20000\n","\n","# Convert the labels to one-hot encoding\n","y = df['label']\n","print('Shape of label tensor:', y.shape)\n","\n","# Split data into training and testing sets\n","X_train, X_test, y_train, y_test = train_test_split(padded_sequences, df['label'].values, test_size=0.2)\n","\n","# Build the model\n","model = Sequential()\n","model.add(Embedding(input_dim=MAX_NB_WORDS, output_dim=EMBEDDING_DIM, input_length=Max_Len))\n","model.add(Conv1D(128, 5, activation='relu'))\n","model.add(Dropout(0.3))\n","model.add(MaxPooling1D(2))\n","model.add(Conv1D(128, 5, activation='relu'))\n","model.add(MaxPooling1D(2))\n","model.add(Bidirectional(LSTM(32)))\n","\n","model.add(Dense(1, activation='sigmoid'))\n","\n","# Compile the model\n","model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n","print(model.summary())\n","\n","# Train the model\n","model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=10, batch_size=128)\n","\n","# Evaluate the model\n","y_pred = model.predict(X_test)\n","y_pred = (y_pred > 0.5).astype(int)\n","\n","# Calculate the evaluation metrics\n","accuracy = accuracy_score(y_test, y_pred)\n","precision = precision_score(y_test, y_pred)\n","recall = recall_score(y_test, y_pred)\n","f1 = f1_score(y_test, y_pred)\n","\n","print(\"Accuracy:\", accuracy)\n","print(\"Precision:\", precision)\n","print(\"Recall:\", recall)\n","print(\"F1-Score:\", f1)\n","\n","# Save the model\n","model.save('/content/drive/MyDrive/Colab Notebooks/weights/FN/hybrid_fn.h5')\n","\n","# Load the saved model\n","# loaded_model = tf.keras.models.load_model('lstm_fn.h5')\n"]},{"cell_type":"markdown","metadata":{"id":"uYnJCHPuE-pd"},"source":["# GLOVE HYBRID\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"FVtCtuGu12wL","outputId":"b02534a9-80f9-4282-c97d-3ac7a34db25d","executionInfo":{"status":"ok","timestamp":1680534005022,"user_tz":-330,"elapsed":355171,"user":{"displayName":"Ramacharan Reddy Kasireddy","userId":"17080325997413282440"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["Found 30666 unique tokens.\n","Shape of data tensor: (17544, 3498)\n","Shape of label tensor: (17544,)\n","Model: \"sequential_6\"\n","_________________________________________________________________\n"," Layer (type)                Output Shape              Param #   \n","=================================================================\n"," embedding_6 (Embedding)     (None, 3498, 300)         6000000   \n","                                                                 \n"," conv1d_8 (Conv1D)           (None, 3494, 128)         192128    \n","                                                                 \n"," dropout_4 (Dropout)         (None, 3494, 128)         0         \n","                                                                 \n"," max_pooling1d_8 (MaxPooling  (None, 1747, 128)        0         \n"," 1D)                                                             \n","                                                                 \n"," conv1d_9 (Conv1D)           (None, 1743, 128)         82048     \n","                                                                 \n"," max_pooling1d_9 (MaxPooling  (None, 871, 128)         0         \n"," 1D)                                                             \n","                                                                 \n"," bidirectional_6 (Bidirectio  (None, 64)               41216     \n"," nal)                                                            \n","                                                                 \n"," dense_6 (Dense)             (None, 1)                 65        \n","                                                                 \n","=================================================================\n","Total params: 6,315,457\n","Trainable params: 315,457\n","Non-trainable params: 6,000,000\n","_________________________________________________________________\n","None\n","Epoch 1/10\n","110/110 [==============================] - 33s 266ms/step - loss: 0.5296 - accuracy: 0.7349 - val_loss: 0.4141 - val_accuracy: 0.8225\n","Epoch 2/10\n","110/110 [==============================] - 28s 255ms/step - loss: 0.3327 - accuracy: 0.8623 - val_loss: 0.2861 - val_accuracy: 0.8832\n","Epoch 3/10\n","110/110 [==============================] - 28s 252ms/step - loss: 0.2271 - accuracy: 0.9114 - val_loss: 0.2295 - val_accuracy: 0.9074\n","Epoch 4/10\n","110/110 [==============================] - 28s 255ms/step - loss: 0.1572 - accuracy: 0.9415 - val_loss: 0.2017 - val_accuracy: 0.9202\n","Epoch 5/10\n","110/110 [==============================] - 28s 255ms/step - loss: 0.1050 - accuracy: 0.9650 - val_loss: 0.2445 - val_accuracy: 0.9145\n","Epoch 6/10\n","110/110 [==============================] - 28s 252ms/step - loss: 0.0725 - accuracy: 0.9756 - val_loss: 0.2240 - val_accuracy: 0.9231\n","Epoch 7/10\n","110/110 [==============================] - 28s 253ms/step - loss: 0.0501 - accuracy: 0.9841 - val_loss: 0.2459 - val_accuracy: 0.9154\n","Epoch 8/10\n","110/110 [==============================] - 28s 253ms/step - loss: 0.0379 - accuracy: 0.9880 - val_loss: 0.2747 - val_accuracy: 0.9256\n","Epoch 9/10\n","110/110 [==============================] - 28s 253ms/step - loss: 0.0276 - accuracy: 0.9914 - val_loss: 0.2391 - val_accuracy: 0.9288\n","Epoch 10/10\n","110/110 [==============================] - 28s 252ms/step - loss: 0.0279 - accuracy: 0.9920 - val_loss: 0.2868 - val_accuracy: 0.9202\n","110/110 [==============================] - 4s 31ms/step\n","Accuracy: 0.9202051866628669\n","Precision: 0.9504260263361735\n","Recall: 0.8503118503118503\n","F1-Score: 0.8975859546452084\n"]}],"source":["import numpy as np\n","import pandas as pd\n","from keras.preprocessing.text import Tokenizer\n","from keras_preprocessing.sequence import pad_sequences\n","from keras.layers import Embedding, Conv1D, MaxPooling1D, Bidirectional, LSTM, Dense, Dropout\n","from keras.models import Sequential\n","from sklearn.model_selection import train_test_split\n","from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n","\n","# Load the data from CSV file\n","df = pd.read_csv('/content/drive/MyDrive/New/Clean/FakeNews_clean.csv')\n","\n","MAX_NB_WORDS = 50000  # Maximum number of words to be used in the tokenizer\n","VALIDATION_SPLIT = 0.2\n","\n","# Split data into statements and labels\n","statements = df['statement'].astype(str)\n","labels = df['label']\n","\n","# Tokenize statements\n","tokenizer = Tokenizer()\n","tokenizer.fit_on_texts(statements)\n","word_index = tokenizer.word_index\n","sequences = tokenizer.texts_to_sequences(statements)\n","print('Found %s unique tokens.' % len(word_index))\n","\n","# Pad sequences\n","Max_Len = max([len(x) for x in statements])\n","padded_sequences = pad_sequences(sequences, maxlen=Max_Len)\n","\n","print('Shape of data tensor:', padded_sequences.shape)\n","\n","# Define the maximum number of words to keep in the vocabulary\n","MAX_NUM_WORDS = 20000\n","\n","# Convert the labels to one-hot encoding\n","y = df['label']\n","print('Shape of label tensor:', y.shape)\n","\n","# Split data into training and testing sets\n","X_train, X_test, y_train, y_test = train_test_split(padded_sequences, df['label'].values, test_size=0.2)\n","\n","\n","# Load the GloVe embeddings\n","embeddings_index = {}\n","f = open('/content/drive/MyDrive/Colab Notebooks/weights/glove.6B.300d.txt', encoding='utf8')\n","for line in f:\n","    values = line.split()\n","    word = values[0]\n","    coefs = np.asarray(values[1:], dtype='float32')\n","    embeddings_index[word] = coefs\n","f.close()\n","\n","# Create an embedding matrix\n","embedding_dim = 300\n","word_index = tokenizer.word_index\n","num_words = min(MAX_NUM_WORDS, len(word_index) + 1)\n","embedding_matrix = np.zeros((num_words, embedding_dim))\n","for word, i in word_index.items():\n","    if i >= MAX_NUM_WORDS:\n","        continue\n","    embedding_vector = embeddings_index.get(word)\n","    if embedding_vector is not None:\n","        embedding_matrix[i] = embedding_vector\n","\n","# Build the model\n","model = Sequential()\n","model.add(Embedding(input_dim=num_words, output_dim=embedding_dim, input_length=Max_Len, weights=[embedding_matrix], trainable=False))\n","model.add(Conv1D(128, 5, activation='relu'))\n","model.add(Dropout(0.3))\n","model.add(MaxPooling1D(2))\n","model.add(Conv1D(128, 5, activation='relu'))\n","model.add(MaxPooling1D(2))\n","model.add(Bidirectional(LSTM(32)))\n","\n","model.add(Dense(1, activation='sigmoid'))\n","\n","# Compile the model\n","model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n","print(model.summary())\n","\n","# Train the model\n","history = model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=10, batch_size=128)\n","\n","# Evaluate the model\n","y_pred = model.predict(X_test)\n","y_pred = (y_pred > 0.5).astype(int)\n","\n","# Calculate the evaluation metrics\n","accuracy = accuracy_score(y_test, y_pred)\n","precision = precision_score(y_test, y_pred)\n","recall = recall_score(y_test, y_pred)\n","f1 = f1_score(y_test, y_pred)\n","\n","print(\"Accuracy:\", accuracy)\n","print(\"Precision:\", precision)\n","print(\"Recall:\", recall)\n","print(\"F1-Score:\", f1)\n","\n","# Save the model\n","model.save('/content/drive/MyDrive/Colab Notebooks/weights/FN/glove6b300_fn.h5')\n","\n","# Load the saved model\n","# loaded_model = tf.keras.models.load_model('lstm_fn.h5')\n"]},{"cell_type":"code","source":["import numpy as np\n","import pandas as pd\n","from keras.preprocessing.text import Tokenizer\n","from keras_preprocessing.sequence import pad_sequences\n","from keras.layers import Embedding, Conv1D, MaxPooling1D, Bidirectional, LSTM, Dense, Dropout\n","from keras.models import Sequential\n","from sklearn.model_selection import train_test_split\n","from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n","\n","# Load the data from CSV file\n","df = pd.read_csv('/content/drive/MyDrive/New/Clean/FakeNews_clean.csv')\n","\n","MAX_NB_WORDS = 50000  # Maximum number of words to be used in the tokenizer\n","VALIDATION_SPLIT = 0.2\n","\n","# Split data into statements and labels\n","statements = df['statement'].astype(str)\n","labels = df['label']\n","\n","# Tokenize statements\n","tokenizer = Tokenizer()\n","tokenizer.fit_on_texts(statements)\n","word_index = tokenizer.word_index\n","sequences = tokenizer.texts_to_sequences(statements)\n","print('Found %s unique tokens.' % len(word_index))\n","\n","# Pad sequences\n","Max_Len = max([len(x) for x in statements])\n","padded_sequences = pad_sequences(sequences, maxlen=Max_Len)\n","\n","print('Shape of data tensor:', padded_sequences.shape)\n","\n","# Define the maximum number of words to keep in the vocabulary\n","MAX_NUM_WORDS = 20000\n","\n","# Convert the labels to one-hot encoding\n","y = df['label']\n","print('Shape of label tensor:', y.shape)\n","\n","# Split data into training and testing sets\n","X_train, X_test, y_train, y_test = train_test_split(padded_sequences, df['label'].values, test_size=0.2)\n","\n","\n","# Load the GloVe embeddings\n","embeddings_index = {}\n","f = open('/content/drive/MyDrive/Colab Notebooks/weights/glove.42B.300d.txt', encoding='utf8')\n","for line in f:\n","    values = line.split()\n","    word = values[0]\n","    coefs = np.asarray(values[1:], dtype='float32')\n","    embeddings_index[word] = coefs\n","f.close()\n","\n","# Create an embedding matrix\n","embedding_dim = 300\n","word_index = tokenizer.word_index\n","num_words = min(MAX_NUM_WORDS, len(word_index) + 1)\n","embedding_matrix = np.zeros((num_words, embedding_dim))\n","for word, i in word_index.items():\n","    if i >= MAX_NUM_WORDS:\n","        continue\n","    embedding_vector = embeddings_index.get(word)\n","    if embedding_vector is not None:\n","        embedding_matrix[i] = embedding_vector\n","\n","# Build the model\n","model = Sequential()\n","model.add(Embedding(input_dim=num_words, output_dim=embedding_dim, input_length=Max_Len, weights=[embedding_matrix], trainable=False))\n","model.add(Conv1D(128, 5, activation='relu'))\n","model.add(Dropout(0.3))\n","model.add(MaxPooling1D(2))\n","model.add(Conv1D(128, 5, activation='relu'))\n","model.add(MaxPooling1D(2))\n","model.add(Bidirectional(LSTM(32)))\n","\n","model.add(Dense(1, activation='sigmoid'))\n","\n","# Compile the model\n","model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n","print(model.summary())\n","\n","# Train the model\n","history = model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=10, batch_size=128)\n","\n","# Evaluate the model\n","y_pred = model.predict(X_test)\n","y_pred = (y_pred > 0.5).astype(int)\n","\n","# Calculate the evaluation metrics\n","accuracy = accuracy_score(y_test, y_pred)\n","precision = precision_score(y_test, y_pred)\n","recall = recall_score(y_test, y_pred)\n","f1 = f1_score(y_test, y_pred)\n","\n","print(\"Accuracy:\", accuracy)\n","print(\"Precision:\", precision)\n","print(\"Recall:\", recall)\n","print(\"F1-Score:\", f1)\n","\n","# Save the model\n","model.save('/content/drive/MyDrive/Colab Notebooks/weights/FN/glove42b300_fn.h5')\n","\n","# Load the saved model\n","# loaded_model = tf.keras.models.load_model('lstm_fn.h5')\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"NeXUxyu0jsUQ","executionInfo":{"status":"ok","timestamp":1680534952579,"user_tz":-330,"elapsed":443943,"user":{"displayName":"Ramacharan Reddy Kasireddy","userId":"17080325997413282440"}},"outputId":"a2e18c05-9584-4d7d-b866-acac28cba14f"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Found 30666 unique tokens.\n","Shape of data tensor: (17544, 3498)\n","Shape of label tensor: (17544,)\n","Model: \"sequential_7\"\n","_________________________________________________________________\n"," Layer (type)                Output Shape              Param #   \n","=================================================================\n"," embedding_7 (Embedding)     (None, 3498, 300)         6000000   \n","                                                                 \n"," conv1d_10 (Conv1D)          (None, 3494, 128)         192128    \n","                                                                 \n"," dropout_5 (Dropout)         (None, 3494, 128)         0         \n","                                                                 \n"," max_pooling1d_10 (MaxPoolin  (None, 1747, 128)        0         \n"," g1D)                                                            \n","                                                                 \n"," conv1d_11 (Conv1D)          (None, 1743, 128)         82048     \n","                                                                 \n"," max_pooling1d_11 (MaxPoolin  (None, 871, 128)         0         \n"," g1D)                                                            \n","                                                                 \n"," bidirectional_7 (Bidirectio  (None, 64)               41216     \n"," nal)                                                            \n","                                                                 \n"," dense_7 (Dense)             (None, 1)                 65        \n","                                                                 \n","=================================================================\n","Total params: 6,315,457\n","Trainable params: 315,457\n","Non-trainable params: 6,000,000\n","_________________________________________________________________\n","None\n","Epoch 1/10\n","110/110 [==============================] - 34s 270ms/step - loss: 0.5259 - accuracy: 0.7437 - val_loss: 0.4322 - val_accuracy: 0.7962\n","Epoch 2/10\n","110/110 [==============================] - 28s 253ms/step - loss: 0.4040 - accuracy: 0.8211 - val_loss: 0.4586 - val_accuracy: 0.7837\n","Epoch 3/10\n","110/110 [==============================] - 28s 253ms/step - loss: 0.2934 - accuracy: 0.8806 - val_loss: 0.2428 - val_accuracy: 0.9085\n","Epoch 4/10\n","110/110 [==============================] - 28s 256ms/step - loss: 0.2021 - accuracy: 0.9265 - val_loss: 0.2075 - val_accuracy: 0.9219\n","Epoch 5/10\n","110/110 [==============================] - 28s 253ms/step - loss: 0.1585 - accuracy: 0.9429 - val_loss: 0.2121 - val_accuracy: 0.9156\n","Epoch 6/10\n","110/110 [==============================] - 28s 252ms/step - loss: 0.1128 - accuracy: 0.9626 - val_loss: 0.1947 - val_accuracy: 0.9216\n","Epoch 7/10\n","110/110 [==============================] - 28s 253ms/step - loss: 0.0729 - accuracy: 0.9772 - val_loss: 0.1786 - val_accuracy: 0.9302\n","Epoch 8/10\n","110/110 [==============================] - 28s 253ms/step - loss: 0.0458 - accuracy: 0.9873 - val_loss: 0.1884 - val_accuracy: 0.9285\n","Epoch 9/10\n","110/110 [==============================] - 28s 252ms/step - loss: 0.0430 - accuracy: 0.9862 - val_loss: 0.1944 - val_accuracy: 0.9330\n","Epoch 10/10\n","110/110 [==============================] - 28s 252ms/step - loss: 0.0287 - accuracy: 0.9922 - val_loss: 0.2032 - val_accuracy: 0.9345\n","110/110 [==============================] - 4s 31ms/step\n","Accuracy: 0.9344542604730692\n","Precision: 0.9284722222222223\n","Recall: 0.9132513661202186\n","F1-Score: 0.9207988980716253\n"]}]},{"cell_type":"markdown","source":["# Word2Vec"],"metadata":{"id":"BTpjjzcl2061"}},{"cell_type":"code","source":["import numpy as np\n","import pandas as pd\n","from tensorflow.keras.preprocessing.text import Tokenizer\n","from tensorflow.keras.preprocessing.sequence import pad_sequences\n","from keras.layers import Embedding, Conv1D, MaxPooling1D, Bidirectional, LSTM, Dense, Dropout\n","from keras.models import Sequential\n","from sklearn.model_selection import train_test_split\n","from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n","\n","\n","# Load the data from CSV file\n","df = pd.read_csv('/content/drive/MyDrive/New/Clean/FakeNews_clean.csv')\n","\n","MAX_NB_WORDS = 50000  # Maximum number of words to be used in the tokenizer\n","VALIDATION_SPLIT = 0.2\n","\n","# Split data into statements and labels\n","statements = df['statement'].astype(str)\n","labels = df['label']\n","\n","# Tokenize statements\n","tokenizer = Tokenizer()\n","tokenizer.fit_on_texts(statements)\n","word_index = tokenizer.word_index\n","sequences = tokenizer.texts_to_sequences(statements)\n","\n","# Pad sequences\n","Max_Len = max([len(x) for x in statements])\n","padded_sequences = pad_sequences(sequences, maxlen=Max_Len)\n","\n","print('Shape of data tensor:', padded_sequences.shape)\n","\n","# Define the maximum number of words to keep in the vocabulary\n","MAX_NUM_WORDS = 20000\n","\n","# Convert the labels to one-hot encoding\n","y = df['label']\n","print('Shape of label tensor:', y.shape)\n","\n","# Split data into training and testing sets\n","X_train, X_test, y_train, y_test = train_test_split(padded_sequences, df['label'].values, test_size=0.2)\n","\n","# Load the Word2Vec embeddings\n","from gensim.models.word2vec import Word2Vec\n","import gensim.downloader as api\n","\n","# Load the pre-trained Word2Vec model\n","w2v_model = api.load('word2vec-google-news-300')\n","\n","# Create an embedding matrix\n","embedding_dim = 300\n","word_index = tokenizer.word_index\n","num_words = min(MAX_NUM_WORDS, len(word_index) + 1)\n","embedding_matrix = np.zeros((num_words, embedding_dim))\n","for word, i in word_index.items():\n","    if i >= MAX_NUM_WORDS:\n","        continue\n","    if word in w2v_model:\n","        embedding_vector = w2v_model[word]\n","        embedding_matrix[i] = embedding_vector\n","\n","# Build the model\n","model = Sequential()\n","model.add(Embedding(input_dim=num_words, output_dim=embedding_dim, input_length=Max_Len, weights=[embedding_matrix], trainable=False))\n","model.add(Conv1D(128, 5, activation='relu'))\n","model.add(Dropout(0.3))\n","model.add(MaxPooling1D(2))\n","model.add(Conv1D(128, 5, activation='relu'))\n","model.add(MaxPooling1D(2))\n","model.add(Bidirectional(LSTM(32)))\n","\n","model.add(Dense(1, activation='sigmoid'))\n","\n","# Compile the model\n","model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n","print(model.summary())\n","\n","# Train the model\n","history = model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=10, batch_size=128)\n","\n","# Evaluate the model\n","y_pred = model.predict(X_test)\n","y_pred = (y_pred > 0.5).astype(int)\n","\n","# Calculate the evaluation metrics\n","accuracy = accuracy_score(y_test, y_pred)\n","precision = precision_score(y_test, y_pred)\n","recall = recall_score(y_test, y_pred)\n","f1 = f1_score(y_test, y_pred)\n","\n","print(\"Accuracy:\", accuracy)\n","print(\"Precision:\", precision)\n","print(\"Recall:\", recall)\n","print(\"F1-Score:\", f1)\n","\n","# Save the model\n","model.save('/content/drive/MyDrive/Colab Notebooks/weights/FN/word2vec_fn.h5')\n","\n","# Load the saved model\n","# loaded_model = tf.keras.models.load_model('lstm_fn.h5')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"NK08lVHV1Ky_","executionInfo":{"status":"ok","timestamp":1680539515378,"user_tz":-330,"elapsed":671767,"user":{"displayName":"Ramacharan Reddy Kasireddy","userId":"17080325997413282440"}},"outputId":"80c44e15-6f0d-4876-ff79-42e91676d5ad"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Shape of data tensor: (17544, 3498)\n","Shape of label tensor: (17544,)\n","[==================================================] 100.0% 1662.8/1662.8MB downloaded\n","Model: \"sequential_9\"\n","_________________________________________________________________\n"," Layer (type)                Output Shape              Param #   \n","=================================================================\n"," embedding_9 (Embedding)     (None, 3498, 300)         6000000   \n","                                                                 \n"," conv1d_14 (Conv1D)          (None, 3494, 128)         192128    \n","                                                                 \n"," dropout_7 (Dropout)         (None, 3494, 128)         0         \n","                                                                 \n"," max_pooling1d_14 (MaxPoolin  (None, 1747, 128)        0         \n"," g1D)                                                            \n","                                                                 \n"," conv1d_15 (Conv1D)          (None, 1743, 128)         82048     \n","                                                                 \n"," max_pooling1d_15 (MaxPoolin  (None, 871, 128)         0         \n"," g1D)                                                            \n","                                                                 \n"," bidirectional_9 (Bidirectio  (None, 64)               41216     \n"," nal)                                                            \n","                                                                 \n"," dense_9 (Dense)             (None, 1)                 65        \n","                                                                 \n","=================================================================\n","Total params: 6,315,457\n","Trainable params: 315,457\n","Non-trainable params: 6,000,000\n","_________________________________________________________________\n","None\n","Epoch 1/10\n","110/110 [==============================] - 33s 267ms/step - loss: 0.5583 - accuracy: 0.7112 - val_loss: 0.4523 - val_accuracy: 0.7979\n","Epoch 2/10\n","110/110 [==============================] - 28s 252ms/step - loss: 0.4218 - accuracy: 0.8125 - val_loss: 0.4489 - val_accuracy: 0.8042\n","Epoch 3/10\n","110/110 [==============================] - 28s 251ms/step - loss: 0.3053 - accuracy: 0.8742 - val_loss: 0.2745 - val_accuracy: 0.8857\n","Epoch 4/10\n","110/110 [==============================] - 28s 254ms/step - loss: 0.2269 - accuracy: 0.9121 - val_loss: 0.2569 - val_accuracy: 0.8920\n","Epoch 5/10\n","110/110 [==============================] - 28s 252ms/step - loss: 0.1779 - accuracy: 0.9286 - val_loss: 0.2036 - val_accuracy: 0.9154\n","Epoch 6/10\n","110/110 [==============================] - 28s 251ms/step - loss: 0.1253 - accuracy: 0.9578 - val_loss: 0.1709 - val_accuracy: 0.9336\n","Epoch 7/10\n","110/110 [==============================] - 28s 251ms/step - loss: 0.0896 - accuracy: 0.9695 - val_loss: 0.2052 - val_accuracy: 0.9268\n","Epoch 8/10\n","110/110 [==============================] - 28s 251ms/step - loss: 0.0696 - accuracy: 0.9772 - val_loss: 0.1729 - val_accuracy: 0.9313\n","Epoch 9/10\n","110/110 [==============================] - 28s 251ms/step - loss: 0.0449 - accuracy: 0.9864 - val_loss: 0.3457 - val_accuracy: 0.9040\n","Epoch 10/10\n","110/110 [==============================] - 28s 251ms/step - loss: 0.0406 - accuracy: 0.9867 - val_loss: 0.2070 - val_accuracy: 0.9245\n","110/110 [==============================] - 4s 30ms/step\n","Accuracy: 0.9244799088059276\n","Precision: 0.9605067064083458\n","Recall: 0.8587608261159227\n","F1-Score: 0.9067886035877594\n"]}]},{"cell_type":"markdown","source":["# FastText"],"metadata":{"id":"2OwJyZaC2t9R"}},{"cell_type":"code","source":["import numpy as np\n","import pandas as pd\n","from tensorflow.keras.preprocessing.text import Tokenizer\n","from tensorflow.keras.preprocessing.sequence import pad_sequences\n","from keras.layers import Embedding, Conv1D, MaxPooling1D, Bidirectional, LSTM, Dense, Dropout\n","from keras.models import Sequential\n","from sklearn.model_selection import train_test_split\n","from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n","\n","# Load the data from CSV file\n","df = pd.read_csv('/content/drive/MyDrive/New/Clean/FakeNews_clean.csv')\n","\n","MAX_NB_WORDS = 50000  # Maximum number of words to be used in the tokenizer\n","VALIDATION_SPLIT = 0.2\n","\n","# Split data into statements and labels\n","statements = df['statement'].astype(str)\n","labels = df['label']\n","\n","# Tokenize statements\n","tokenizer = Tokenizer()\n","tokenizer.fit_on_texts(statements)\n","word_index = tokenizer.word_index\n","sequences = tokenizer.texts_to_sequences(statements)\n","\n","# Pad sequences\n","Max_Len = max([len(x) for x in statements])\n","padded_sequences = pad_sequences(sequences, maxlen=Max_Len)\n","\n","print('Shape of data tensor:', padded_sequences.shape)\n","\n","# Define the maximum number of words to keep in the vocabulary\n","MAX_NUM_WORDS = 20000\n","\n","# Convert the labels to one-hot encoding\n","y = df['label']\n","print('Shape of label tensor:', y.shape)\n","\n","# Split data into training and testing sets\n","X_train, X_test, y_train, y_test = train_test_split(padded_sequences, df['label'].values, test_size=0.2)\n","\n","# Load the Word2Vec embeddings\n","from gensim.models.word2vec import Word2Vec\n","import gensim.downloader as api\n","\n","# Load the pre-trained Word2Vec model\n","ft_model = api.load('fasttext-wiki-news-subwords-300')\n","\n","# Create an embedding matrix\n","embedding_dim = 300\n","word_index = tokenizer.word_index\n","num_words = min(MAX_NUM_WORDS, len(word_index) + 1)\n","embedding_matrix = np.zeros((num_words, embedding_dim))\n","for word, i in word_index.items():\n","    if i >= MAX_NUM_WORDS:\n","        continue\n","    if word in ft_model:\n","        embedding_vector = ft_model[word]\n","        embedding_matrix[i] = embedding_vector\n","\n","# Build the model\n","model = Sequential()\n","model.add(Embedding(input_dim=num_words, output_dim=embedding_dim, input_length=Max_Len, weights=[embedding_matrix], trainable=False))\n","model.add(Conv1D(128, 5, activation='relu'))\n","model.add(Dropout(0.3))\n","model.add(MaxPooling1D(2))\n","model.add(Conv1D(128, 5, activation='relu'))\n","model.add(MaxPooling1D(2))\n","model.add(Bidirectional(LSTM(32)))\n","model.add(Dense(1, activation='sigmoid'))\n","\n","# Compile the model\n","model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n","print(model.summary())\n","\n","# Train the model\n","history = model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=10, batch_size=128)\n","\n","# Evaluate the model\n","y_pred = model.predict(X_test)\n","y_pred = (y_pred > 0.5).astype(int)\n","\n","# Calculate the evaluation metrics\n","accuracy = accuracy_score(y_test, y_pred)\n","precision = precision_score(y_test, y_pred)\n","recall = recall_score(y_test, y_pred)\n","f1 = f1_score(y_test, y_pred)\n","\n","print(\"Accuracy:\", accuracy)\n","print(\"Precision:\", precision)\n","print(\"Recall:\", recall)\n","print(\"F1-Score:\", f1)\n","\n","# Save the model\n","model.save('/content/drive/MyDrive/Colab Notebooks/weights/FN/ft_fn.h5')\n","\n","# Load the saved model\n","# loaded_model = tf.keras.models.load_model('lstm_fn.h5')\n"],"metadata":{"id":"tNjlQJ1GBEu7","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1680537980491,"user_tz":-330,"elapsed":645806,"user":{"displayName":"Ramacharan Reddy Kasireddy","userId":"17080325997413282440"}},"outputId":"238b9e4e-d680-4df1-f1bb-89cd7aec779a"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Shape of data tensor: (17544, 3498)\n","Shape of label tensor: (17544,)\n","[==================================================] 100.0% 958.5/958.4MB downloaded\n","Model: \"sequential_8\"\n","_________________________________________________________________\n"," Layer (type)                Output Shape              Param #   \n","=================================================================\n"," embedding_8 (Embedding)     (None, 3498, 300)         6000000   \n","                                                                 \n"," conv1d_12 (Conv1D)          (None, 3494, 128)         192128    \n","                                                                 \n"," dropout_6 (Dropout)         (None, 3494, 128)         0         \n","                                                                 \n"," max_pooling1d_12 (MaxPoolin  (None, 1747, 128)        0         \n"," g1D)                                                            \n","                                                                 \n"," conv1d_13 (Conv1D)          (None, 1743, 128)         82048     \n","                                                                 \n"," max_pooling1d_13 (MaxPoolin  (None, 871, 128)         0         \n"," g1D)                                                            \n","                                                                 \n"," bidirectional_8 (Bidirectio  (None, 64)               41216     \n"," nal)                                                            \n","                                                                 \n"," dense_8 (Dense)             (None, 1)                 65        \n","                                                                 \n","=================================================================\n","Total params: 6,315,457\n","Trainable params: 315,457\n","Non-trainable params: 6,000,000\n","_________________________________________________________________\n","None\n","Epoch 1/10\n","110/110 [==============================] - 34s 268ms/step - loss: 0.6066 - accuracy: 0.6757 - val_loss: 0.5484 - val_accuracy: 0.7484\n","Epoch 2/10\n","110/110 [==============================] - 28s 254ms/step - loss: 0.4612 - accuracy: 0.7940 - val_loss: 0.3989 - val_accuracy: 0.8341\n","Epoch 3/10\n","110/110 [==============================] - 28s 253ms/step - loss: 0.3743 - accuracy: 0.8410 - val_loss: 0.3362 - val_accuracy: 0.8629\n","Epoch 4/10\n","110/110 [==============================] - 28s 256ms/step - loss: 0.3109 - accuracy: 0.8714 - val_loss: 0.2715 - val_accuracy: 0.8880\n","Epoch 5/10\n","110/110 [==============================] - 28s 254ms/step - loss: 0.2464 - accuracy: 0.9030 - val_loss: 0.2514 - val_accuracy: 0.8963\n","Epoch 6/10\n","110/110 [==============================] - 28s 252ms/step - loss: 0.2082 - accuracy: 0.9201 - val_loss: 0.2118 - val_accuracy: 0.9142\n","Epoch 7/10\n","110/110 [==============================] - 28s 253ms/step - loss: 0.1689 - accuracy: 0.9392 - val_loss: 0.1831 - val_accuracy: 0.9285\n","Epoch 8/10\n","110/110 [==============================] - 28s 253ms/step - loss: 0.1477 - accuracy: 0.9472 - val_loss: 0.2268 - val_accuracy: 0.9048\n","Epoch 9/10\n","110/110 [==============================] - 28s 253ms/step - loss: 0.1313 - accuracy: 0.9525 - val_loss: 0.1829 - val_accuracy: 0.9330\n","Epoch 10/10\n","110/110 [==============================] - 28s 253ms/step - loss: 0.0879 - accuracy: 0.9709 - val_loss: 0.1932 - val_accuracy: 0.9330\n","110/110 [==============================] - 4s 30ms/step\n","Accuracy: 0.933029353092049\n","Precision: 0.9550038789759504\n","Recall: 0.8742897727272727\n","F1-Score: 0.9128661475713755\n"]}]}],"metadata":{"accelerator":"GPU","colab":{"collapsed_sections":["G-GAEKWh41AQ","RrPTHD0tEkw6","SB2BWfsxFOPo"],"machine_shape":"hm","provenance":[{"file_id":"195VpL2hxfaVbGUg5vKzJWOjcrZszboc6","timestamp":1676873879368}],"toc_visible":true},"gpuClass":"standard","kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"},"vscode":{"interpreter":{"hash":"aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"}}},"nbformat":4,"nbformat_minor":0}