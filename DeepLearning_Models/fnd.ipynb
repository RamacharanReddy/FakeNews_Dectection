{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"OVUe9CXD5b8g","executionInfo":{"status":"ok","timestamp":1681668563659,"user_tz":-330,"elapsed":6822,"user":{"displayName":"Ramacharan Reddy Kasireddy","userId":"17080325997413282440"}},"outputId":"8380b9cd-b9f7-443e-8e6d-2f7e2d01c4bb"},"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Requirement already satisfied: tensorflow in /usr/local/lib/python3.9/dist-packages (2.12.0)\n","Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.9/dist-packages (from tensorflow) (16.0.0)\n","Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.9/dist-packages (from tensorflow) (3.3.0)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.9/dist-packages (from tensorflow) (23.0)\n","Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.9/dist-packages (from tensorflow) (1.4.0)\n","Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.9/dist-packages (from tensorflow) (0.2.0)\n","Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.9/dist-packages (from tensorflow) (1.6.3)\n","Requirement already satisfied: gast<=0.4.0,>=0.2.1 in /usr/local/lib/python3.9/dist-packages (from tensorflow) (0.4.0)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.9/dist-packages (from tensorflow) (67.6.1)\n","Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.9/dist-packages (from tensorflow) (0.32.0)\n","Requirement already satisfied: wrapt<1.15,>=1.11.0 in /usr/local/lib/python3.9/dist-packages (from tensorflow) (1.14.1)\n","Requirement already satisfied: tensorflow-estimator<2.13,>=2.12.0 in /usr/local/lib/python3.9/dist-packages (from tensorflow) (2.12.0)\n","Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.9/dist-packages (from tensorflow) (4.5.0)\n","Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.9/dist-packages (from tensorflow) (2.2.0)\n","Requirement already satisfied: tensorboard<2.13,>=2.12 in /usr/local/lib/python3.9/dist-packages (from tensorflow) (2.12.1)\n","Requirement already satisfied: keras<2.13,>=2.12.0 in /usr/local/lib/python3.9/dist-packages (from tensorflow) (2.12.0)\n","Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.9/dist-packages (from tensorflow) (3.8.0)\n","Requirement already satisfied: flatbuffers>=2.0 in /usr/local/lib/python3.9/dist-packages (from tensorflow) (23.3.3)\n","Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /usr/local/lib/python3.9/dist-packages (from tensorflow) (3.20.3)\n","Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.9/dist-packages (from tensorflow) (1.16.0)\n","Requirement already satisfied: jax>=0.3.15 in /usr/local/lib/python3.9/dist-packages (from tensorflow) (0.4.8)\n","Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.9/dist-packages (from tensorflow) (1.53.0)\n","Requirement already satisfied: numpy<1.24,>=1.22 in /usr/local/lib/python3.9/dist-packages (from tensorflow) (1.22.4)\n","Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.9/dist-packages (from astunparse>=1.6.0->tensorflow) (0.40.0)\n","Requirement already satisfied: scipy>=1.7 in /usr/local/lib/python3.9/dist-packages (from jax>=0.3.15->tensorflow) (1.10.1)\n","Requirement already satisfied: ml-dtypes>=0.0.3 in /usr/local/lib/python3.9/dist-packages (from jax>=0.3.15->tensorflow) (0.0.4)\n","Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.9/dist-packages (from tensorboard<2.13,>=2.12->tensorflow) (2.17.2)\n","Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.9/dist-packages (from tensorboard<2.13,>=2.12->tensorflow) (2.2.3)\n","Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.9/dist-packages (from tensorboard<2.13,>=2.12->tensorflow) (2.27.1)\n","Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.9/dist-packages (from tensorboard<2.13,>=2.12->tensorflow) (1.8.1)\n","Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.9/dist-packages (from tensorboard<2.13,>=2.12->tensorflow) (0.7.0)\n","Requirement already satisfied: google-auth-oauthlib<1.1,>=0.5 in /usr/local/lib/python3.9/dist-packages (from tensorboard<2.13,>=2.12->tensorflow) (1.0.0)\n","Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.9/dist-packages (from tensorboard<2.13,>=2.12->tensorflow) (3.4.3)\n","Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.9/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow) (0.2.8)\n","Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.9/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow) (4.9)\n","Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.9/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow) (5.3.0)\n","Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.9/dist-packages (from google-auth-oauthlib<1.1,>=0.5->tensorboard<2.13,>=2.12->tensorflow) (1.3.1)\n","Requirement already satisfied: importlib-metadata>=4.4 in /usr/local/lib/python3.9/dist-packages (from markdown>=2.6.8->tensorboard<2.13,>=2.12->tensorflow) (6.3.0)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.9/dist-packages (from requests<3,>=2.21.0->tensorboard<2.13,>=2.12->tensorflow) (3.4)\n","Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests<3,>=2.21.0->tensorboard<2.13,>=2.12->tensorflow) (1.26.15)\n","Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.9/dist-packages (from requests<3,>=2.21.0->tensorboard<2.13,>=2.12->tensorflow) (2.0.12)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.9/dist-packages (from requests<3,>=2.21.0->tensorboard<2.13,>=2.12->tensorflow) (2022.12.7)\n","Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.9/dist-packages (from werkzeug>=1.0.1->tensorboard<2.13,>=2.12->tensorflow) (2.1.2)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.9/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<2.13,>=2.12->tensorflow) (3.15.0)\n","Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.9/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow) (0.4.8)\n","Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.9/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<1.1,>=0.5->tensorboard<2.13,>=2.12->tensorflow) (3.2.2)\n"]}],"source":["!pip install tensorflow"]},{"cell_type":"code","source":["!pip install Keras-Preprocessing"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"6Dd1ZD6RFl82","executionInfo":{"status":"ok","timestamp":1681668567719,"user_tz":-330,"elapsed":4063,"user":{"displayName":"Ramacharan Reddy Kasireddy","userId":"17080325997413282440"}},"outputId":"1f05e603-d40f-43d3-f329-3e125c111413"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting Keras-Preprocessing\n","  Downloading Keras_Preprocessing-1.1.2-py2.py3-none-any.whl (42 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.6/42.6 kB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.9/dist-packages (from Keras-Preprocessing) (1.16.0)\n","Requirement already satisfied: numpy>=1.9.1 in /usr/local/lib/python3.9/dist-packages (from Keras-Preprocessing) (1.22.4)\n","Installing collected packages: Keras-Preprocessing\n","Successfully installed Keras-Preprocessing-1.1.2\n"]}]},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"KQVP5KQC8hyh","executionInfo":{"status":"ok","timestamp":1681668586695,"user_tz":-330,"elapsed":18979,"user":{"displayName":"Ramacharan Reddy Kasireddy","userId":"17080325997413282440"}},"outputId":"865f7d77-38f9-42d0-efb0-e0c3eded132d"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"markdown","source":["# Linear SVM"],"metadata":{"id":"nLhMkvjo6ENk"}},{"cell_type":"code","source":["# Importing required libraries\n","import pandas as pd\n","from sklearn.feature_extraction.text import TfidfVectorizer\n","from sklearn.model_selection import cross_val_score, cross_val_predict\n","from sklearn.metrics import accuracy_score, recall_score, precision_score, f1_score\n","from sklearn.svm import LinearSVC\n","\n","# Reading data into a pandas DataFrame\n","data = pd.read_csv (r'/content/drive/MyDrive/New/Clean/FakeNewsD_clean.csv')"],"metadata":{"id":"09blDvVG5pN4"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from sklearn.feature_extraction.text import TfidfVectorizer\n","tfidf_v = TfidfVectorizer(max_features=5000, ngram_range=(1,3))\n","X = tfidf_v.fit_transform(data['statement'].values.astype('U'))\n","y = data['label'].values"],"metadata":{"id":"CmqzkuG95p41"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import joblib\n","\n","# Creating a Linear SVM classifier\n","svm = LinearSVC()\n","\n","# Performing five-fold cross-validation\n","scores = cross_val_score(svm, X, y, cv=5)\n","\n","# Printing the cross-validation scores\n","print(\"Cross-validation scores:\", scores)\n","\n","# Computing and printing accuracy, recall, precision, and F1 score\n","y_pred = cross_val_predict(svm, X, y, cv=5)\n","accuracy = accuracy_score(y, y_pred)\n","recall = recall_score(y, y_pred)\n","precision = precision_score(y, y_pred)\n","f1 = f1_score(y, y_pred)\n","print(\"Accuracy:\", accuracy)\n","print(\"Recall:\", recall)\n","print(\"Precision:\", precision)\n","print(\"F1 score:\", f1)\n","\n","# Save the trained model to a file\n","joblib.dump(svm, '/content/drive/MyDrive/Colab Notebooks/weights/FND/linearsvc_fnd.pkl')\n"," "],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"u8dwOW4B5vN_","executionInfo":{"status":"ok","timestamp":1680090078592,"user_tz":-330,"elapsed":981,"user":{"displayName":"Ramacharan Reddy Kasireddy","userId":"17080325997413282440"}},"outputId":"63d0b087-fe2d-4afa-ea36-a7eebad79967"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Cross-validation scores: [0.97196262 0.97975078 0.97040498 0.98130841 0.97352025]\n","Accuracy: 0.9753894080996884\n","Recall: 0.9819136522753792\n","Precision: 0.9722703639514731\n","F1 score: 0.9770682148040638\n"]},{"output_type":"execute_result","data":{"text/plain":["['/content/drive/MyDrive/Colab Notebooks/weights/FND/linearsvc_fnd.pkl']"]},"metadata":{},"execution_count":18}]},{"cell_type":"markdown","source":["# LSTM\n"],"metadata":{"id":"RrPTHD0tEkw6"}},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"irUXCM2q5b8g","executionInfo":{"status":"ok","timestamp":1680089310619,"user_tz":-330,"elapsed":53054,"user":{"displayName":"Ramacharan Reddy Kasireddy","userId":"17080325997413282440"}},"outputId":"5feede55-1571-44e9-a2c1-1ebfcb404fd7"},"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 1/10\n","81/81 - 21s - loss: 0.4610 - accuracy: 0.8649 - val_loss: 0.1731 - val_accuracy: 0.9455 - 21s/epoch - 256ms/step\n","Epoch 2/10\n","81/81 - 8s - loss: 0.0919 - accuracy: 0.9747 - val_loss: 0.0913 - val_accuracy: 0.9673 - 8s/epoch - 93ms/step\n","Epoch 3/10\n","81/81 - 5s - loss: 0.0291 - accuracy: 0.9949 - val_loss: 0.0863 - val_accuracy: 0.9688 - 5s/epoch - 61ms/step\n","Epoch 4/10\n","81/81 - 3s - loss: 0.0123 - accuracy: 0.9981 - val_loss: 0.0752 - val_accuracy: 0.9704 - 3s/epoch - 35ms/step\n","Epoch 5/10\n","81/81 - 3s - loss: 0.0091 - accuracy: 0.9988 - val_loss: 0.1303 - val_accuracy: 0.9626 - 3s/epoch - 36ms/step\n","Epoch 6/10\n","81/81 - 3s - loss: 0.0228 - accuracy: 0.9934 - val_loss: 0.0698 - val_accuracy: 0.9751 - 3s/epoch - 37ms/step\n","Epoch 7/10\n","81/81 - 2s - loss: 0.0054 - accuracy: 0.9996 - val_loss: 0.1174 - val_accuracy: 0.9548 - 2s/epoch - 24ms/step\n","Epoch 8/10\n","81/81 - 2s - loss: 0.0023 - accuracy: 0.9996 - val_loss: 0.1103 - val_accuracy: 0.9673 - 2s/epoch - 26ms/step\n","Epoch 9/10\n","81/81 - 1s - loss: 9.6905e-04 - accuracy: 1.0000 - val_loss: 0.1412 - val_accuracy: 0.9657 - 1s/epoch - 14ms/step\n","Epoch 10/10\n","81/81 - 2s - loss: 7.6074e-04 - accuracy: 1.0000 - val_loss: 0.1582 - val_accuracy: 0.9642 - 2s/epoch - 26ms/step\n","21/21 [==============================] - 0s 3ms/step\n","Accuracy: 0.9641744548286605\n","Precision: 0.9770114942528736\n","Recall: 0.9577464788732394\n","F1-Score: 0.9672830725462305\n"]}],"source":["import numpy as np\n","import pandas as pd\n","import tensorflow as tf\n","from sklearn.model_selection import train_test_split\n","from tensorflow.keras.preprocessing.text import Tokenizer\n","from tensorflow.keras.preprocessing.sequence import pad_sequences\n","from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n","\n","df = pd.read_csv (r'/content/drive/MyDrive/New/Clean/FakeNewsD_clean.csv')\n","# Tokenize the text\n","tokenizer = Tokenizer(num_words=5000, oov_token=\"<OOV>\")\n","tokenizer.fit_on_texts(df['statement'].values)\n","word_index = tokenizer.word_index\n","sequences = tokenizer.texts_to_sequences(df['statement'].values)\n","padded = pad_sequences(sequences, maxlen=100)\n","\n","# Split the data into training and testing sets\n","X_train, X_test, y_train, y_test = train_test_split(padded, df['label'].values, test_size=0.2, random_state=0)\n","\n","# Build the model\n","model = tf.keras.Sequential([\n","    tf.keras.layers.Embedding(input_dim=5000, output_dim=32, input_length=100),\n","    tf.keras.layers.LSTM(32),\n","    tf.keras.layers.Dense(1, activation='sigmoid')\n","])\n","\n","# Compile the model\n","model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n","\n","# Train the model\n","history = model.fit(X_train, y_train, epochs=10, validation_data=(X_test, y_test), verbose=2)\n","\n","# Evaluate the model\n","y_pred = model.predict(X_test)\n","y_pred = (y_pred > 0.5).astype(int)\n","\n","# Calculate the evaluation metrics\n","accuracy = accuracy_score(y_test, y_pred)\n","precision = precision_score(y_test, y_pred)\n","recall = recall_score(y_test, y_pred)\n","f1 = f1_score(y_test, y_pred)\n","\n","print(\"Accuracy:\", accuracy)\n","print(\"Precision:\", precision)\n","print(\"Recall:\", recall)\n","print(\"F1-Score:\", f1)\n","\n","# Save the model\n","model.save('lstm_fnd.h5')\n","\n","# Load the saved model\n","# loaded_model = tf.keras.models.load_model('lstm_fn.h5')\n"]},{"cell_type":"markdown","source":["# BI LSTM\n"],"metadata":{"id":"SB2BWfsxFOPo"}},{"cell_type":"code","source":["import numpy as np\n","import pandas as pd\n","from keras.preprocessing.text import Tokenizer\n","from keras_preprocessing.sequence import pad_sequences\n","from keras.models import Sequential\n","from keras.layers import Dense, Embedding, LSTM, Bidirectional\n","from sklearn.model_selection import train_test_split\n","from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n","\n","# Load the data from CSV file\n","data = pd.read_csv('/content/drive/MyDrive/New/Clean/FakeNewsD_clean.csv')\n","\n","# Split data into statements and labels\n","statements = data['statement']\n","labels = data['label']\n","\n","# Tokenize statements\n","tokenizer = Tokenizer()\n","tokenizer.fit_on_texts(statements)\n","word_index = tokenizer.word_index\n","sequences = tokenizer.texts_to_sequences(statements)\n","\n","# Pad sequences\n","max_len = 100\n","padded_sequences = pad_sequences(sequences, maxlen=max_len)\n","\n","# Split data into training and testing sets\n","x_train, x_test, y_train, y_test = train_test_split(padded_sequences, labels, test_size=0.2)\n","\n","# Define model architecture\n","model = Sequential()\n","model.add(Embedding(len(word_index) + 1, 128, input_length=max_len))\n","model.add(Bidirectional(LSTM(64, return_sequences=True)))\n","model.add(Bidirectional(LSTM(32)))\n","model.add(Dense(1, activation='sigmoid'))\n","\n","# Compile model\n","model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n","print(model.summary())\n","\n","# Train model\n","model.fit(x_train, y_train, epochs=10, batch_size=32, validation_data=(x_test, y_test))\n","\n","# Evaluate the model\n","y_pred = model.predict(x_test)\n","y_pred = (y_pred > 0.5).astype(int)\n","\n","# Calculate the evaluation metrics\n","accuracy = accuracy_score(y_test, y_pred)\n","precision = precision_score(y_test, y_pred)\n","recall = recall_score(y_test, y_pred)\n","f1 = f1_score(y_test, y_pred)\n","\n","print(\"Accuracy:\", accuracy)\n","print(\"Precision:\", precision)\n","print(\"Recall:\", recall)\n","print(\"F1-Score:\", f1)\n","\n","# Save the model\n","model.save('bilstm_fnd.h5')\n","\n","# Load the saved model\n","# loaded_model = tf.keras.models.load_model('lstm_fn.h5')\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"gXMEaLBUzmti","executionInfo":{"status":"ok","timestamp":1680089419265,"user_tz":-330,"elapsed":68171,"user":{"displayName":"Ramacharan Reddy Kasireddy","userId":"17080325997413282440"}},"outputId":"6ec6b73c-4171-4645-df1e-b8d7ff8989ef"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Model: \"sequential_1\"\n","_________________________________________________________________\n"," Layer (type)                Output Shape              Param #   \n","=================================================================\n"," embedding_2 (Embedding)     (None, 100, 128)          2289920   \n","                                                                 \n"," bidirectional (Bidirectiona  (None, 100, 128)         98816     \n"," l)                                                              \n","                                                                 \n"," bidirectional_1 (Bidirectio  (None, 64)               41216     \n"," nal)                                                            \n","                                                                 \n"," dense_1 (Dense)             (None, 1)                 65        \n","                                                                 \n","=================================================================\n","Total params: 2,430,017\n","Trainable params: 2,430,017\n","Non-trainable params: 0\n","_________________________________________________________________\n","None\n","Epoch 1/10\n","81/81 [==============================] - 21s 182ms/step - loss: 0.3164 - accuracy: 0.8777 - val_loss: 0.1064 - val_accuracy: 0.9720\n","Epoch 2/10\n","81/81 [==============================] - 10s 118ms/step - loss: 0.0378 - accuracy: 0.9910 - val_loss: 0.1130 - val_accuracy: 0.9704\n","Epoch 3/10\n","81/81 [==============================] - 7s 82ms/step - loss: 0.0064 - accuracy: 0.9984 - val_loss: 0.1068 - val_accuracy: 0.9735\n","Epoch 4/10\n","81/81 [==============================] - 6s 74ms/step - loss: 0.0015 - accuracy: 1.0000 - val_loss: 0.2054 - val_accuracy: 0.9595\n","Epoch 5/10\n","81/81 [==============================] - 5s 63ms/step - loss: 0.0032 - accuracy: 0.9996 - val_loss: 0.1566 - val_accuracy: 0.9688\n","Epoch 6/10\n","81/81 [==============================] - 4s 52ms/step - loss: 5.6570e-04 - accuracy: 1.0000 - val_loss: 0.1652 - val_accuracy: 0.9704\n","Epoch 7/10\n","81/81 [==============================] - 3s 40ms/step - loss: 3.7430e-04 - accuracy: 1.0000 - val_loss: 0.1799 - val_accuracy: 0.9704\n","Epoch 8/10\n","81/81 [==============================] - 3s 36ms/step - loss: 2.7320e-04 - accuracy: 1.0000 - val_loss: 0.1873 - val_accuracy: 0.9704\n","Epoch 9/10\n","81/81 [==============================] - 3s 34ms/step - loss: 2.1156e-04 - accuracy: 1.0000 - val_loss: 0.1949 - val_accuracy: 0.9704\n","Epoch 10/10\n","81/81 [==============================] - 2s 29ms/step - loss: 1.6989e-04 - accuracy: 1.0000 - val_loss: 0.2055 - val_accuracy: 0.9688\n","21/21 [==============================] - 1s 7ms/step\n","Accuracy: 0.9688473520249221\n","Precision: 0.9758308157099698\n","Recall: 0.9641791044776119\n","F1-Score: 0.96996996996997\n"]}]},{"cell_type":"markdown","source":["# HYBRID\n"],"metadata":{"id":"U01_EgTCJRRD"}},{"cell_type":"code","source":["import numpy as np\n","import pandas as pd\n","from keras.preprocessing.text import Tokenizer\n","from keras_preprocessing.sequence import pad_sequences\n","from keras.layers import Embedding, Conv1D, MaxPooling1D, Bidirectional, LSTM, Dense, Dropout\n","from keras.models import Sequential\n","from sklearn.model_selection import train_test_split\n","from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n","\n","df = pd.read_csv (r'/content/drive/MyDrive/New/Clean/FakeNewsD_clean.csv')\n","\n","# Preprocessing\n","MAX_NB_WORDS = 50000  # Maximum number of words to be used in the tokenizer\n","MAX_SEQUENCE_LENGTH = 300  # Maximum length of each news statement\n","EMBEDDING_DIM = 100  # Dimension of the word embedding\n","VALIDATION_SPLIT = 0.2  # Percentage of data to use for validation\n","BATCH_SIZE = 128\n","EPOCHS = 10\n","\n","# Tokenize the news statements\n","tokenizer = Tokenizer(num_words=MAX_NB_WORDS)\n","tokenizer.fit_on_texts(df['statement'])\n","word_index = tokenizer.word_index\n","print('Found %s unique tokens.' % len(word_index))\n","\n","# Convert the news statements to sequences of integers\n","sequences = tokenizer.texts_to_sequences(df['statement'])\n","\n","# Pad the sequences\n","Max_Len = max([len(x) for x in df['statement']])\n","padded = pad_sequences(sequences, maxlen=Max_Len)\n","\n","print('Shape of data tensor:', padded.shape)\n","\n","# Define the maximum number of words to keep in the vocabulary\n","MAX_NUM_WORDS = 20000\n","\n","# Convert the labels to one-hot encoding\n","y = df['label']\n","print('Shape of label tensor:', y.shape)\n","\n","# Split data into training and testing sets\n","X_train, X_test, y_train, y_test = train_test_split(padded, df['label'].values, test_size=0.2)\n","\n","# Build the model\n","model = Sequential()\n","model.add(Embedding(input_dim=MAX_NB_WORDS, output_dim=EMBEDDING_DIM, input_length=Max_Len))\n","model.add(Conv1D(128, 5, activation='relu'))\n","model.add(Dropout(0.3))\n","model.add(MaxPooling1D(2))\n","model.add(Conv1D(128, 5, activation='relu'))\n","model.add(MaxPooling1D(2))\n","model.add(Bidirectional(LSTM(32)))\n","\n","model.add(Dense(1, activation='sigmoid'))\n","\n","# Compile the model\n","model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n","print(model.summary())\n","\n","# Train the model\n","model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=10, batch_size=128)\n","\n","# Evaluate the model\n","y_pred = model.predict(X_test)\n","y_pred = (y_pred > 0.5).astype(int)\n","\n","# Calculate the evaluation metrics\n","accuracy = accuracy_score(y_test, y_pred)\n","precision = precision_score(y_test, y_pred)\n","recall = recall_score(y_test, y_pred)\n","f1 = f1_score(y_test, y_pred)\n","\n","print(\"Accuracy:\", accuracy)\n","print(\"Precision:\", precision)\n","print(\"Recall:\", recall)\n","print(\"F1-Score:\", f1)\n","\n","# Save the model\n","model.save('hybrid_fnd.h5')\n","\n","# Load the saved model\n","# loaded_model = tf.keras.models.load_model('lstm_fn.h5')\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"5tKos3UgMVnP","executionInfo":{"status":"ok","timestamp":1680171687184,"user_tz":-330,"elapsed":219866,"user":{"displayName":"Ramacharan Reddy Kasireddy","userId":"17080325997413282440"}},"outputId":"23208ef4-9363-4d20-a272-0992d336a3da"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Found 17889 unique tokens.\n","Shape of data tensor: (3210, 15629)\n","Shape of label tensor: (3210,)\n","Model: \"sequential\"\n","_________________________________________________________________\n"," Layer (type)                Output Shape              Param #   \n","=================================================================\n"," embedding (Embedding)       (None, 15629, 100)        5000000   \n","                                                                 \n"," conv1d (Conv1D)             (None, 15625, 128)        64128     \n","                                                                 \n"," dropout (Dropout)           (None, 15625, 128)        0         \n","                                                                 \n"," max_pooling1d (MaxPooling1D  (None, 7812, 128)        0         \n"," )                                                               \n","                                                                 \n"," conv1d_1 (Conv1D)           (None, 7808, 128)         82048     \n","                                                                 \n"," max_pooling1d_1 (MaxPooling  (None, 3904, 128)        0         \n"," 1D)                                                             \n","                                                                 \n"," bidirectional (Bidirectiona  (None, 64)               41216     \n"," l)                                                              \n","                                                                 \n"," dense (Dense)               (None, 1)                 65        \n","                                                                 \n","=================================================================\n","Total params: 5,187,457\n","Trainable params: 5,187,457\n","Non-trainable params: 0\n","_________________________________________________________________\n","None\n","Epoch 1/10\n","21/21 [==============================] - 36s 923ms/step - loss: 0.6253 - accuracy: 0.6530 - val_loss: 0.3701 - val_accuracy: 0.8754\n","Epoch 2/10\n","21/21 [==============================] - 19s 890ms/step - loss: 0.1669 - accuracy: 0.9529 - val_loss: 0.1681 - val_accuracy: 0.9393\n","Epoch 3/10\n","21/21 [==============================] - 19s 900ms/step - loss: 0.0374 - accuracy: 0.9918 - val_loss: 0.1642 - val_accuracy: 0.9517\n","Epoch 4/10\n","21/21 [==============================] - 19s 915ms/step - loss: 0.0150 - accuracy: 0.9973 - val_loss: 0.1439 - val_accuracy: 0.9486\n","Epoch 5/10\n","21/21 [==============================] - 19s 916ms/step - loss: 0.0065 - accuracy: 0.9996 - val_loss: 0.1690 - val_accuracy: 0.9424\n","Epoch 6/10\n","21/21 [==============================] - 19s 904ms/step - loss: 0.0037 - accuracy: 1.0000 - val_loss: 0.1627 - val_accuracy: 0.9470\n","Epoch 7/10\n","21/21 [==============================] - 19s 902ms/step - loss: 0.0030 - accuracy: 1.0000 - val_loss: 0.1689 - val_accuracy: 0.9439\n","Epoch 8/10\n","21/21 [==============================] - 19s 906ms/step - loss: 0.0024 - accuracy: 1.0000 - val_loss: 0.1784 - val_accuracy: 0.9424\n","Epoch 9/10\n","21/21 [==============================] - 19s 913ms/step - loss: 0.0020 - accuracy: 1.0000 - val_loss: 0.1860 - val_accuracy: 0.9424\n","Epoch 10/10\n","21/21 [==============================] - 19s 909ms/step - loss: 0.0018 - accuracy: 1.0000 - val_loss: 0.1924 - val_accuracy: 0.9424\n","21/21 [==============================] - 3s 112ms/step\n","Accuracy: 0.942367601246106\n","Precision: 0.9563953488372093\n","Recall: 0.9373219373219374\n","F1-Score: 0.9467625899280575\n"]}]},{"cell_type":"markdown","source":["# GLOVE HYBRID\n"],"metadata":{"id":"uYnJCHPuE-pd"}},{"cell_type":"code","source":["import numpy as np\n","import pandas as pd\n","from keras.preprocessing.text import Tokenizer\n","from keras_preprocessing.sequence import pad_sequences\n","from keras.layers import Embedding, Conv1D, MaxPooling1D, Bidirectional, LSTM, Dense, Dropout\n","from keras.models import Sequential\n","from sklearn.model_selection import train_test_split\n","from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n","\n","df = pd.read_csv (r'/content/drive/MyDrive/New/Clean/FakeNewsD_clean.csv')\n","\n","MAX_NB_WORDS = 50000  # Maximum number of words to be used in the tokenizer\n","VALIDATION_SPLIT = 0.2\n","\n","# Tokenize the news statements\n","tokenizer = Tokenizer(num_words=MAX_NB_WORDS)\n","tokenizer.fit_on_texts(df['statement'])\n","word_index = tokenizer.word_index\n","print('Found %s unique tokens.' % len(word_index))\n","\n","# Convert the news statements to sequences of integers\n","sequences = tokenizer.texts_to_sequences(df['statement'])\n","\n","# Pad the sequences\n","Max_Len = max([len(x) for x in df['statement']])\n","padded = pad_sequences(sequences, maxlen=Max_Len)\n","\n","print('Shape of data tensor:', padded.shape)\n","\n","# Define the maximum number of words to keep in the vocabulary\n","MAX_NUM_WORDS = 20000\n","\n","# Convert the labels to one-hot encoding\n","y = df['label']\n","print('Shape of label tensor:', y.shape)\n","\n","# Split data into training and testing sets\n","X_train, X_test, y_train, y_test = train_test_split(padded, df['label'].values, test_size=0.2)\n","\n","\n","# Load the GloVe embeddings\n","embeddings_index = {}\n","f = open('/content/drive/MyDrive/Colab Notebooks/weights/glove.6B.300d.txt', encoding='utf8')\n","for line in f:\n","    values = line.split()\n","    word = values[0]\n","    coefs = np.asarray(values[1:], dtype='float32')\n","    embeddings_index[word] = coefs\n","f.close()\n","\n","# Create an embedding matrix\n","embedding_dim = 300\n","word_index = tokenizer.word_index\n","num_words = min(MAX_NUM_WORDS, len(word_index) + 1)\n","embedding_matrix = np.zeros((num_words, embedding_dim))\n","for word, i in word_index.items():\n","    if i >= MAX_NUM_WORDS:\n","        continue\n","    embedding_vector = embeddings_index.get(word)\n","    if embedding_vector is not None:\n","        embedding_matrix[i] = embedding_vector\n","\n","# Build the model\n","model = Sequential()\n","model.add(Embedding(input_dim=num_words, output_dim=embedding_dim, input_length=Max_Len, weights=[embedding_matrix], trainable=False))\n","model.add(Conv1D(128, 5, activation='relu'))\n","model.add(Dropout(0.3))\n","model.add(MaxPooling1D(2))\n","model.add(Conv1D(128, 5, activation='relu'))\n","model.add(MaxPooling1D(2))\n","model.add(Bidirectional(LSTM(32)))\n","\n","model.add(Dense(1, activation='sigmoid'))\n","\n","# Compile the model\n","model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n","print(model.summary())\n","\n","# Train the model\n","history = model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=10, batch_size=128)\n","\n","# Evaluate the model\n","y_pred = model.predict(X_test)\n","y_pred = (y_pred > 0.5).astype(int)\n","\n","# Calculate the evaluation metrics\n","accuracy = accuracy_score(y_test, y_pred)\n","precision = precision_score(y_test, y_pred)\n","recall = recall_score(y_test, y_pred)\n","f1 = f1_score(y_test, y_pred)\n","\n","print(\"Accuracy:\", accuracy)\n","print(\"Precision:\", precision)\n","print(\"Recall:\", recall)\n","print(\"F1-Score:\", f1)\n","\n","# Save the model\n","model.save('/content/drive/MyDrive/Colab Notebooks/weights/FND/glove6b300_fnd.h5')\n","\n","# Load the saved model\n","# loaded_model = tf.keras.models.load_model('lstm_fn.h5')\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"LMhvemkK9_Oo","executionInfo":{"status":"ok","timestamp":1680239216773,"user_tz":-330,"elapsed":293338,"user":{"displayName":"Ramacharan Reddy Kasireddy","userId":"17080325997413282440"}},"outputId":"66631859-963f-431d-924f-d85f2c57ac8e"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Found 17889 unique tokens.\n","Shape of data tensor: (3210, 15629)\n","Shape of label tensor: (3210,)\n","Model: \"sequential_1\"\n","_________________________________________________________________\n"," Layer (type)                Output Shape              Param #   \n","=================================================================\n"," embedding_1 (Embedding)     (None, 15629, 300)        5367000   \n","                                                                 \n"," conv1d_1 (Conv1D)           (None, 15625, 128)        192128    \n","                                                                 \n"," dropout (Dropout)           (None, 15625, 128)        0         \n","                                                                 \n"," max_pooling1d (MaxPooling1D  (None, 7812, 128)        0         \n"," )                                                               \n","                                                                 \n"," conv1d_2 (Conv1D)           (None, 7808, 128)         82048     \n","                                                                 \n"," max_pooling1d_1 (MaxPooling  (None, 3904, 128)        0         \n"," 1D)                                                             \n","                                                                 \n"," bidirectional (Bidirectiona  (None, 64)               41216     \n"," l)                                                              \n","                                                                 \n"," dense (Dense)               (None, 1)                 65        \n","                                                                 \n","=================================================================\n","Total params: 5,682,457\n","Trainable params: 315,457\n","Non-trainable params: 5,367,000\n","_________________________________________________________________\n","None\n","Epoch 1/10\n","21/21 [==============================] - 43s 1s/step - loss: 0.5719 - accuracy: 0.7150 - val_loss: 0.4243 - val_accuracy: 0.8146\n","Epoch 2/10\n","21/21 [==============================] - 21s 988ms/step - loss: 0.3284 - accuracy: 0.8618 - val_loss: 0.2879 - val_accuracy: 0.8941\n","Epoch 3/10\n","21/21 [==============================] - 21s 1s/step - loss: 0.2007 - accuracy: 0.9283 - val_loss: 0.2061 - val_accuracy: 0.9206\n","Epoch 4/10\n","21/21 [==============================] - 21s 1s/step - loss: 0.1254 - accuracy: 0.9630 - val_loss: 0.1743 - val_accuracy: 0.9455\n","Epoch 5/10\n","21/21 [==============================] - 22s 1s/step - loss: 0.0746 - accuracy: 0.9813 - val_loss: 0.1639 - val_accuracy: 0.9455\n","Epoch 6/10\n","21/21 [==============================] - 21s 1s/step - loss: 0.0460 - accuracy: 0.9907 - val_loss: 0.1456 - val_accuracy: 0.9486\n","Epoch 7/10\n","21/21 [==============================] - 21s 1s/step - loss: 0.0479 - accuracy: 0.9883 - val_loss: 0.1488 - val_accuracy: 0.9564\n","Epoch 8/10\n","21/21 [==============================] - 21s 1s/step - loss: 0.0222 - accuracy: 0.9973 - val_loss: 0.1392 - val_accuracy: 0.9611\n","Epoch 9/10\n","21/21 [==============================] - 21s 1s/step - loss: 0.0129 - accuracy: 0.9988 - val_loss: 0.1660 - val_accuracy: 0.9486\n","Epoch 10/10\n","21/21 [==============================] - 21s 1s/step - loss: 0.0097 - accuracy: 0.9996 - val_loss: 0.1569 - val_accuracy: 0.9595\n","21/21 [==============================] - 5s 142ms/step\n","Accuracy: 0.9595015576323987\n","Precision: 0.9472140762463344\n","Recall: 0.9758308157099698\n","F1-Score: 0.9613095238095238\n"]}]},{"cell_type":"code","source":["import numpy as np\n","import pandas as pd\n","from keras.preprocessing.text import Tokenizer\n","from keras_preprocessing.sequence import pad_sequences\n","from keras.layers import Embedding, Conv1D, MaxPooling1D, Bidirectional, LSTM, Dense, Dropout\n","from keras.models import Sequential\n","from sklearn.model_selection import train_test_split\n","from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n","\n","df = pd.read_csv (r'/content/drive/MyDrive/New/Clean/FakeNewsD_clean.csv')\n","\n","MAX_NB_WORDS = 50000  # Maximum number of words to be used in the tokenizer\n","VALIDATION_SPLIT = 0.2\n","\n","# Tokenize the news statements\n","tokenizer = Tokenizer(num_words=MAX_NB_WORDS)\n","tokenizer.fit_on_texts(df['statement'])\n","word_index = tokenizer.word_index\n","print('Found %s unique tokens.' % len(word_index))\n","\n","# Convert the news statements to sequences of integers\n","sequences = tokenizer.texts_to_sequences(df['statement'])\n","\n","# Pad the sequences\n","Max_Len = max([len(x) for x in df['statement']])\n","padded = pad_sequences(sequences, maxlen=Max_Len)\n","\n","print('Shape of data tensor:', padded.shape)\n","\n","# Define the maximum number of words to keep in the vocabulary\n","MAX_NUM_WORDS = 20000\n","\n","# Convert the labels to one-hot encoding\n","y = df['label']\n","print('Shape of label tensor:', y.shape)\n","\n","# Split data into training and testing sets\n","X_train, X_test, y_train, y_test = train_test_split(padded, df['label'].values, test_size=0.2)\n","\n","\n","# Load the GloVe embeddings\n","embeddings_index = {}\n","f = open('/content/drive/MyDrive/Colab Notebooks/weights/glove.42B.300d.txt', encoding='utf8')\n","for line in f:\n","    values = line.split()\n","    word = values[0]\n","    coefs = np.asarray(values[1:], dtype='float32')\n","    embeddings_index[word] = coefs\n","f.close()\n","\n","# Create an embedding matrix\n","embedding_dim = 300\n","word_index = tokenizer.word_index\n","num_words = min(MAX_NUM_WORDS, len(word_index) + 1)\n","embedding_matrix = np.zeros((num_words, embedding_dim))\n","for word, i in word_index.items():\n","    if i >= MAX_NUM_WORDS:\n","        continue\n","    embedding_vector = embeddings_index.get(word)\n","    if embedding_vector is not None:\n","        embedding_matrix[i] = embedding_vector\n","\n","# Build the model\n","model = Sequential()\n","model.add(Embedding(input_dim=num_words, output_dim=embedding_dim, input_length=Max_Len, weights=[embedding_matrix], trainable=False))\n","model.add(Conv1D(128, 5, activation='relu'))\n","model.add(Dropout(0.3))\n","model.add(MaxPooling1D(2))\n","model.add(Conv1D(128, 5, activation='relu'))\n","model.add(MaxPooling1D(2))\n","model.add(Bidirectional(LSTM(32)))\n","\n","model.add(Dense(1, activation='sigmoid'))\n","\n","# Compile the model\n","model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n","print(model.summary())\n","\n","# Train the model\n","history = model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=10, batch_size=128)\n","\n","# Evaluate the model\n","y_pred = model.predict(X_test)\n","y_pred = (y_pred > 0.5).astype(int)\n","\n","# Calculate the evaluation metrics\n","accuracy = accuracy_score(y_test, y_pred)\n","precision = precision_score(y_test, y_pred)\n","recall = recall_score(y_test, y_pred)\n","f1 = f1_score(y_test, y_pred)\n","\n","print(\"Accuracy:\", accuracy)\n","print(\"Precision:\", precision)\n","print(\"Recall:\", recall)\n","print(\"F1-Score:\", f1)\n","\n","# Save the model\n","model.save('/content/drive/MyDrive/Colab Notebooks/weights/FND/glove42b_fnd.h5')\n","\n","# Load the saved model\n","# loaded_model = tf.keras.models.load_model('lstm_fn.h5')\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"kxxNJW2M9CyN","executionInfo":{"status":"ok","timestamp":1680239604665,"user_tz":-330,"elapsed":387895,"user":{"displayName":"Ramacharan Reddy Kasireddy","userId":"17080325997413282440"}},"outputId":"4a68959c-eae4-445b-9fd4-d63a3ccc1703"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Found 17889 unique tokens.\n","Shape of data tensor: (3210, 15629)\n","Shape of label tensor: (3210,)\n","Model: \"sequential_2\"\n","_________________________________________________________________\n"," Layer (type)                Output Shape              Param #   \n","=================================================================\n"," embedding_2 (Embedding)     (None, 15629, 300)        5367000   \n","                                                                 \n"," conv1d_3 (Conv1D)           (None, 15625, 128)        192128    \n","                                                                 \n"," dropout_1 (Dropout)         (None, 15625, 128)        0         \n","                                                                 \n"," max_pooling1d_2 (MaxPooling  (None, 7812, 128)        0         \n"," 1D)                                                             \n","                                                                 \n"," conv1d_4 (Conv1D)           (None, 7808, 128)         82048     \n","                                                                 \n"," max_pooling1d_3 (MaxPooling  (None, 3904, 128)        0         \n"," 1D)                                                             \n","                                                                 \n"," bidirectional_1 (Bidirectio  (None, 64)               41216     \n"," nal)                                                            \n","                                                                 \n"," dense_1 (Dense)             (None, 1)                 65        \n","                                                                 \n","=================================================================\n","Total params: 5,682,457\n","Trainable params: 315,457\n","Non-trainable params: 5,367,000\n","_________________________________________________________________\n","None\n","Epoch 1/10\n","21/21 [==============================] - 26s 1s/step - loss: 0.5165 - accuracy: 0.7543 - val_loss: 0.3690 - val_accuracy: 0.8380\n","Epoch 2/10\n","21/21 [==============================] - 22s 1s/step - loss: 0.3067 - accuracy: 0.8785 - val_loss: 0.2804 - val_accuracy: 0.8956\n","Epoch 3/10\n","21/21 [==============================] - 21s 1s/step - loss: 0.2133 - accuracy: 0.9276 - val_loss: 0.2977 - val_accuracy: 0.8847\n","Epoch 4/10\n","21/21 [==============================] - 21s 1s/step - loss: 0.1950 - accuracy: 0.9315 - val_loss: 0.2125 - val_accuracy: 0.9143\n","Epoch 5/10\n","21/21 [==============================] - 21s 1s/step - loss: 0.1103 - accuracy: 0.9677 - val_loss: 0.2229 - val_accuracy: 0.9143\n","Epoch 6/10\n","21/21 [==============================] - 21s 1s/step - loss: 0.0737 - accuracy: 0.9813 - val_loss: 0.1396 - val_accuracy: 0.9595\n","Epoch 7/10\n","21/21 [==============================] - 21s 1s/step - loss: 0.0439 - accuracy: 0.9922 - val_loss: 0.1281 - val_accuracy: 0.9611\n","Epoch 8/10\n","21/21 [==============================] - 21s 1s/step - loss: 0.0346 - accuracy: 0.9942 - val_loss: 0.1212 - val_accuracy: 0.9611\n","Epoch 9/10\n","21/21 [==============================] - 21s 1s/step - loss: 0.0232 - accuracy: 0.9969 - val_loss: 0.1210 - val_accuracy: 0.9595\n","Epoch 10/10\n","21/21 [==============================] - 21s 1s/step - loss: 0.0169 - accuracy: 0.9981 - val_loss: 0.1231 - val_accuracy: 0.9642\n","21/21 [==============================] - 4s 142ms/step\n","Accuracy: 0.9641744548286605\n","Precision: 0.9587020648967551\n","Recall: 0.9730538922155688\n","F1-Score: 0.9658246656760772\n"]}]},{"cell_type":"markdown","source":["# Word2Vec"],"metadata":{"id":"h3rzst1Vljek"}},{"cell_type":"code","source":["import numpy as np\n","import pandas as pd\n","from tensorflow.keras.preprocessing.text import Tokenizer\n","from tensorflow.keras.preprocessing.sequence import pad_sequences\n","from keras.layers import Embedding, Conv1D, MaxPooling1D, Bidirectional, LSTM, Dense, Dropout\n","from keras.models import Sequential\n","from sklearn.model_selection import train_test_split\n","from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n","\n","\n","df = pd.read_csv (r'/content/drive/MyDrive/New/Clean/FakeNewsD_clean.csv')\n","\n","MAX_NB_WORDS = 50000  # Maximum number of words to be used in the tokenizer\n","VALIDATION_SPLIT = 0.2\n","\n","# Tokenize the news statements\n","tokenizer = Tokenizer(num_words=MAX_NB_WORDS)\n","tokenizer.fit_on_texts(df['statement'])\n","word_index = tokenizer.word_index\n","print('Found %s unique tokens.' % len(word_index))\n","\n","# Convert the news statements to sequences of integers\n","sequences = tokenizer.texts_to_sequences(df['statement'])\n","\n","# Pad the sequences\n","Max_Len = max([len(x) for x in df['statement']])\n","padded = pad_sequences(sequences, maxlen=Max_Len)\n","\n","print('Shape of data tensor:', padded.shape)\n","\n","# Define the maximum number of words to keep in the vocabulary\n","MAX_NUM_WORDS = 20000\n","\n","# Convert the labels to one-hot encoding\n","y = df['label']\n","print('Shape of label tensor:', y.shape)\n","\n","# Split data into training and testing sets\n","X_train, X_test, y_train, y_test = train_test_split(padded, df['label'].values, test_size=0.2)\n","\n","# Load the Word2Vec embeddings\n","from gensim.models.word2vec import Word2Vec\n","import gensim.downloader as api\n","\n","# Load the pre-trained Word2Vec model\n","w2v_model = api.load('word2vec-google-news-300')\n","\n","# Create an embedding matrix\n","embedding_dim = 300\n","word_index = tokenizer.word_index\n","num_words = min(MAX_NUM_WORDS, len(word_index) + 1)\n","embedding_matrix = np.zeros((num_words, embedding_dim))\n","for word, i in word_index.items():\n","    if i >= MAX_NUM_WORDS:\n","        continue\n","    if word in w2v_model:\n","        embedding_vector = w2v_model[word]\n","        embedding_matrix[i] = embedding_vector\n","\n","# Build the model\n","model = Sequential()\n","model.add(Embedding(input_dim=num_words, output_dim=embedding_dim, input_length=Max_Len, weights=[embedding_matrix], trainable=False))\n","model.add(Conv1D(128, 5, activation='relu'))\n","model.add(Dropout(0.3))\n","model.add(MaxPooling1D(2))\n","model.add(Conv1D(128, 5, activation='relu'))\n","model.add(MaxPooling1D(2))\n","model.add(Bidirectional(LSTM(32)))\n","\n","model.add(Dense(1, activation='sigmoid'))\n","\n","# Compile the model\n","model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n","print(model.summary())\n","\n","# Train the model\n","history = model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=10, batch_size=128)\n","\n","# Evaluate the model\n","y_pred = model.predict(X_test)\n","y_pred = (y_pred > 0.5).astype(int)\n","\n","# Calculate the evaluation metrics\n","accuracy = accuracy_score(y_test, y_pred)\n","precision = precision_score(y_test, y_pred)\n","recall = recall_score(y_test, y_pred)\n","f1 = f1_score(y_test, y_pred)\n","\n","print(\"Accuracy:\", accuracy)\n","print(\"Precision:\", precision)\n","print(\"Recall:\", recall)\n","print(\"F1-Score:\", f1)\n","\n","# Save the model\n","model.save('/content/drive/MyDrive/Colab Notebooks/weights/FND/hybrid_word2vec_fnd.h5')\n","\n","# Load the saved model\n","# loaded_model = tf.keras.models.load_model('lstm_fn.h5')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"PY5CiUXzly92","executionInfo":{"status":"ok","timestamp":1680174113610,"user_tz":-330,"elapsed":320160,"user":{"displayName":"Ramacharan Reddy Kasireddy","userId":"17080325997413282440"}},"outputId":"fbabaf09-f61f-45c3-d826-eea8af5a5777"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Found 17889 unique tokens.\n","Shape of data tensor: (3210, 15629)\n","Shape of label tensor: (3210,)\n","Model: \"sequential_1\"\n","_________________________________________________________________\n"," Layer (type)                Output Shape              Param #   \n","=================================================================\n"," embedding_1 (Embedding)     (None, 15629, 300)        5367000   \n","                                                                 \n"," conv1d_1 (Conv1D)           (None, 15625, 128)        192128    \n","                                                                 \n"," dropout (Dropout)           (None, 15625, 128)        0         \n","                                                                 \n"," max_pooling1d (MaxPooling1D  (None, 7812, 128)        0         \n"," )                                                               \n","                                                                 \n"," conv1d_2 (Conv1D)           (None, 7808, 128)         82048     \n","                                                                 \n"," max_pooling1d_1 (MaxPooling  (None, 3904, 128)        0         \n"," 1D)                                                             \n","                                                                 \n"," bidirectional (Bidirectiona  (None, 64)               41216     \n"," l)                                                              \n","                                                                 \n"," dense (Dense)               (None, 1)                 65        \n","                                                                 \n","=================================================================\n","Total params: 5,682,457\n","Trainable params: 315,457\n","Non-trainable params: 5,367,000\n","_________________________________________________________________\n","None\n","Epoch 1/10\n","21/21 [==============================] - 41s 1s/step - loss: 0.5706 - accuracy: 0.7352 - val_loss: 0.4557 - val_accuracy: 0.8022\n","Epoch 2/10\n","21/21 [==============================] - 23s 1s/step - loss: 0.3059 - accuracy: 0.8738 - val_loss: 0.2064 - val_accuracy: 0.9268\n","Epoch 3/10\n","21/21 [==============================] - 24s 1s/step - loss: 0.1954 - accuracy: 0.9256 - val_loss: 0.1651 - val_accuracy: 0.9470\n","Epoch 4/10\n","21/21 [==============================] - 24s 1s/step - loss: 0.1445 - accuracy: 0.9509 - val_loss: 0.1377 - val_accuracy: 0.9502\n","Epoch 5/10\n","21/21 [==============================] - 24s 1s/step - loss: 0.0937 - accuracy: 0.9727 - val_loss: 0.1621 - val_accuracy: 0.9377\n","Epoch 6/10\n","21/21 [==============================] - 24s 1s/step - loss: 0.0753 - accuracy: 0.9809 - val_loss: 0.1051 - val_accuracy: 0.9657\n","Epoch 7/10\n","21/21 [==============================] - 24s 1s/step - loss: 0.0316 - accuracy: 0.9953 - val_loss: 0.1047 - val_accuracy: 0.9642\n","Epoch 8/10\n","21/21 [==============================] - 24s 1s/step - loss: 0.0933 - accuracy: 0.9669 - val_loss: 0.2025 - val_accuracy: 0.9283\n","Epoch 9/10\n","21/21 [==============================] - 24s 1s/step - loss: 0.0637 - accuracy: 0.9817 - val_loss: 0.1183 - val_accuracy: 0.9595\n","Epoch 10/10\n","21/21 [==============================] - 24s 1s/step - loss: 0.0253 - accuracy: 0.9945 - val_loss: 0.1031 - val_accuracy: 0.9657\n","21/21 [==============================] - 4s 137ms/step\n","Accuracy: 0.9657320872274143\n","Precision: 0.9663865546218487\n","Recall: 0.971830985915493\n","F1-Score: 0.9691011235955056\n"]}]},{"cell_type":"markdown","source":["# Fast Text"],"metadata":{"id":"Ztj3gVVXTxUD"}},{"cell_type":"code","source":["import numpy as np\n","import pandas as pd\n","from tensorflow.keras.preprocessing.text import Tokenizer\n","from tensorflow.keras.preprocessing.sequence import pad_sequences\n","from keras.layers import Embedding, Conv1D, MaxPooling1D, Bidirectional, LSTM, Dense, Dropout\n","from keras.models import Sequential\n","from sklearn.model_selection import train_test_split\n","from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n","\n","\n","df = pd.read_csv (r'/content/drive/MyDrive/New/Clean/FakeNewsD_clean.csv')\n","\n","MAX_NB_WORDS = 50000  # Maximum number of words to be used in the tokenizer\n","VALIDATION_SPLIT = 0.2\n","\n","# Tokenize the news statements\n","tokenizer = Tokenizer(num_words=MAX_NB_WORDS)\n","tokenizer.fit_on_texts(df['statement'])\n","word_index = tokenizer.word_index\n","print('Found %s unique tokens.' % len(word_index))\n","\n","# Convert the news statements to sequences of integers\n","sequences = tokenizer.texts_to_sequences(df['statement'])\n","\n","# Pad the sequences\n","Max_Len = max([len(x) for x in df['statement']])\n","padded = pad_sequences(sequences, maxlen=Max_Len)\n","\n","print('Shape of data tensor:', padded.shape)\n","\n","# Define the maximum number of words to keep in the vocabulary\n","MAX_NUM_WORDS = 20000\n","\n","# Convert the labels to one-hot encoding\n","y = df['label']\n","print('Shape of label tensor:', y.shape)\n","\n","# Split data into training and testing sets\n","X_train, X_test, y_train, y_test = train_test_split(padded, df['label'].values, test_size=0.2)\n","\n","# Load the Word2Vec embeddings\n","from gensim.models.word2vec import Word2Vec\n","import gensim.downloader as api\n","\n","# Load the pre-trained Word2Vec model\n","ft_model = api.load('fasttext-wiki-news-subwords-300')\n","\n","# Create an embedding matrix\n","embedding_dim = 300\n","word_index = tokenizer.word_index\n","num_words = min(MAX_NUM_WORDS, len(word_index) + 1)\n","embedding_matrix = np.zeros((num_words, embedding_dim))\n","for word, i in word_index.items():\n","    if i >= MAX_NUM_WORDS:\n","        continue\n","    if word in ft_model:\n","        embedding_vector = ft_model[word]\n","        embedding_matrix[i] = embedding_vector\n","\n","# Build the model\n","model = Sequential()\n","model.add(Embedding(input_dim=num_words, output_dim=embedding_dim, input_length=Max_Len, weights=[embedding_matrix], trainable=False))\n","model.add(Conv1D(128, 5, activation='relu'))\n","model.add(Dropout(0.3))\n","model.add(MaxPooling1D(2))\n","model.add(Conv1D(128, 5, activation='relu'))\n","model.add(MaxPooling1D(2))\n","model.add(Bidirectional(LSTM(32)))\n","model.add(Dense(1, activation='sigmoid'))\n","\n","# Compile the model\n","model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n","print(model.summary())\n","\n","# Train the model\n","history = model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=10, batch_size=128)\n","\n","# Evaluate the model\n","y_pred = model.predict(X_test)\n","y_pred = (y_pred > 0.5).astype(int)\n","\n","# Calculate the evaluation metrics\n","accuracy = accuracy_score(y_test, y_pred)\n","precision = precision_score(y_test, y_pred)\n","recall = recall_score(y_test, y_pred)\n","f1 = f1_score(y_test, y_pred)\n","\n","print(\"Accuracy:\", accuracy)\n","print(\"Precision:\", precision)\n","print(\"Recall:\", recall)\n","print(\"F1-Score:\", f1)\n","\n","# Save the model\n","model.save('/content/drive/MyDrive/Colab Notebooks/weights/FND/hybrid_ft_fnd.h5')\n","\n","# Load the saved model\n","# loaded_model = tf.keras.models.load_model('lstm_fn.h5')\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"bzxuJQ3GKFyD","executionInfo":{"status":"ok","timestamp":1680175155543,"user_tz":-330,"elapsed":434128,"user":{"displayName":"Ramacharan Reddy Kasireddy","userId":"17080325997413282440"}},"outputId":"b6ab8fab-db5c-476e-f7af-476bf6952bc2"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Found 17889 unique tokens.\n","Shape of data tensor: (3210, 15629)\n","Shape of label tensor: (3210,)\n","Model: \"sequential\"\n","_________________________________________________________________\n"," Layer (type)                Output Shape              Param #   \n","=================================================================\n"," embedding (Embedding)       (None, 15629, 300)        5367000   \n","                                                                 \n"," conv1d (Conv1D)             (None, 15625, 128)        192128    \n","                                                                 \n"," dropout (Dropout)           (None, 15625, 128)        0         \n","                                                                 \n"," max_pooling1d (MaxPooling1D  (None, 7812, 128)        0         \n"," )                                                               \n","                                                                 \n"," conv1d_1 (Conv1D)           (None, 7808, 128)         82048     \n","                                                                 \n"," max_pooling1d_1 (MaxPooling  (None, 3904, 128)        0         \n"," 1D)                                                             \n","                                                                 \n"," bidirectional (Bidirectiona  (None, 64)               41216     \n"," l)                                                              \n","                                                                 \n"," dense (Dense)               (None, 1)                 65        \n","                                                                 \n","=================================================================\n","Total params: 5,682,457\n","Trainable params: 315,457\n","Non-trainable params: 5,367,000\n","_________________________________________________________________\n","None\n","Epoch 1/10\n","21/21 [==============================] - 42s 1s/step - loss: 0.6287 - accuracy: 0.6811 - val_loss: 0.4476 - val_accuracy: 0.8162\n","Epoch 2/10\n","21/21 [==============================] - 23s 1s/step - loss: 0.3753 - accuracy: 0.8435 - val_loss: 0.3659 - val_accuracy: 0.8396\n","Epoch 3/10\n","21/21 [==============================] - 24s 1s/step - loss: 0.2569 - accuracy: 0.9011 - val_loss: 0.2410 - val_accuracy: 0.8879\n","Epoch 4/10\n","21/21 [==============================] - 25s 1s/step - loss: 0.2346 - accuracy: 0.9054 - val_loss: 0.2575 - val_accuracy: 0.9034\n","Epoch 5/10\n","21/21 [==============================] - 24s 1s/step - loss: 0.1854 - accuracy: 0.9369 - val_loss: 0.2913 - val_accuracy: 0.8754\n","Epoch 6/10\n","21/21 [==============================] - 24s 1s/step - loss: 0.1420 - accuracy: 0.9463 - val_loss: 0.1790 - val_accuracy: 0.9283\n","Epoch 7/10\n","21/21 [==============================] - 24s 1s/step - loss: 0.0932 - accuracy: 0.9692 - val_loss: 0.1566 - val_accuracy: 0.9408\n","Epoch 8/10\n","21/21 [==============================] - 24s 1s/step - loss: 0.0644 - accuracy: 0.9809 - val_loss: 0.1288 - val_accuracy: 0.9455\n","Epoch 9/10\n","21/21 [==============================] - 24s 1s/step - loss: 0.0491 - accuracy: 0.9868 - val_loss: 0.1750 - val_accuracy: 0.9299\n","Epoch 10/10\n","21/21 [==============================] - 24s 1s/step - loss: 0.0507 - accuracy: 0.9860 - val_loss: 0.2003 - val_accuracy: 0.9268\n","21/21 [==============================] - 4s 136ms/step\n","Accuracy: 0.926791277258567\n","Precision: 0.8857868020304569\n","Recall: 0.9943019943019943\n","F1-Score: 0.9369127516778523\n"]}]},{"cell_type":"code","source":[],"metadata":{"id":"MWxr6PFcQoAO"},"execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"},"vscode":{"interpreter":{"hash":"aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"}},"colab":{"provenance":[],"collapsed_sections":["PmF1iUhDEey3"],"machine_shape":"hm"},"gpuClass":"premium","accelerator":"GPU"},"nbformat":4,"nbformat_minor":0}