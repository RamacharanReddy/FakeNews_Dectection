{"cells":[{"cell_type":"code","source":["!pip install Keras-Preprocessing"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"6Dd1ZD6RFl82","executionInfo":{"status":"ok","timestamp":1681238523038,"user_tz":-330,"elapsed":4935,"user":{"displayName":"Ramacharan Reddy Kasireddy","userId":"17080325997413282440"}},"outputId":"63d2366e-3e6e-4ac3-939c-d58c45dd814e"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting Keras-Preprocessing\n","  Downloading Keras_Preprocessing-1.1.2-py2.py3-none-any.whl (42 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.6/42.6 kB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.9/dist-packages (from Keras-Preprocessing) (1.16.0)\n","Requirement already satisfied: numpy>=1.9.1 in /usr/local/lib/python3.9/dist-packages (from Keras-Preprocessing) (1.22.4)\n","Installing collected packages: Keras-Preprocessing\n","Successfully installed Keras-Preprocessing-1.1.2\n"]}]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"OVUe9CXD5b8g","executionInfo":{"status":"ok","timestamp":1681238525537,"user_tz":-330,"elapsed":2506,"user":{"displayName":"Ramacharan Reddy Kasireddy","userId":"17080325997413282440"}},"outputId":"224b785c-f9cc-4a96-a3b6-6b8f32467ec8"},"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Requirement already satisfied: tensorflow in /usr/local/lib/python3.9/dist-packages (2.12.0)\n","Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.9/dist-packages (from tensorflow) (0.2.0)\n","Requirement already satisfied: flatbuffers>=2.0 in /usr/local/lib/python3.9/dist-packages (from tensorflow) (23.3.3)\n","Requirement already satisfied: jax>=0.3.15 in /usr/local/lib/python3.9/dist-packages (from tensorflow) (0.4.7)\n","Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.9/dist-packages (from tensorflow) (3.3.0)\n","Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.9/dist-packages (from tensorflow) (4.5.0)\n","Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /usr/local/lib/python3.9/dist-packages (from tensorflow) (3.20.3)\n","Requirement already satisfied: tensorflow-estimator<2.13,>=2.12.0 in /usr/local/lib/python3.9/dist-packages (from tensorflow) (2.12.0)\n","Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.9/dist-packages (from tensorflow) (1.53.0)\n","Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.9/dist-packages (from tensorflow) (1.16.0)\n","Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.9/dist-packages (from tensorflow) (1.6.3)\n","Requirement already satisfied: numpy<1.24,>=1.22 in /usr/local/lib/python3.9/dist-packages (from tensorflow) (1.22.4)\n","Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.9/dist-packages (from tensorflow) (1.4.0)\n","Requirement already satisfied: keras<2.13,>=2.12.0 in /usr/local/lib/python3.9/dist-packages (from tensorflow) (2.12.0)\n","Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.9/dist-packages (from tensorflow) (3.8.0)\n","Requirement already satisfied: tensorboard<2.13,>=2.12 in /usr/local/lib/python3.9/dist-packages (from tensorflow) (2.12.1)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.9/dist-packages (from tensorflow) (67.6.1)\n","Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.9/dist-packages (from tensorflow) (16.0.0)\n","Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.9/dist-packages (from tensorflow) (0.32.0)\n","Requirement already satisfied: wrapt<1.15,>=1.11.0 in /usr/local/lib/python3.9/dist-packages (from tensorflow) (1.14.1)\n","Requirement already satisfied: gast<=0.4.0,>=0.2.1 in /usr/local/lib/python3.9/dist-packages (from tensorflow) (0.4.0)\n","Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.9/dist-packages (from tensorflow) (2.2.0)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.9/dist-packages (from tensorflow) (23.0)\n","Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.9/dist-packages (from astunparse>=1.6.0->tensorflow) (0.40.0)\n","Requirement already satisfied: ml-dtypes>=0.0.3 in /usr/local/lib/python3.9/dist-packages (from jax>=0.3.15->tensorflow) (0.0.4)\n","Requirement already satisfied: scipy>=1.7 in /usr/local/lib/python3.9/dist-packages (from jax>=0.3.15->tensorflow) (1.10.1)\n","Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.9/dist-packages (from tensorboard<2.13,>=2.12->tensorflow) (2.27.1)\n","Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.9/dist-packages (from tensorboard<2.13,>=2.12->tensorflow) (0.7.0)\n","Requirement already satisfied: google-auth-oauthlib<1.1,>=0.5 in /usr/local/lib/python3.9/dist-packages (from tensorboard<2.13,>=2.12->tensorflow) (1.0.0)\n","Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.9/dist-packages (from tensorboard<2.13,>=2.12->tensorflow) (1.8.1)\n","Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.9/dist-packages (from tensorboard<2.13,>=2.12->tensorflow) (2.2.3)\n","Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.9/dist-packages (from tensorboard<2.13,>=2.12->tensorflow) (3.4.3)\n","Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.9/dist-packages (from tensorboard<2.13,>=2.12->tensorflow) (2.17.2)\n","Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.9/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow) (0.2.8)\n","Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.9/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow) (4.9)\n","Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.9/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow) (5.3.0)\n","Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.9/dist-packages (from google-auth-oauthlib<1.1,>=0.5->tensorboard<2.13,>=2.12->tensorflow) (1.3.1)\n","Requirement already satisfied: importlib-metadata>=4.4 in /usr/local/lib/python3.9/dist-packages (from markdown>=2.6.8->tensorboard<2.13,>=2.12->tensorflow) (6.2.0)\n","Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.9/dist-packages (from requests<3,>=2.21.0->tensorboard<2.13,>=2.12->tensorflow) (2.0.12)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.9/dist-packages (from requests<3,>=2.21.0->tensorboard<2.13,>=2.12->tensorflow) (3.4)\n","Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests<3,>=2.21.0->tensorboard<2.13,>=2.12->tensorflow) (1.26.15)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.9/dist-packages (from requests<3,>=2.21.0->tensorboard<2.13,>=2.12->tensorflow) (2022.12.7)\n","Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.9/dist-packages (from werkzeug>=1.0.1->tensorboard<2.13,>=2.12->tensorflow) (2.1.2)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.9/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<2.13,>=2.12->tensorflow) (3.15.0)\n","Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.9/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow) (0.4.8)\n","Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.9/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<1.1,>=0.5->tensorboard<2.13,>=2.12->tensorflow) (3.2.2)\n"]}],"source":["!pip install tensorflow"]},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"KQVP5KQC8hyh","executionInfo":{"status":"ok","timestamp":1681238550798,"user_tz":-330,"elapsed":25266,"user":{"displayName":"Ramacharan Reddy Kasireddy","userId":"17080325997413282440"}},"outputId":"ebe051bf-aa75-47f1-df99-751efe5457c4"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"markdown","source":["# Linear SVM"],"metadata":{"id":"vrdvZ1mO6c8y"}},{"cell_type":"code","source":["# Importing required libraries\n","import pandas as pd\n","from sklearn.feature_extraction.text import TfidfVectorizer\n","from sklearn.model_selection import cross_val_score, cross_val_predict\n","from sklearn.metrics import accuracy_score, recall_score, precision_score, f1_score\n","from sklearn.svm import LinearSVC\n","\n","# Reading data into a pandas DataFrame\n","data = pd.read_csv (r'/content/drive/MyDrive/New/Clean/isot_clean.csv')"],"metadata":{"id":"UR8oFtnX6jSx"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from sklearn.feature_extraction.text import TfidfVectorizer\n","tfidf_v = TfidfVectorizer(max_features=5000, ngram_range=(1,3))\n","X = tfidf_v.fit_transform(data['statement'].values.astype('U'))\n","y = data['label'].values"],"metadata":{"id":"LGIsDMuZ6g8v"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import joblib\n","\n","# Creating a Linear SVM classifier\n","svm = LinearSVC()\n","\n","# Performing five-fold cross-validation\n","scores = cross_val_score(svm, X, y, cv=5)\n","\n","# Printing the cross-validation scores\n","print(\"Cross-validation scores:\", scores)\n","\n","# Computing and printing accuracy, recall, precision, and F1 score\n","y_pred = cross_val_predict(svm, X, y, cv=5)\n","accuracy = accuracy_score(y, y_pred)\n","recall = recall_score(y, y_pred)\n","precision = precision_score(y, y_pred)\n","f1 = f1_score(y, y_pred)\n","print(\"Accuracy:\", accuracy)\n","print(\"Recall:\", recall)\n","print(\"Precision:\", precision)\n","print(\"F1 score:\", f1)\n","\n","# Save the trained model to a file\n","joblib.dump(svm, '/content/drive/MyDrive/Colab Notebooks/weights/ISOT/linearsvc_isot.pkl')\n"," "],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"O8rRMbX96bz3","executionInfo":{"status":"ok","timestamp":1680289293133,"user_tz":-330,"elapsed":4243,"user":{"displayName":"Ramacharan Reddy Kasireddy","userId":"17080325997413282440"}},"outputId":"16d82c91-2d4b-458e-f1e9-add1a074edd9"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Cross-validation scores: [0.98187982 0.9811784  0.98258125 0.98129311 0.98105928]\n","Accuracy: 0.9815983913206136\n","Recall: 0.9816726060998751\n","Precision: 0.981899824090362\n","F1 score: 0.9817862019486682\n"]},{"output_type":"execute_result","data":{"text/plain":["['/content/drive/MyDrive/Colab Notebooks/weights/ISOT/linearsvc_isot.pkl']"]},"metadata":{},"execution_count":8}]},{"cell_type":"markdown","source":["# LSTM\n"],"metadata":{"id":"RrPTHD0tEkw6"}},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"irUXCM2q5b8g","executionInfo":{"status":"ok","timestamp":1680289461928,"user_tz":-330,"elapsed":163656,"user":{"displayName":"Ramacharan Reddy Kasireddy","userId":"17080325997413282440"}},"outputId":"187899a0-5581-4ba4-91e5-e79847d501fd"},"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 1/10\n","1070/1070 - 61s - loss: 0.1631 - accuracy: 0.9412 - val_loss: 0.0974 - val_accuracy: 0.9653 - 61s/epoch - 57ms/step\n","Epoch 2/10\n","1070/1070 - 13s - loss: 0.0703 - accuracy: 0.9767 - val_loss: 0.0898 - val_accuracy: 0.9682 - 13s/epoch - 12ms/step\n","Epoch 3/10\n","1070/1070 - 9s - loss: 0.0478 - accuracy: 0.9847 - val_loss: 0.1023 - val_accuracy: 0.9693 - 9s/epoch - 9ms/step\n","Epoch 4/10\n","1070/1070 - 9s - loss: 0.0389 - accuracy: 0.9874 - val_loss: 0.1051 - val_accuracy: 0.9703 - 9s/epoch - 8ms/step\n","Epoch 5/10\n","1070/1070 - 9s - loss: 0.0237 - accuracy: 0.9927 - val_loss: 0.0984 - val_accuracy: 0.9739 - 9s/epoch - 8ms/step\n","Epoch 6/10\n","1070/1070 - 9s - loss: 0.0195 - accuracy: 0.9939 - val_loss: 0.1051 - val_accuracy: 0.9694 - 9s/epoch - 8ms/step\n","Epoch 7/10\n","1070/1070 - 8s - loss: 0.0158 - accuracy: 0.9951 - val_loss: 0.1179 - val_accuracy: 0.9697 - 8s/epoch - 8ms/step\n","Epoch 8/10\n","1070/1070 - 9s - loss: 0.0088 - accuracy: 0.9977 - val_loss: 0.1674 - val_accuracy: 0.9384 - 9s/epoch - 8ms/step\n","Epoch 9/10\n","1070/1070 - 8s - loss: 0.0111 - accuracy: 0.9966 - val_loss: 0.1307 - val_accuracy: 0.9675 - 8s/epoch - 8ms/step\n","Epoch 10/10\n","1070/1070 - 8s - loss: 0.0063 - accuracy: 0.9985 - val_loss: 0.1353 - val_accuracy: 0.9721 - 8s/epoch - 8ms/step\n","268/268 [==============================] - 1s 3ms/step\n","Accuracy: 0.9720598550385784\n","Precision: 0.9653419593345656\n","Recall: 0.9791422545113663\n","F1-Score: 0.972193135543921\n"]}],"source":["import numpy as np\n","import pandas as pd\n","import tensorflow as tf\n","from sklearn.model_selection import train_test_split\n","from keras.preprocessing.text import Tokenizer\n","from keras_preprocessing.sequence import pad_sequences\n","from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n","\n","df = pd.read_csv (r'/content/drive/MyDrive/New/Clean/isot_clean.csv')\n","\n","# Tokenize the text\n","# Split data into statements and labels\n","statements = df['statement'].astype(str)\n","labels = df['label']\n","\n","# Tokenize statements\n","tokenizer = Tokenizer()\n","tokenizer.fit_on_texts(statements)\n","word_index = tokenizer.word_index\n","sequences = tokenizer.texts_to_sequences(statements)\n","\n","# Pad sequences\n","Max_Len = 100\n","padded_sequences = pad_sequences(sequences, maxlen=Max_Len)\n","\n","\n","# Split the data into training and testing sets\n","X_train, X_test, y_train, y_test = train_test_split(padded_sequences, df['label'].values, test_size=0.2, random_state=0)\n","\n","# Build the model\n","model = tf.keras.Sequential([\n","    tf.keras.layers.Embedding(input_dim=5000, output_dim=32, input_length=Max_Len),\n","    tf.keras.layers.LSTM(32),\n","    tf.keras.layers.Dense(1, activation='sigmoid')\n","])\n","\n","# Compile the model\n","model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n","\n","# Train the model\n","history = model.fit(X_train, y_train, epochs=10, validation_data=(X_test, y_test), verbose=2)\n","\n","# Evaluate the model\n","y_pred = model.predict(X_test)\n","y_pred = (y_pred > 0.5).astype(int)\n","\n","# Calculate the evaluation metrics\n","accuracy = accuracy_score(y_test, y_pred)\n","precision = precision_score(y_test, y_pred)\n","recall = recall_score(y_test, y_pred)\n","f1 = f1_score(y_test, y_pred)\n","\n","print(\"Accuracy:\", accuracy)\n","print(\"Precision:\", precision)\n","print(\"Recall:\", recall)\n","print(\"F1-Score:\", f1)\n","\n","# Save the model\n","model.save('/content/drive/MyDrive/Colab Notebooks/weights/ISOT/lstm_isot.h5')\n","\n","# Load the saved model\n","# loaded_model = tf.keras.models.load_model('lstm_fn.h5')\n"]},{"cell_type":"markdown","source":["# BI LSTM\n"],"metadata":{"id":"SB2BWfsxFOPo"}},{"cell_type":"code","source":["import numpy as np\n","import pandas as pd\n","from keras.preprocessing.text import Tokenizer\n","from keras_preprocessing.sequence import pad_sequences\n","from keras.models import Sequential\n","from keras.layers import Dense, Embedding, LSTM, Bidirectional\n","from sklearn.model_selection import train_test_split\n","from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n","\n","# Load the data from CSV file\n","data = pd.read_csv('/content/drive/MyDrive/New/Clean/isot_clean.csv')\n","\n","# Split data into statements and labels\n","statements = data['statement']\n","labels = data['label']\n","\n","# Tokenize statements\n","tokenizer = Tokenizer()\n","tokenizer.fit_on_texts(statements)\n","word_index = tokenizer.word_index\n","sequences = tokenizer.texts_to_sequences(statements)\n","\n","# Pad sequences\n","max_len = 100\n","padded_sequences = pad_sequences(sequences, maxlen=max_len)\n","\n","# Split data into training and testing sets\n","x_train, x_test, y_train, y_test = train_test_split(padded_sequences, labels, test_size=0.2)\n","\n","# Define model architecture\n","model = Sequential()\n","model.add(Embedding(len(word_index) + 1, 128, input_length=max_len))\n","model.add(Bidirectional(LSTM(64, return_sequences=True)))\n","model.add(Bidirectional(LSTM(32)))\n","model.add(Dense(1, activation='sigmoid'))\n","\n","# Compile model\n","model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n","print(model.summary())\n","\n","# Train model\n","model.fit(x_train, y_train, epochs=10, batch_size=32, validation_data=(x_test, y_test))\n","\n","# Evaluate the model\n","y_pred = model.predict(x_test)\n","y_pred = (y_pred > 0.5).astype(int)\n","\n","# Calculate the evaluation metrics\n","accuracy = accuracy_score(y_test, y_pred)\n","precision = precision_score(y_test, y_pred)\n","recall = recall_score(y_test, y_pred)\n","f1 = f1_score(y_test, y_pred)\n","\n","print(\"Accuracy:\", accuracy)\n","print(\"Precision:\", precision)\n","print(\"Recall:\", recall)\n","print(\"F1-Score:\", f1)\n","\n","\n","# Save the model\n","model.save('/content/drive/MyDrive/Colab Notebooks/weights/ISOT/bilstm_isot.h5')\n","\n","# Load the saved model\n","# loaded_model = tf.keras.models.load_model('lstm_fn.h5')\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"FLFOmQOGC9BG","executionInfo":{"status":"ok","timestamp":1680289753550,"user_tz":-330,"elapsed":281705,"user":{"displayName":"Ramacharan Reddy Kasireddy","userId":"17080325997413282440"}},"outputId":"0dbb89b4-5e77-4d3f-81e8-6f85ef62aa0b"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Model: \"sequential_1\"\n","_________________________________________________________________\n"," Layer (type)                Output Shape              Param #   \n","=================================================================\n"," embedding_1 (Embedding)     (None, 100, 128)          3463680   \n","                                                                 \n"," bidirectional (Bidirectiona  (None, 100, 128)         98816     \n"," l)                                                              \n","                                                                 \n"," bidirectional_1 (Bidirectio  (None, 64)               41216     \n"," nal)                                                            \n","                                                                 \n"," dense_1 (Dense)             (None, 1)                 65        \n","                                                                 \n","=================================================================\n","Total params: 3,603,777\n","Trainable params: 3,603,777\n","Non-trainable params: 0\n","_________________________________________________________________\n","None\n","Epoch 1/10\n","1070/1070 [==============================] - 76s 65ms/step - loss: 0.1566 - accuracy: 0.9412 - val_loss: 0.0983 - val_accuracy: 0.9671\n","Epoch 2/10\n","1070/1070 [==============================] - 26s 24ms/step - loss: 0.0582 - accuracy: 0.9814 - val_loss: 0.0799 - val_accuracy: 0.9718\n","Epoch 3/10\n","1070/1070 [==============================] - 24s 22ms/step - loss: 0.0297 - accuracy: 0.9913 - val_loss: 0.0985 - val_accuracy: 0.9682\n","Epoch 4/10\n","1070/1070 [==============================] - 21s 19ms/step - loss: 0.0202 - accuracy: 0.9939 - val_loss: 0.1006 - val_accuracy: 0.9710\n","Epoch 5/10\n","1070/1070 [==============================] - 20s 19ms/step - loss: 0.0123 - accuracy: 0.9964 - val_loss: 0.1144 - val_accuracy: 0.9712\n","Epoch 6/10\n","1070/1070 [==============================] - 20s 19ms/step - loss: 0.0103 - accuracy: 0.9970 - val_loss: 0.1426 - val_accuracy: 0.9696\n","Epoch 7/10\n","1070/1070 [==============================] - 20s 19ms/step - loss: 0.0105 - accuracy: 0.9967 - val_loss: 0.1223 - val_accuracy: 0.9707\n","Epoch 8/10\n","1070/1070 [==============================] - 19s 18ms/step - loss: 0.0048 - accuracy: 0.9982 - val_loss: 0.1272 - val_accuracy: 0.9729\n","Epoch 9/10\n","1070/1070 [==============================] - 20s 19ms/step - loss: 0.0053 - accuracy: 0.9984 - val_loss: 0.1288 - val_accuracy: 0.9735\n","Epoch 10/10\n","1070/1070 [==============================] - 19s 18ms/step - loss: 0.0044 - accuracy: 0.9986 - val_loss: 0.1431 - val_accuracy: 0.9710\n","268/268 [==============================] - 3s 7ms/step\n","Accuracy: 0.9710077156885667\n","Precision: 0.9686577333636157\n","Recall: 0.9748571428571429\n","F1-Score: 0.9717475506949191\n"]}]},{"cell_type":"markdown","source":["# HYBRID\n"],"metadata":{"id":"U01_EgTCJRRD"}},{"cell_type":"code","source":["import numpy as np\n","import pandas as pd\n","from keras.preprocessing.text import Tokenizer\n","from keras_preprocessing.sequence import pad_sequences\n","from keras.layers import Embedding, Conv1D, MaxPooling1D, Bidirectional, LSTM, Dense, Dropout\n","from keras.models import Sequential\n","from sklearn.model_selection import train_test_split\n","from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n","\n","# Preprocessing\n","MAX_NB_WORDS = 50000  # Maximum number of words to be used in the tokenizer\n","MAX_SEQUENCE_LENGTH = 300  # Maximum length of each news statement\n","EMBEDDING_DIM = 100  # Dimension of the word embedding\n","VALIDATION_SPLIT = 0.2  # Percentage of data to use for validation\n","BATCH_SIZE = 128\n","EPOCHS = 10\n","\n","# Load the data from CSV file\n","df = pd.read_csv('/content/drive/MyDrive/New/Clean/isot_clean.csv')\n","\n","# Split data into statements and labels\n","statements = df['statement'].astype(str)\n","labels = df['label']\n","\n","# Tokenize statements\n","tokenizer = Tokenizer()\n","tokenizer.fit_on_texts(statements)\n","word_index = tokenizer.word_index\n","sequences = tokenizer.texts_to_sequences(statements)\n","\n","# Pad sequences\n","Max_Len = max([len(x) for x in statements])\n","padded_sequences = pad_sequences(sequences, maxlen=Max_Len)\n","\n","print('Shape of data tensor:', padded_sequences.shape)\n","\n","# Define the maximum number of words to keep in the vocabulary\n","MAX_NUM_WORDS = 20000\n","\n","# Convert the labels to one-hot encoding\n","y = df['label']\n","print('Shape of label tensor:', y.shape)\n","\n","# Split data into training and testing sets\n","X_train, X_test, y_train, y_test = train_test_split(padded_sequences, df['label'].values, test_size=0.2)\n","\n","# Build the model\n","model = Sequential()\n","model.add(Embedding(input_dim=MAX_NB_WORDS, output_dim=EMBEDDING_DIM, input_length=Max_Len))\n","model.add(Conv1D(128, 5, activation='relu'))\n","model.add(Dropout(0.3))\n","model.add(MaxPooling1D(2))\n","model.add(Conv1D(128, 5, activation='relu'))\n","model.add(MaxPooling1D(2))\n","model.add(Bidirectional(LSTM(32)))\n","\n","model.add(Dense(1, activation='sigmoid'))\n","\n","# Compile the model\n","model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n","print(model.summary())\n","\n","# Train the model\n","model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=10, batch_size=128)\n","\n","# Evaluate the model\n","y_pred = model.predict(X_test)\n","y_pred = (y_pred > 0.5).astype(int)\n","\n","# Calculate the evaluation metrics\n","accuracy = accuracy_score(y_test, y_pred)\n","precision = precision_score(y_test, y_pred)\n","recall = recall_score(y_test, y_pred)\n","f1 = f1_score(y_test, y_pred)\n","\n","print(\"Accuracy:\", accuracy)\n","print(\"Precision:\", precision)\n","print(\"Recall:\", recall)\n","print(\"F1-Score:\", f1)\n","\n","# Save the model\n","model.save('/content/drive/MyDrive/Colab Notebooks/weights/ISOT/hybrid_isot.h5')\n","\n","# Load the saved model\n","# loaded_model = tf.keras.models.load_model('lstm_fn.h5')\n"],"metadata":{"id":"3zIpb4OAGU1G","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1680291700400,"user_tz":-330,"elapsed":1611938,"user":{"displayName":"Ramacharan Reddy Kasireddy","userId":"17080325997413282440"}},"outputId":"2b5b925d-349e-4786-b60b-0bc8bfcdb9d3"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Shape of data tensor: (42768, 29569)\n","Shape of label tensor: (42768,)\n","Model: \"sequential\"\n","_________________________________________________________________\n"," Layer (type)                Output Shape              Param #   \n","=================================================================\n"," embedding (Embedding)       (None, 29569, 100)        5000000   \n","                                                                 \n"," conv1d (Conv1D)             (None, 29565, 128)        64128     \n","                                                                 \n"," dropout (Dropout)           (None, 29565, 128)        0         \n","                                                                 \n"," max_pooling1d (MaxPooling1D  (None, 14782, 128)       0         \n"," )                                                               \n","                                                                 \n"," conv1d_1 (Conv1D)           (None, 14778, 128)        82048     \n","                                                                 \n"," max_pooling1d_1 (MaxPooling  (None, 7389, 128)        0         \n"," 1D)                                                             \n","                                                                 \n"," bidirectional (Bidirectiona  (None, 64)               41216     \n"," l)                                                              \n","                                                                 \n"," dense (Dense)               (None, 1)                 65        \n","                                                                 \n","=================================================================\n","Total params: 5,187,457\n","Trainable params: 5,187,457\n","Non-trainable params: 0\n","_________________________________________________________________\n","None\n","Epoch 1/10\n","268/268 [==============================] - 195s 654ms/step - loss: 0.1705 - accuracy: 0.9294 - val_loss: 0.0817 - val_accuracy: 0.9733\n","Epoch 2/10\n","268/268 [==============================] - 159s 592ms/step - loss: 0.0483 - accuracy: 0.9847 - val_loss: 0.0703 - val_accuracy: 0.9759\n","Epoch 3/10\n","268/268 [==============================] - 156s 581ms/step - loss: 0.0261 - accuracy: 0.9925 - val_loss: 0.0721 - val_accuracy: 0.9785\n","Epoch 4/10\n","268/268 [==============================] - 152s 569ms/step - loss: 0.0185 - accuracy: 0.9950 - val_loss: 0.0772 - val_accuracy: 0.9777\n","Epoch 5/10\n","268/268 [==============================] - 150s 561ms/step - loss: 0.0141 - accuracy: 0.9961 - val_loss: 0.0736 - val_accuracy: 0.9807\n","Epoch 6/10\n","268/268 [==============================] - 148s 554ms/step - loss: 0.0085 - accuracy: 0.9980 - val_loss: 0.0740 - val_accuracy: 0.9808\n","Epoch 7/10\n","268/268 [==============================] - 148s 553ms/step - loss: 0.0095 - accuracy: 0.9976 - val_loss: 0.0694 - val_accuracy: 0.9819\n","Epoch 8/10\n","268/268 [==============================] - 148s 551ms/step - loss: 0.0042 - accuracy: 0.9992 - val_loss: 0.0777 - val_accuracy: 0.9814\n","Epoch 9/10\n","268/268 [==============================] - 146s 544ms/step - loss: 0.0028 - accuracy: 0.9995 - val_loss: 0.0909 - val_accuracy: 0.9807\n","Epoch 10/10\n","268/268 [==============================] - 146s 545ms/step - loss: 0.0025 - accuracy: 0.9996 - val_loss: 0.0993 - val_accuracy: 0.9787\n","268/268 [==============================] - 39s 139ms/step\n","Accuracy: 0.9787234042553191\n","Precision: 0.9712984054669704\n","Recall: 0.987037037037037\n","F1-Score: 0.9791044776119403\n"]}]},{"cell_type":"markdown","source":["# GLOVE HYBRID\n"],"metadata":{"id":"uYnJCHPuE-pd"}},{"cell_type":"code","source":["import numpy as np\n","import pandas as pd\n","from keras.preprocessing.text import Tokenizer\n","from keras_preprocessing.sequence import pad_sequences\n","from keras.layers import Embedding, Conv1D, MaxPooling1D, Bidirectional, LSTM, Dense, Dropout\n","from keras.models import Sequential\n","from sklearn.model_selection import train_test_split\n","from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n","\n","# Load the data from CSV file\n","df = pd.read_csv('/content/drive/MyDrive/New/Clean/isot_clean.csv')\n","\n","MAX_NB_WORDS = 50000  # Maximum number of words to be used in the tokenizer\n","VALIDATION_SPLIT = 0.2\n","\n","# Split data into statements and labels\n","statements = df['statement'].astype(str)\n","labels = df['label']\n","\n","# Tokenize statements\n","tokenizer = Tokenizer()\n","tokenizer.fit_on_texts(statements)\n","word_index = tokenizer.word_index\n","sequences = tokenizer.texts_to_sequences(statements)\n","\n","# Pad sequences\n","Max_Len = max([len(x) for x in statements])\n","padded_sequences = pad_sequences(sequences, maxlen=Max_Len)\n","\n","print('Shape of data tensor:', padded_sequences.shape)\n","\n","# Define the maximum number of words to keep in the vocabulary\n","MAX_NUM_WORDS = 20000\n","\n","# Convert the labels to one-hot encoding\n","y = df['label']\n","print('Shape of label tensor:', y.shape)\n","\n","# Split data into training and testing sets\n","X_train, X_test, y_train, y_test = train_test_split(padded_sequences, df['label'].values, test_size=0.2)\n","\n","\n","# Load the GloVe embeddings\n","embeddings_index = {}\n","f = open('/content/drive/MyDrive/Colab Notebooks/weights/glove.6B.300d.txt', encoding='utf8')\n","for line in f:\n","    values = line.split()\n","    word = values[0]\n","    coefs = np.asarray(values[1:], dtype='float32')\n","    embeddings_index[word] = coefs\n","f.close()\n","\n","# Create an embedding matrix\n","embedding_dim = 300\n","word_index = tokenizer.word_index\n","num_words = min(MAX_NUM_WORDS, len(word_index) + 1)\n","embedding_matrix = np.zeros((num_words, embedding_dim))\n","for word, i in word_index.items():\n","    if i >= MAX_NUM_WORDS:\n","        continue\n","    embedding_vector = embeddings_index.get(word)\n","    if embedding_vector is not None:\n","        embedding_matrix[i] = embedding_vector\n","\n","# Build the model\n","model = Sequential()\n","model.add(Embedding(input_dim=num_words, output_dim=embedding_dim, input_length=Max_Len, weights=[embedding_matrix], trainable=False))\n","model.add(Conv1D(128, 5, activation='relu'))\n","model.add(Dropout(0.3))\n","model.add(MaxPooling1D(2))\n","model.add(Conv1D(128, 5, activation='relu'))\n","model.add(MaxPooling1D(2))\n","model.add(Bidirectional(LSTM(32)))\n","\n","model.add(Dense(1, activation='sigmoid'))\n","\n","# Compile the model\n","model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n","print(model.summary())\n","\n","# Train the model\n","history = model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=10, batch_size=128)\n","\n","# Evaluate the model\n","y_pred = model.predict(X_test)\n","y_pred = (y_pred > 0.5).astype(int)\n","\n","# Calculate the evaluation metrics\n","accuracy = accuracy_score(y_test, y_pred)\n","precision = precision_score(y_test, y_pred)\n","recall = recall_score(y_test, y_pred)\n","f1 = f1_score(y_test, y_pred)\n","\n","print(\"Accuracy:\", accuracy)\n","print(\"Precision:\", precision)\n","print(\"Recall:\", recall)\n","print(\"F1-Score:\", f1)\n","\n","# Save the model\n","model.save('/content/drive/MyDrive/Colab Notebooks/weights/ISOT/glove6b300_isot.h5')\n","\n","# Load the saved model\n","# loaded_model = tf.keras.models.load_model('lstm_fn.h5')\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"FVtCtuGu12wL","outputId":"53114669-193b-499b-e3e6-9badb233a3aa","executionInfo":{"status":"ok","timestamp":1680316615133,"user_tz":-330,"elapsed":1519589,"user":{"displayName":"Ramacharan Reddy Kasireddy","userId":"17080325997413282440"}}},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Shape of data tensor: (42768, 29569)\n","Shape of label tensor: (42768,)\n","Model: \"sequential\"\n","_________________________________________________________________\n"," Layer (type)                Output Shape              Param #   \n","=================================================================\n"," embedding (Embedding)       (None, 29569, 300)        6000000   \n","                                                                 \n"," conv1d (Conv1D)             (None, 29565, 128)        192128    \n","                                                                 \n"," dropout (Dropout)           (None, 29565, 128)        0         \n","                                                                 \n"," max_pooling1d (MaxPooling1D  (None, 14782, 128)       0         \n"," )                                                               \n","                                                                 \n"," conv1d_1 (Conv1D)           (None, 14778, 128)        82048     \n","                                                                 \n"," max_pooling1d_1 (MaxPooling  (None, 7389, 128)        0         \n"," 1D)                                                             \n","                                                                 \n"," bidirectional (Bidirectiona  (None, 64)               41216     \n"," l)                                                              \n","                                                                 \n"," dense (Dense)               (None, 1)                 65        \n","                                                                 \n","=================================================================\n","Total params: 6,315,457\n","Trainable params: 315,457\n","Non-trainable params: 6,000,000\n","_________________________________________________________________\n","None\n","Epoch 1/10\n","268/268 [==============================] - 178s 576ms/step - loss: 0.2364 - accuracy: 0.9028 - val_loss: 0.1390 - val_accuracy: 0.9563\n","Epoch 2/10\n","268/268 [==============================] - 140s 522ms/step - loss: 0.1110 - accuracy: 0.9604 - val_loss: 0.0878 - val_accuracy: 0.9697\n","Epoch 3/10\n","268/268 [==============================] - 140s 522ms/step - loss: 0.0698 - accuracy: 0.9764 - val_loss: 0.0597 - val_accuracy: 0.9801\n","Epoch 4/10\n","268/268 [==============================] - 140s 522ms/step - loss: 0.0471 - accuracy: 0.9844 - val_loss: 0.0515 - val_accuracy: 0.9835\n","Epoch 5/10\n","268/268 [==============================] - 140s 522ms/step - loss: 0.0288 - accuracy: 0.9910 - val_loss: 0.0668 - val_accuracy: 0.9774\n","Epoch 6/10\n","268/268 [==============================] - 140s 521ms/step - loss: 0.0187 - accuracy: 0.9942 - val_loss: 0.0485 - val_accuracy: 0.9835\n","Epoch 7/10\n","268/268 [==============================] - 140s 521ms/step - loss: 0.0150 - accuracy: 0.9954 - val_loss: 0.0456 - val_accuracy: 0.9855\n","Epoch 8/10\n","268/268 [==============================] - 140s 521ms/step - loss: 0.0114 - accuracy: 0.9963 - val_loss: 0.0466 - val_accuracy: 0.9848\n","Epoch 9/10\n","268/268 [==============================] - 140s 521ms/step - loss: 0.0121 - accuracy: 0.9963 - val_loss: 0.0763 - val_accuracy: 0.9794\n","Epoch 10/10\n","268/268 [==============================] - 140s 522ms/step - loss: 0.0125 - accuracy: 0.9958 - val_loss: 0.0407 - val_accuracy: 0.9873\n","268/268 [==============================] - 40s 142ms/step\n","Accuracy: 0.9872574234276362\n","Precision: 0.9845550945136008\n","Recall: 0.990261998608857\n","F1-Score: 0.9874003005432899\n"]}]},{"cell_type":"code","source":["0.0456import numpy as np\n","import pandas as pd\n","from keras.preprocessing.text import Tokenizer\n","from keras_preprocessing.sequence import pad_sequences\n","from keras.layers import Embedding, Conv1D, MaxPooling1D, Bidirectional, LSTM, Dense, Dropout\n","from keras.models import Sequential\n","from sklearn.model_selection import train_test_split\n","from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n","\n","# Load the data from CSV file\n","df = pd.read_csv('/content/drive/MyDrive/New/Clean/isot_clean.csv')\n","\n","MAX_NB_WORDS = 50000  # Maximum number of words to be used in the tokenizer\n","VALIDATION_SPLIT = 0.2\n","\n","# Split data into statements and labels\n","statements = df['statement'].astype(str)\n","labels = df['label']\n","\n","# Tokenize statements\n","tokenizer = Tokenizer()\n","tokenizer.fit_on_texts(statements)\n","word_index = tokenizer.word_index\n","sequences = tokenizer.texts_to_sequences(statements)\n","\n","# Pad sequences\n","Max_Len = max([len(x) for x in statements])\n","padded_sequences = pad_sequences(sequences, maxlen=Max_Len)\n","\n","print('Shape of data tensor:', padded_sequences.shape)\n","\n","# Define the maximum number of words to keep in the vocabulary\n","MAX_NUM_WORDS = 20000\n","\n","# Convert the labels to one-hot encoding\n","y = df['label']\n","print('Shape of label tensor:', y.shape)\n","\n","# Split data into training and testing sets\n","X_train, X_test, y_train, y_test = train_test_split(padded_sequences, df['label'].values, test_size=0.2)\n","\n","# Load the GloVe embeddings\n","embeddings_index = {}\n","f = open('/content/drive/MyDrive/Colab Notebooks/weights/glove.42B.300d.txt', encoding='utf8')\n","for line in f:\n","    values = line.split()\n","    word = values[0]\n","    coefs = np.asarray(values[1:], dtype='float32')\n","    embeddings_index[word] = coefs\n","f.close()\n","\n","# Create an embedding matrix\n","embedding_dim = 300\n","word_index = tokenizer.word_index\n","num_words = min(MAX_NUM_WORDS, len(word_index) + 1)\n","embedding_matrix = np.zeros((num_words, embedding_dim))\n","for word, i in word_index.items():\n","    if i >= MAX_NUM_WORDS:\n","        continue\n","    embedding_vector = embeddings_index.get(word)\n","    if embedding_vector is not None:\n","        embedding_matrix[i] = embedding_vector\n","\n","# Build the model\n","model = Sequential()\n","model.add(Embedding(input_dim=num_words, output_dim=embedding_dim, input_length=Max_Len, weights=[embedding_matrix], trainable=False))\n","model.add(Conv1D(128, 5, activation='relu'))\n","model.add(Dropout(0.3))\n","model.add(MaxPooling1D(2))\n","model.add(Conv1D(128, 5, activation='relu'))\n","model.add(MaxPooling1D(2))\n","model.add(Bidirectional(LSTM(32)))\n","\n","model.add(Dense(1, activation='sigmoid'))\n","\n","# Compile the model\n","model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n","print(model.summary())\n","\n","# Train the model\n","history = model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=10, batch_size=128)\n","\n","# Evaluate the model\n","y_pred = model.predict(X_test)\n","y_pred = (y_pred > 0.5).astype(int)\n","\n","# Calculate the evaluation metrics\n","accuracy = accuracy_score(y_test, y_pred)\n","precision = precision_score(y_test, y_pred)\n","recall = recall_score(y_test, y_pred)\n","f1 = f1_score(y_test, y_pred)\n","\n","print(\"Accuracy:\", accuracy)\n","print(\"Precision:\", precision)\n","print(\"Recall:\", recall)\n","print(\"F1-Score:\", f1)\n","\n","# Save the model\n","model.save('/content/drive/MyDrive/Colab Notebooks/weights/ISOT/glove42b_isot.h5')\n","\n","# Load the saved model\n","# loaded_model = tf.keras.models.load_model('lstm_fn.h5')\n"],"metadata":{"id":"SisW9NP20s0r","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1680318183295,"user_tz":-330,"elapsed":1566762,"user":{"displayName":"Ramacharan Reddy Kasireddy","userId":"17080325997413282440"}},"outputId":"12bd3894-ef7b-4008-9b57-05547ea3aa4f"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Shape of data tensor: (42768, 29569)\n","Shape of label tensor: (42768,)\n","Model: \"sequential_1\"\n","_________________________________________________________________\n"," Layer (type)                Output Shape              Param #   \n","=================================================================\n"," embedding_1 (Embedding)     (None, 29569, 300)        6000000   \n","                                                                 \n"," conv1d_2 (Conv1D)           (None, 29565, 128)        192128    \n","                                                                 \n"," dropout_1 (Dropout)         (None, 29565, 128)        0         \n","                                                                 \n"," max_pooling1d_2 (MaxPooling  (None, 14782, 128)       0         \n"," 1D)                                                             \n","                                                                 \n"," conv1d_3 (Conv1D)           (None, 14778, 128)        82048     \n","                                                                 \n"," max_pooling1d_3 (MaxPooling  (None, 7389, 128)        0         \n"," 1D)                                                             \n","                                                                 \n"," bidirectional_1 (Bidirectio  (None, 64)               41216     \n"," nal)                                                            \n","                                                                 \n"," dense_1 (Dense)             (None, 1)                 65        \n","                                                                 \n","=================================================================\n","Total params: 6,315,457\n","Trainable params: 315,457\n","Non-trainable params: 6,000,000\n","_________________________________________________________________\n","None\n","Epoch 1/10\n","268/268 [==============================] - 145s 525ms/step - loss: 0.2355 - accuracy: 0.9020 - val_loss: 0.1103 - val_accuracy: 0.9613\n","Epoch 2/10\n","268/268 [==============================] - 140s 522ms/step - loss: 0.0988 - accuracy: 0.9655 - val_loss: 0.0842 - val_accuracy: 0.9731\n","Epoch 3/10\n","268/268 [==============================] - 140s 522ms/step - loss: 0.0616 - accuracy: 0.9797 - val_loss: 0.0687 - val_accuracy: 0.9784\n","Epoch 4/10\n","268/268 [==============================] - 140s 523ms/step - loss: 0.0355 - accuracy: 0.9892 - val_loss: 0.0509 - val_accuracy: 0.9833\n","Epoch 5/10\n","268/268 [==============================] - 140s 523ms/step - loss: 0.0223 - accuracy: 0.9932 - val_loss: 0.0550 - val_accuracy: 0.9840\n","Epoch 6/10\n","268/268 [==============================] - 140s 523ms/step - loss: 0.0156 - accuracy: 0.9956 - val_loss: 0.0518 - val_accuracy: 0.9835\n","Epoch 7/10\n","268/268 [==============================] - 140s 523ms/step - loss: 0.0095 - accuracy: 0.9974 - val_loss: 0.0722 - val_accuracy: 0.9773\n","Epoch 8/10\n","268/268 [==============================] - 140s 523ms/step - loss: 0.0144 - accuracy: 0.9956 - val_loss: 0.0515 - val_accuracy: 0.9850\n","Epoch 9/10\n","268/268 [==============================] - 140s 523ms/step - loss: 0.0113 - accuracy: 0.9964 - val_loss: 0.0599 - val_accuracy: 0.9821\n","Epoch 10/10\n","268/268 [==============================] - 140s 524ms/step - loss: 0.0069 - accuracy: 0.9979 - val_loss: 0.0528 - val_accuracy: 0.9852\n","268/268 [==============================] - 38s 140ms/step\n","Accuracy: 0.9851531447276128\n","Precision: 0.9836179049377018\n","Recall: 0.9870340356564019\n","F1-Score: 0.9853230093609153\n"]}]},{"cell_type":"markdown","source":["# Word2Vec"],"metadata":{"id":"agwIytpAwzcW"}},{"cell_type":"code","source":["import numpy as np\n","import pandas as pd\n","from tensorflow.keras.preprocessing.text import Tokenizer\n","from tensorflow.keras.preprocessing.sequence import pad_sequences\n","from keras.layers import Embedding, Conv1D, MaxPooling1D, Bidirectional, LSTM, Dense, Dropout\n","from keras.models import Sequential\n","from sklearn.model_selection import train_test_split\n","from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n","\n","\n","df = pd.read_csv('/content/drive/MyDrive/New/Clean/isot_clean.csv')\n","\n","MAX_NB_WORDS = 50000  # Maximum number of words to be used in the tokenizer\n","VALIDATION_SPLIT = 0.2\n","\n","# Tokenize the news statements\n","tokenizer = Tokenizer(num_words=MAX_NB_WORDS)\n","tokenizer.fit_on_texts(df['statement'])\n","word_index = tokenizer.word_index\n","print('Found %s unique tokens.' % len(word_index))\n","\n","# Convert the news statements to sequences of integers\n","sequences = tokenizer.texts_to_sequences(df['statement'])\n","\n","# Pad the sequences\n","Max_Len = max([len(x) for x in df['statement']])\n","padded = pad_sequences(sequences, maxlen=Max_Len)\n","\n","print('Shape of data tensor:', padded.shape)\n","\n","# Define the maximum number of words to keep in the vocabulary\n","MAX_NUM_WORDS = 20000\n","\n","# Convert the labels to one-hot encoding\n","y = df['label']\n","print('Shape of label tensor:', y.shape)\n","\n","# Split data into training and testing sets\n","X_train, X_test, y_train, y_test = train_test_split(padded, df['label'].values, test_size=0.2)\n","\n","# Load the Word2Vec embeddings\n","from gensim.models.word2vec import Word2Vec\n","import gensim.downloader as api\n","\n","# Load the pre-trained Word2Vec model\n","w2v_model = api.load('word2vec-google-news-300')\n","\n","# Create an embedding matrix\n","embedding_dim = 300\n","word_index = tokenizer.word_index\n","num_words = min(MAX_NUM_WORDS, len(word_index) + 1)\n","embedding_matrix = np.zeros((num_words, embedding_dim))\n","for word, i in word_index.items():\n","    if i >= MAX_NUM_WORDS:\n","        continue\n","    if word in w2v_model:\n","        embedding_vector = w2v_model[word]\n","        embedding_matrix[i] = embedding_vector\n","\n","# Build the model\n","model = Sequential()\n","model.add(Embedding(input_dim=num_words, output_dim=embedding_dim, input_length=Max_Len, weights=[embedding_matrix], trainable=False))\n","model.add(Conv1D(128, 5, activation='relu'))\n","model.add(Dropout(0.3))\n","model.add(MaxPooling1D(2))\n","model.add(Conv1D(128, 5, activation='relu'))\n","model.add(MaxPooling1D(2))\n","model.add(Bidirectional(LSTM(32)))\n","\n","model.add(Dense(1, activation='sigmoid'))\n","\n","# Compile the model\n","model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n","print(model.summary())\n","\n","# Train the model\n","history = model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=10, batch_size=128)\n","\n","# Evaluate the model\n","y_pred = model.predict(X_test)\n","y_pred = (y_pred > 0.5).astype(int)\n","\n","# Calculate the evaluation metrics\n","accuracy = accuracy_score(y_test, y_pred)\n","precision = precision_score(y_test, y_pred)\n","recall = recall_score(y_test, y_pred)\n","f1 = f1_score(y_test, y_pred)\n","\n","print(\"Accuracy:\", accuracy)\n","print(\"Precision:\", precision)\n","print(\"Recall:\", recall)\n","print(\"F1-Score:\", f1)\n","\n","# Save the model\n","model.save('/content/drive/MyDrive/Colab Notebooks/weights/ISOT/hybrid_word2vec_isot.h5')\n","# Load the saved model\n","# loaded_model = tf.keras.models.load_model('lstm_fn.h5')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"QT2v1WPeoELo","executionInfo":{"status":"ok","timestamp":1681242142530,"user_tz":-330,"elapsed":1770665,"user":{"displayName":"Ramacharan Reddy Kasireddy","userId":"17080325997413282440"}},"outputId":"075e9c34-d678-4f15-ed18-1c2a0280d89e"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Found 27059 unique tokens.\n","Shape of data tensor: (42768, 29569)\n","Shape of label tensor: (42768,)\n","[==================================================] 100.0% 1662.8/1662.8MB downloaded\n","Model: \"sequential_1\"\n","_________________________________________________________________\n"," Layer (type)                Output Shape              Param #   \n","=================================================================\n"," embedding_1 (Embedding)     (None, 29569, 300)        6000000   \n","                                                                 \n"," conv1d_2 (Conv1D)           (None, 29565, 128)        192128    \n","                                                                 \n"," dropout_1 (Dropout)         (None, 29565, 128)        0         \n","                                                                 \n"," max_pooling1d_2 (MaxPooling  (None, 14782, 128)       0         \n"," 1D)                                                             \n","                                                                 \n"," conv1d_3 (Conv1D)           (None, 14778, 128)        82048     \n","                                                                 \n"," max_pooling1d_3 (MaxPooling  (None, 7389, 128)        0         \n"," 1D)                                                             \n","                                                                 \n"," bidirectional_1 (Bidirectio  (None, 64)               41216     \n"," nal)                                                            \n","                                                                 \n"," dense_1 (Dense)             (None, 1)                 65        \n","                                                                 \n","=================================================================\n","Total params: 6,315,457\n","Trainable params: 315,457\n","Non-trainable params: 6,000,000\n","_________________________________________________________________\n","None\n","Epoch 1/10\n","268/268 [==============================] - 144s 523ms/step - loss: 0.2353 - accuracy: 0.9026 - val_loss: 0.1200 - val_accuracy: 0.9578\n","Epoch 2/10\n","268/268 [==============================] - 139s 519ms/step - loss: 0.1047 - accuracy: 0.9623 - val_loss: 0.0810 - val_accuracy: 0.9697\n","Epoch 3/10\n","268/268 [==============================] - 139s 520ms/step - loss: 0.0683 - accuracy: 0.9767 - val_loss: 0.0603 - val_accuracy: 0.9787\n","Epoch 4/10\n","268/268 [==============================] - 139s 520ms/step - loss: 0.0553 - accuracy: 0.9823 - val_loss: 0.0614 - val_accuracy: 0.9799\n","Epoch 5/10\n","268/268 [==============================] - 139s 520ms/step - loss: 0.0341 - accuracy: 0.9899 - val_loss: 0.0655 - val_accuracy: 0.9791\n","Epoch 6/10\n","268/268 [==============================] - 139s 521ms/step - loss: 0.0249 - accuracy: 0.9920 - val_loss: 0.0787 - val_accuracy: 0.9756\n","Epoch 7/10\n","268/268 [==============================] - 139s 521ms/step - loss: 0.0212 - accuracy: 0.9928 - val_loss: 0.0545 - val_accuracy: 0.9857\n","Epoch 8/10\n","268/268 [==============================] - 139s 520ms/step - loss: 0.0166 - accuracy: 0.9946 - val_loss: 0.0578 - val_accuracy: 0.9854\n","Epoch 9/10\n","268/268 [==============================] - 139s 520ms/step - loss: 0.0119 - accuracy: 0.9963 - val_loss: 0.0510 - val_accuracy: 0.9852\n","Epoch 10/10\n","268/268 [==============================] - 139s 520ms/step - loss: 0.0114 - accuracy: 0.9966 - val_loss: 0.0649 - val_accuracy: 0.9804\n","268/268 [==============================] - 37s 135ms/step\n","Accuracy: 0.9803600654664485\n","Precision: 0.9702726866338847\n","Recall: 0.9920018281535649\n","F1-Score: 0.9810169491525423\n"]}]},{"cell_type":"markdown","source":["# fastText"],"metadata":{"id":"DXkR8ftqwqhy"}},{"cell_type":"code","source":["import numpy as np\n","import pandas as pd\n","from tensorflow.keras.preprocessing.text import Tokenizer\n","from tensorflow.keras.preprocessing.sequence import pad_sequences\n","from keras.layers import Embedding, Conv1D, MaxPooling1D, Bidirectional, LSTM, Dense, Dropout\n","from keras.models import Sequential\n","from sklearn.model_selection import train_test_split\n","from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n","\n","# Load the data from CSV file\n","df = pd.read_csv('/content/drive/MyDrive/New/Clean/isot_clean.csv')\n","\n","MAX_NB_WORDS = 50000  # Maximum number of words to be used in the tokenizer\n","VALIDATION_SPLIT = 0.2\n","\n","# Split data into statements and labels\n","statements = df['statement'].astype(str)\n","labels = df['label']\n","\n","# Tokenize statements\n","tokenizer = Tokenizer()\n","tokenizer.fit_on_texts(statements)\n","word_index = tokenizer.word_index\n","sequences = tokenizer.texts_to_sequences(statements)\n","\n","# Pad sequences\n","Max_Len = max([len(x) for x in statements])\n","padded_sequences = pad_sequences(sequences, maxlen=Max_Len)\n","\n","print('Shape of data tensor:', padded_sequences.shape)\n","\n","# Define the maximum number of words to keep in the vocabulary\n","MAX_NUM_WORDS = 20000\n","\n","# Convert the labels to one-hot encoding\n","y = df['label']\n","print('Shape of label tensor:', y.shape)\n","\n","# Split data into training and testing sets\n","X_train, X_test, y_train, y_test = train_test_split(padded_sequences, df['label'].values, test_size=0.2)\n","\n","# Load the Word2Vec embeddings\n","from gensim.models.word2vec import Word2Vec\n","import gensim.downloader as api\n","\n","# Load the pre-trained Word2Vec model\n","ft_model = api.load('fasttext-wiki-news-subwords-300')\n","\n","# Create an embedding matrix\n","embedding_dim = 300\n","word_index = tokenizer.word_index\n","num_words = min(MAX_NUM_WORDS, len(word_index) + 1)\n","embedding_matrix = np.zeros((num_words, embedding_dim))\n","for word, i in word_index.items():\n","    if i >= MAX_NUM_WORDS:\n","        continue\n","    if word in ft_model:\n","        embedding_vector = ft_model[word]\n","        embedding_matrix[i] = embedding_vector\n","\n","# Build the model\n","model = Sequential()\n","model.add(Embedding(input_dim=num_words, output_dim=embedding_dim, input_length=Max_Len, weights=[embedding_matrix], trainable=False))\n","model.add(Conv1D(128, 5, activation='relu'))\n","model.add(Dropout(0.3))\n","model.add(MaxPooling1D(2))\n","model.add(Conv1D(128, 5, activation='relu'))\n","model.add(MaxPooling1D(2))\n","model.add(Bidirectional(LSTM(32)))\n","model.add(Dense(1, activation='sigmoid'))\n","\n","# Compile the model\n","model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n","print(model.summary())\n","\n","# Train the model\n","history = model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=10, batch_size=128)\n","\n","# Evaluate the model\n","y_pred = model.predict(X_test)\n","y_pred = (y_pred > 0.5).astype(int)\n","\n","# Calculate the evaluation metrics\n","accuracy = accuracy_score(y_test, y_pred)\n","precision = precision_score(y_test, y_pred)\n","recall = recall_score(y_test, y_pred)\n","f1 = f1_score(y_test, y_pred)\n","\n","print(\"Accuracy:\", accuracy)\n","print(\"Precision:\", precision)\n","print(\"Recall:\", recall)\n","print(\"F1-Score:\", f1)\n","\n","# Save the model\n","model.save('/content/drive/MyDrive/Colab Notebooks/weights/ISOT/ft_isot.h5')\n","\n","# Load the saved model\n","# loaded_model = tf.keras.models.load_model('lstm_fn.h5')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"uMAr8MgPn37y","executionInfo":{"status":"ok","timestamp":1681240371870,"user_tz":-330,"elapsed":1821074,"user":{"displayName":"Ramacharan Reddy Kasireddy","userId":"17080325997413282440"}},"outputId":"5f9779a5-42cd-43d5-84ac-e464ce09da93"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Shape of data tensor: (42768, 29569)\n","Shape of label tensor: (42768,)\n","[==================================================] 100.0% 958.5/958.4MB downloaded\n","Model: \"sequential\"\n","_________________________________________________________________\n"," Layer (type)                Output Shape              Param #   \n","=================================================================\n"," embedding (Embedding)       (None, 29569, 300)        6000000   \n","                                                                 \n"," conv1d (Conv1D)             (None, 29565, 128)        192128    \n","                                                                 \n"," dropout (Dropout)           (None, 29565, 128)        0         \n","                                                                 \n"," max_pooling1d (MaxPooling1D  (None, 14782, 128)       0         \n"," )                                                               \n","                                                                 \n"," conv1d_1 (Conv1D)           (None, 14778, 128)        82048     \n","                                                                 \n"," max_pooling1d_1 (MaxPooling  (None, 7389, 128)        0         \n"," 1D)                                                             \n","                                                                 \n"," bidirectional (Bidirectiona  (None, 64)               41216     \n"," l)                                                              \n","                                                                 \n"," dense (Dense)               (None, 1)                 65        \n","                                                                 \n","=================================================================\n","Total params: 6,315,457\n","Trainable params: 315,457\n","Non-trainable params: 6,000,000\n","_________________________________________________________________\n","None\n","Epoch 1/10\n","268/268 [==============================] - 175s 573ms/step - loss: 0.2540 - accuracy: 0.8994 - val_loss: 0.1597 - val_accuracy: 0.9434\n","Epoch 2/10\n","268/268 [==============================] - 139s 519ms/step - loss: 0.1277 - accuracy: 0.9539 - val_loss: 0.0990 - val_accuracy: 0.9667\n","Epoch 3/10\n","268/268 [==============================] - 139s 519ms/step - loss: 0.0862 - accuracy: 0.9698 - val_loss: 0.0727 - val_accuracy: 0.9749\n","Epoch 4/10\n","268/268 [==============================] - 139s 519ms/step - loss: 0.0623 - accuracy: 0.9783 - val_loss: 0.0679 - val_accuracy: 0.9749\n","Epoch 5/10\n","268/268 [==============================] - 139s 519ms/step - loss: 0.0491 - accuracy: 0.9840 - val_loss: 0.0654 - val_accuracy: 0.9752\n","Epoch 6/10\n","268/268 [==============================] - 139s 519ms/step - loss: 0.0423 - accuracy: 0.9856 - val_loss: 0.0524 - val_accuracy: 0.9828\n","Epoch 7/10\n","268/268 [==============================] - 139s 519ms/step - loss: 0.0324 - accuracy: 0.9892 - val_loss: 0.0482 - val_accuracy: 0.9840\n","Epoch 8/10\n","268/268 [==============================] - 139s 519ms/step - loss: 0.0179 - accuracy: 0.9948 - val_loss: 0.0536 - val_accuracy: 0.9850\n","Epoch 9/10\n","268/268 [==============================] - 139s 519ms/step - loss: 0.0231 - accuracy: 0.9921 - val_loss: 0.0492 - val_accuracy: 0.9864\n","Epoch 10/10\n","268/268 [==============================] - 139s 519ms/step - loss: 0.0112 - accuracy: 0.9966 - val_loss: 0.0428 - val_accuracy: 0.9861\n","268/268 [==============================] - 39s 136ms/step\n","Accuracy: 0.986088379705401\n","Precision: 0.9822767552828903\n","Recall: 0.9906049495875344\n","F1-Score: 0.9864232743867655\n"]}]},{"cell_type":"code","source":[],"metadata":{"id":"Puw33QTIoC_M"},"execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"},"vscode":{"interpreter":{"hash":"aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"}},"colab":{"provenance":[{"file_id":"1i6cLmOqCb2cdC_JLe0c7gMeWDZJnAVIP","timestamp":1676879404734},{"file_id":"13D7SifO3SZhnheo8lvJjov4kAvesZdrs","timestamp":1676878318895},{"file_id":"195VpL2hxfaVbGUg5vKzJWOjcrZszboc6","timestamp":1676873879368}],"collapsed_sections":["vrdvZ1mO6c8y","RrPTHD0tEkw6","SB2BWfsxFOPo"],"machine_shape":"hm","toc_visible":true},"gpuClass":"premium","accelerator":"GPU"},"nbformat":4,"nbformat_minor":0}