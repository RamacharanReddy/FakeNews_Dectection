{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"OVUe9CXD5b8g","executionInfo":{"status":"ok","timestamp":1680540222769,"user_tz":-330,"elapsed":4715,"user":{"displayName":"Ramacharan Reddy Kasireddy","userId":"17080325997413282440"}},"outputId":"5fdef248-a8ca-4bd7-ee1b-fe0b43d9bdf0"},"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Requirement already satisfied: tensorflow in /usr/local/lib/python3.9/dist-packages (2.12.0)\n","Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.9/dist-packages (from tensorflow) (4.5.0)\n","Requirement already satisfied: gast<=0.4.0,>=0.2.1 in /usr/local/lib/python3.9/dist-packages (from tensorflow) (0.4.0)\n","Requirement already satisfied: numpy<1.24,>=1.22 in /usr/local/lib/python3.9/dist-packages (from tensorflow) (1.22.4)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.9/dist-packages (from tensorflow) (23.0)\n","Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.9/dist-packages (from tensorflow) (0.32.0)\n","Requirement already satisfied: jax>=0.3.15 in /usr/local/lib/python3.9/dist-packages (from tensorflow) (0.4.7)\n","Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.9/dist-packages (from tensorflow) (1.53.0)\n","Requirement already satisfied: tensorflow-estimator<2.13,>=2.12.0 in /usr/local/lib/python3.9/dist-packages (from tensorflow) (2.12.0)\n","Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.9/dist-packages (from tensorflow) (0.2.0)\n","Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /usr/local/lib/python3.9/dist-packages (from tensorflow) (3.20.3)\n","Requirement already satisfied: flatbuffers>=2.0 in /usr/local/lib/python3.9/dist-packages (from tensorflow) (23.3.3)\n","Requirement already satisfied: keras<2.13,>=2.12.0 in /usr/local/lib/python3.9/dist-packages (from tensorflow) (2.12.0)\n","Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.9/dist-packages (from tensorflow) (1.16.0)\n","Requirement already satisfied: tensorboard<2.13,>=2.12 in /usr/local/lib/python3.9/dist-packages (from tensorflow) (2.12.0)\n","Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.9/dist-packages (from tensorflow) (1.6.3)\n","Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.9/dist-packages (from tensorflow) (16.0.0)\n","Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.9/dist-packages (from tensorflow) (1.4.0)\n","Requirement already satisfied: wrapt<1.15,>=1.11.0 in /usr/local/lib/python3.9/dist-packages (from tensorflow) (1.14.1)\n","Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.9/dist-packages (from tensorflow) (3.8.0)\n","Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.9/dist-packages (from tensorflow) (2.2.0)\n","Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.9/dist-packages (from tensorflow) (3.3.0)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.9/dist-packages (from tensorflow) (67.6.1)\n","Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.9/dist-packages (from astunparse>=1.6.0->tensorflow) (0.40.0)\n","Requirement already satisfied: scipy>=1.7 in /usr/local/lib/python3.9/dist-packages (from jax>=0.3.15->tensorflow) (1.10.1)\n","Requirement already satisfied: ml-dtypes>=0.0.3 in /usr/local/lib/python3.9/dist-packages (from jax>=0.3.15->tensorflow) (0.0.4)\n","Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.9/dist-packages (from tensorboard<2.13,>=2.12->tensorflow) (3.4.3)\n","Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.9/dist-packages (from tensorboard<2.13,>=2.12->tensorflow) (2.2.3)\n","Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.9/dist-packages (from tensorboard<2.13,>=2.12->tensorflow) (2.17.0)\n","Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.9/dist-packages (from tensorboard<2.13,>=2.12->tensorflow) (1.8.1)\n","Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.9/dist-packages (from tensorboard<2.13,>=2.12->tensorflow) (2.27.1)\n","Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.9/dist-packages (from tensorboard<2.13,>=2.12->tensorflow) (0.7.0)\n","Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.9/dist-packages (from tensorboard<2.13,>=2.12->tensorflow) (0.4.6)\n","Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.9/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow) (4.9)\n","Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.9/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow) (5.3.0)\n","Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.9/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow) (0.2.8)\n","Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.9/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.13,>=2.12->tensorflow) (1.3.1)\n","Requirement already satisfied: importlib-metadata>=4.4 in /usr/local/lib/python3.9/dist-packages (from markdown>=2.6.8->tensorboard<2.13,>=2.12->tensorflow) (6.1.0)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.9/dist-packages (from requests<3,>=2.21.0->tensorboard<2.13,>=2.12->tensorflow) (2022.12.7)\n","Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests<3,>=2.21.0->tensorboard<2.13,>=2.12->tensorflow) (1.26.15)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.9/dist-packages (from requests<3,>=2.21.0->tensorboard<2.13,>=2.12->tensorflow) (3.4)\n","Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.9/dist-packages (from requests<3,>=2.21.0->tensorboard<2.13,>=2.12->tensorflow) (2.0.12)\n","Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.9/dist-packages (from werkzeug>=1.0.1->tensorboard<2.13,>=2.12->tensorflow) (2.1.2)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.9/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<2.13,>=2.12->tensorflow) (3.15.0)\n","Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.9/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow) (0.4.8)\n","Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.9/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.13,>=2.12->tensorflow) (3.2.2)\n"]}],"source":["!pip install tensorflow"]},{"cell_type":"code","source":["!pip install Keras-Preprocessing"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"6Dd1ZD6RFl82","executionInfo":{"status":"ok","timestamp":1680540227425,"user_tz":-330,"elapsed":4661,"user":{"displayName":"Ramacharan Reddy Kasireddy","userId":"17080325997413282440"}},"outputId":"b36e4547-fc98-43be-c5e9-8cea4aba03dd"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting Keras-Preprocessing\n","  Downloading Keras_Preprocessing-1.1.2-py2.py3-none-any.whl (42 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.6/42.6 KB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: numpy>=1.9.1 in /usr/local/lib/python3.9/dist-packages (from Keras-Preprocessing) (1.22.4)\n","Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.9/dist-packages (from Keras-Preprocessing) (1.16.0)\n","Installing collected packages: Keras-Preprocessing\n","Successfully installed Keras-Preprocessing-1.1.2\n"]}]},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"KQVP5KQC8hyh","executionInfo":{"status":"ok","timestamp":1680540266166,"user_tz":-330,"elapsed":38747,"user":{"displayName":"Ramacharan Reddy Kasireddy","userId":"17080325997413282440"}},"outputId":"cedf8c94-c50c-4795-f42c-f8054b1ef416"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"markdown","source":["# Linear SVM"],"metadata":{"id":"nLhMkvjo6ENk"}},{"cell_type":"code","source":["# Importing required libraries\n","import pandas as pd\n","from sklearn.feature_extraction.text import TfidfVectorizer\n","from sklearn.model_selection import cross_val_score, cross_val_predict\n","from sklearn.metrics import accuracy_score, recall_score, precision_score, f1_score\n","from sklearn.svm import LinearSVC\n","\n","# Reading data into a pandas DataFrame\n","data = pd.read_csv (r'/content/drive/MyDrive/New/Clean/LIAR_clean.csv')"],"metadata":{"id":"09blDvVG5pN4"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from sklearn.feature_extraction.text import TfidfVectorizer\n","tfidf_v = TfidfVectorizer(max_features=5000, ngram_range=(1,3))\n","X = tfidf_v.fit_transform(data['statement'].values.astype('U'))\n","y = data['label'].values"],"metadata":{"id":"CmqzkuG95p41"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import joblib\n","\n","# Creating a Linear SVM classifier\n","svm = LinearSVC()\n","\n","# Performing five-fold cross-validation\n","scores = cross_val_score(svm, X, y, cv=5)\n","\n","# Printing the cross-validation scores\n","print(\"Cross-validation scores:\", scores)\n","\n","# Computing and printing accuracy, recall, precision, and F1 score\n","y_pred = cross_val_predict(svm, X, y, cv=5)\n","accuracy = accuracy_score(y, y_pred)\n","recall = recall_score(y, y_pred)\n","precision = precision_score(y, y_pred)\n","f1 = f1_score(y, y_pred)\n","print(\"Accuracy:\", accuracy)\n","print(\"Recall:\", recall)\n","print(\"Precision:\", precision)\n","print(\"F1 score:\", f1)\n","\n","# Save the trained model to a file\n","joblib.dump(svm, '/content/drive/MyDrive/Colab Notebooks/weights/LIAR/linearsvc_fnd.pkl')\n"," "],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"u8dwOW4B5vN_","executionInfo":{"status":"ok","timestamp":1680281928791,"user_tz":-330,"elapsed":1559,"user":{"displayName":"Ramacharan Reddy Kasireddy","userId":"17080325997413282440"}},"outputId":"836317f8-b42c-416e-fe22-3b5410e5f178"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Cross-validation scores: [0.70838766 0.70708388 0.7053455  0.71794872 0.72707518]\n","Accuracy: 0.7131681877444589\n","Recall: 0.1927064712191634\n","Precision: 0.34092346616065783\n","F1 score: 0.24623115577889448\n"]},{"output_type":"execute_result","data":{"text/plain":["['/content/drive/MyDrive/Colab Notebooks/weights/LIAR/linearsvc_fnd.pkl']"]},"metadata":{},"execution_count":6}]},{"cell_type":"markdown","source":["# LSTM\n"],"metadata":{"id":"RrPTHD0tEkw6"}},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"irUXCM2q5b8g","executionInfo":{"status":"ok","timestamp":1680282364965,"user_tz":-330,"elapsed":44964,"user":{"displayName":"Ramacharan Reddy Kasireddy","userId":"17080325997413282440"}},"outputId":"97328dfa-a4e9-427e-a865-a713ca876e9c"},"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 1/10\n","288/288 - 19s - loss: 0.5597 - accuracy: 0.7558 - val_loss: 0.5441 - val_accuracy: 0.7618 - 19s/epoch - 65ms/step\n","Epoch 2/10\n","288/288 - 4s - loss: 0.5080 - accuracy: 0.7621 - val_loss: 0.5543 - val_accuracy: 0.7575 - 4s/epoch - 15ms/step\n","Epoch 3/10\n","288/288 - 3s - loss: 0.4377 - accuracy: 0.8010 - val_loss: 0.6058 - val_accuracy: 0.7249 - 3s/epoch - 10ms/step\n","Epoch 4/10\n","288/288 - 2s - loss: 0.3713 - accuracy: 0.8347 - val_loss: 0.6499 - val_accuracy: 0.7245 - 2s/epoch - 8ms/step\n","Epoch 5/10\n","288/288 - 3s - loss: 0.3175 - accuracy: 0.8677 - val_loss: 0.7538 - val_accuracy: 0.6980 - 3s/epoch - 9ms/step\n","Epoch 6/10\n","288/288 - 2s - loss: 0.2780 - accuracy: 0.8864 - val_loss: 0.8405 - val_accuracy: 0.6958 - 2s/epoch - 8ms/step\n","Epoch 7/10\n","288/288 - 3s - loss: 0.2489 - accuracy: 0.8977 - val_loss: 0.8819 - val_accuracy: 0.6862 - 3s/epoch - 9ms/step\n","Epoch 8/10\n","288/288 - 2s - loss: 0.2212 - accuracy: 0.9122 - val_loss: 1.0139 - val_accuracy: 0.6601 - 2s/epoch - 8ms/step\n","Epoch 9/10\n","288/288 - 2s - loss: 0.1990 - accuracy: 0.9210 - val_loss: 1.0326 - val_accuracy: 0.6632 - 2s/epoch - 9ms/step\n","Epoch 10/10\n","288/288 - 2s - loss: 0.1784 - accuracy: 0.9302 - val_loss: 1.2675 - val_accuracy: 0.6780 - 2s/epoch - 8ms/step\n","72/72 [==============================] - 1s 3ms/step\n","Accuracy: 0.6779661016949152\n","Precision: 0.3096646942800789\n","Recall: 0.2864963503649635\n","F1-Score: 0.2976303317535545\n"]}],"source":["import numpy as np\n","import pandas as pd\n","import tensorflow as tf\n","from sklearn.model_selection import train_test_split\n","from tensorflow.keras.preprocessing.text import Tokenizer\n","from tensorflow.keras.preprocessing.sequence import pad_sequences\n","from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n","\n","df = pd.read_csv (r'/content/drive/MyDrive/New/Clean/LIAR_clean.csv')\n","# Tokenize the text\n","# Split data into statements and labels\n","statements = df['statement'].astype(str)\n","labels = df['label']\n","\n","# Tokenize statements\n","tokenizer = Tokenizer()\n","tokenizer.fit_on_texts(statements)\n","word_index = tokenizer.word_index\n","sequences = tokenizer.texts_to_sequences(statements)\n","\n","# Pad sequences\n","max_len = 100\n","padded_sequences = pad_sequences(sequences, maxlen=max_len)\n","\n","# Split the data into training and testing sets\n","X_train, X_test, y_train, y_test = train_test_split(padded_sequences, df['label'].values, test_size=0.2, random_state=0)\n","\n","# Build the model\n","model = tf.keras.Sequential([\n","    tf.keras.layers.Embedding(input_dim=5000, output_dim=32, input_length=100),\n","    tf.keras.layers.LSTM(32),\n","    tf.keras.layers.Dense(1, activation='sigmoid')\n","])\n","\n","# Compile the model\n","model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n","\n","# Train the model\n","history = model.fit(X_train, y_train, epochs=10, validation_data=(X_test, y_test), verbose=2)\n","\n","# Evaluate the model\n","y_pred = model.predict(X_test)\n","y_pred = (y_pred > 0.5).astype(int)\n","\n","# Calculate the evaluation metrics\n","accuracy = accuracy_score(y_test, y_pred)\n","precision = precision_score(y_test, y_pred)\n","recall = recall_score(y_test, y_pred)\n","f1 = f1_score(y_test, y_pred)\n","\n","print(\"Accuracy:\", accuracy)\n","print(\"Precision:\", precision)\n","print(\"Recall:\", recall)\n","print(\"F1-Score:\", f1)\n","\n","# Save the model\n","model.save('/content/drive/MyDrive/Colab Notebooks/weights/LIAR/lstm_fnd.h5')\n","\n","# Load the saved model\n","# loaded_model = tf.keras.models.load_model('lstm_fn.h5')\n"]},{"cell_type":"markdown","source":["# BI LSTM\n"],"metadata":{"id":"SB2BWfsxFOPo"}},{"cell_type":"code","source":["import numpy as np\n","import pandas as pd\n","from keras.preprocessing.text import Tokenizer\n","from keras_preprocessing.sequence import pad_sequences\n","from keras.models import Sequential\n","from keras.layers import Dense, Embedding, LSTM, Bidirectional\n","from sklearn.model_selection import train_test_split\n","from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n","\n","# Load the data from CSV file\n","data = pd.read_csv('/content/drive/MyDrive/New/Clean/LIAR_clean.csv')\n","\n","# Split data into statements and labels\n","statements = data['statement'].astype(str)\n","labels = data['label']\n","\n","# Tokenize statements\n","tokenizer = Tokenizer()\n","tokenizer.fit_on_texts(statements)\n","word_index = tokenizer.word_index\n","sequences = tokenizer.texts_to_sequences(statements)\n","\n","# Pad sequences\n","max_len = 100\n","padded_sequences = pad_sequences(sequences, maxlen=max_len)\n","\n","# Split data into training and testing sets\n","x_train, x_test, y_train, y_test = train_test_split(padded_sequences, labels, test_size=0.2)\n","\n","# Define model architecture\n","model = Sequential()\n","model.add(Embedding(len(word_index) + 1, 128, input_length=max_len))\n","model.add(Bidirectional(LSTM(64, return_sequences=True)))\n","model.add(Bidirectional(LSTM(32)))\n","model.add(Dense(1, activation='sigmoid'))\n","\n","# Compile model\n","model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n","print(model.summary())\n","\n","# Train model\n","model.fit(x_train, y_train, epochs=10, batch_size=32, validation_data=(x_test, y_test))\n","\n","# Evaluate the model\n","y_pred = model.predict(x_test)\n","y_pred = (y_pred > 0.5).astype(int)\n","\n","# Calculate the evaluation metrics\n","accuracy = accuracy_score(y_test, y_pred)\n","precision = precision_score(y_test, y_pred)\n","recall = recall_score(y_test, y_pred)\n","f1 = f1_score(y_test, y_pred)\n","\n","print(\"Accuracy:\", accuracy)\n","print(\"Precision:\", precision)\n","print(\"Recall:\", recall)\n","print(\"F1-Score:\", f1)\n","\n","# Save the model\n","model.save('/content/drive/MyDrive/Colab Notebooks/weights/LIAR/bilstm_fnd.h5')\n","\n","# Load the saved model\n","# loaded_model = tf.keras.models.load_model('lstm_fn.h5')\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"gXMEaLBUzmti","executionInfo":{"status":"ok","timestamp":1680282286907,"user_tz":-330,"elapsed":154166,"user":{"displayName":"Ramacharan Reddy Kasireddy","userId":"17080325997413282440"}},"outputId":"739546d3-21cc-4a9c-c15c-47d8e2b96353"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Model: \"sequential\"\n","_________________________________________________________________\n"," Layer (type)                Output Shape              Param #   \n","=================================================================\n"," embedding (Embedding)       (None, 100, 128)          806016    \n","                                                                 \n"," bidirectional (Bidirectiona  (None, 100, 128)         98816     \n"," l)                                                              \n","                                                                 \n"," bidirectional_1 (Bidirectio  (None, 64)               41216     \n"," nal)                                                            \n","                                                                 \n"," dense (Dense)               (None, 1)                 65        \n","                                                                 \n","=================================================================\n","Total params: 946,113\n","Trainable params: 946,113\n","Non-trainable params: 0\n","_________________________________________________________________\n","None\n","Epoch 1/10\n","288/288 [==============================] - 35s 83ms/step - loss: 0.5526 - accuracy: 0.7559 - val_loss: 0.5423 - val_accuracy: 0.7579\n","Epoch 2/10\n","288/288 [==============================] - 9s 31ms/step - loss: 0.4844 - accuracy: 0.7778 - val_loss: 0.5769 - val_accuracy: 0.7532\n","Epoch 3/10\n","288/288 [==============================] - 7s 23ms/step - loss: 0.3886 - accuracy: 0.8321 - val_loss: 0.6228 - val_accuracy: 0.7223\n","Epoch 4/10\n","288/288 [==============================] - 6s 21ms/step - loss: 0.3056 - accuracy: 0.8723 - val_loss: 0.7072 - val_accuracy: 0.7023\n","Epoch 5/10\n","288/288 [==============================] - 6s 20ms/step - loss: 0.2359 - accuracy: 0.9041 - val_loss: 0.9095 - val_accuracy: 0.6993\n","Epoch 6/10\n","288/288 [==============================] - 6s 21ms/step - loss: 0.1836 - accuracy: 0.9236 - val_loss: 1.0285 - val_accuracy: 0.7062\n","Epoch 7/10\n","288/288 [==============================] - 7s 23ms/step - loss: 0.1510 - accuracy: 0.9351 - val_loss: 1.2118 - val_accuracy: 0.6927\n","Epoch 8/10\n","288/288 [==============================] - 7s 24ms/step - loss: 0.1222 - accuracy: 0.9464 - val_loss: 1.4829 - val_accuracy: 0.6619\n","Epoch 9/10\n","288/288 [==============================] - 6s 21ms/step - loss: 0.1027 - accuracy: 0.9544 - val_loss: 1.6219 - val_accuracy: 0.6810\n","Epoch 10/10\n","288/288 [==============================] - 6s 20ms/step - loss: 0.0854 - accuracy: 0.9608 - val_loss: 1.6976 - val_accuracy: 0.6545\n","72/72 [==============================] - 2s 8ms/step\n","Accuracy: 0.6544980443285529\n","Precision: 0.2926829268292683\n","Recall: 0.3016157989228007\n","F1-Score: 0.29708222811671087\n"]}]},{"cell_type":"markdown","source":["# HYBRID\n"],"metadata":{"id":"U01_EgTCJRRD"}},{"cell_type":"code","source":["import numpy as np\n","import pandas as pd\n","from keras.preprocessing.text import Tokenizer\n","from keras_preprocessing.sequence import pad_sequences\n","from keras.layers import Embedding, Conv1D, MaxPooling1D, Bidirectional, LSTM, Dense, Dropout\n","from keras.models import Sequential\n","from sklearn.model_selection import train_test_split\n","from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n","\n","# Preprocessing\n","MAX_NB_WORDS = 50000  # Maximum number of words to be used in the tokenizer\n","MAX_SEQUENCE_LENGTH = 300  # Maximum length of each news statement\n","EMBEDDING_DIM = 100  # Dimension of the word embedding\n","VALIDATION_SPLIT = 0.2  # Percentage of data to use for validation\n","BATCH_SIZE = 128\n","EPOCHS = 10\n","\n","# Load the data from CSV file\n","df = pd.read_csv('/content/drive/MyDrive/New/Clean/LIAR_clean.csv')\n","\n","# Split data into statements and labels\n","statements = df['statement'].astype(str)\n","labels = df['label']\n","\n","# Tokenize statements\n","tokenizer = Tokenizer()\n","tokenizer.fit_on_texts(statements)\n","word_index = tokenizer.word_index\n","sequences = tokenizer.texts_to_sequences(statements)\n","\n","# Pad sequences\n","Max_Len = max([len(x) for x in statements])\n","padded_sequences = pad_sequences(sequences, maxlen=Max_Len)\n","\n","print('Shape of data tensor:', padded_sequences.shape)\n","\n","# Define the maximum number of words to keep in the vocabulary\n","MAX_NUM_WORDS = 20000\n","\n","# Convert the labels to one-hot encoding\n","y = df['label']\n","print('Shape of label tensor:', y.shape)\n","\n","# Split data into training and testing sets\n","X_train, X_test, y_train, y_test = train_test_split(padded_sequences, df['label'].values, test_size=0.2)\n","\n","# Build the model\n","model = Sequential()\n","model.add(Embedding(input_dim=MAX_NB_WORDS, output_dim=EMBEDDING_DIM, input_length=Max_Len))\n","model.add(Conv1D(128, 5, activation='relu'))\n","model.add(Dropout(0.3))\n","model.add(MaxPooling1D(2))\n","model.add(Conv1D(128, 5, activation='relu'))\n","model.add(MaxPooling1D(2))\n","model.add(Bidirectional(LSTM(32)))\n","model.add(Dense(1, activation='sigmoid'))\n","\n","# Compile the model\n","model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n","print(model.summary())\n","\n","# Train the model\n","model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=10, batch_size=128)\n","\n","# Evaluate the model\n","y_pred = model.predict(X_test)\n","y_pred = (y_pred > 0.5).astype(int)\n","\n","# Calculate the evaluation metrics\n","accuracy = accuracy_score(y_test, y_pred)\n","precision = precision_score(y_test, y_pred)\n","recall = recall_score(y_test, y_pred)\n","f1 = f1_score(y_test, y_pred)\n","\n","print(\"Accuracy:\", accuracy)\n","print(\"Precision:\", precision)\n","print(\"Recall:\", recall)\n","print(\"F1-Score:\", f1)\n","\n","# Save the model\n","model.save('hybrid_liar.h5')\n","\n","# Load the saved model\n","# loaded_model = tf.keras.models.load_model('lstm_fn.h5')\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"5tKos3UgMVnP","executionInfo":{"status":"ok","timestamp":1680540483461,"user_tz":-330,"elapsed":151951,"user":{"displayName":"Ramacharan Reddy Kasireddy","userId":"17080325997413282440"}},"outputId":"01c43f9a-6932-420c-e219-2df7309f2e37"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Shape of data tensor: (11505, 1398)\n","Shape of label tensor: (11505,)\n","Model: \"sequential\"\n","_________________________________________________________________\n"," Layer (type)                Output Shape              Param #   \n","=================================================================\n"," embedding (Embedding)       (None, 1398, 100)         5000000   \n","                                                                 \n"," conv1d (Conv1D)             (None, 1394, 128)         64128     \n","                                                                 \n"," dropout (Dropout)           (None, 1394, 128)         0         \n","                                                                 \n"," max_pooling1d (MaxPooling1D  (None, 697, 128)         0         \n"," )                                                               \n","                                                                 \n"," conv1d_1 (Conv1D)           (None, 693, 128)          82048     \n","                                                                 \n"," max_pooling1d_1 (MaxPooling  (None, 346, 128)         0         \n"," 1D)                                                             \n","                                                                 \n"," bidirectional (Bidirectiona  (None, 64)               41216     \n"," l)                                                              \n","                                                                 \n"," dense (Dense)               (None, 1)                 65        \n","                                                                 \n","=================================================================\n","Total params: 5,187,457\n","Trainable params: 5,187,457\n","Non-trainable params: 0\n","_________________________________________________________________\n","None\n","Epoch 1/10\n","72/72 [==============================] - 32s 226ms/step - loss: 0.5612 - accuracy: 0.7565 - val_loss: 0.5594 - val_accuracy: 0.7584\n","Epoch 2/10\n","72/72 [==============================] - 10s 138ms/step - loss: 0.5463 - accuracy: 0.7565 - val_loss: 0.5464 - val_accuracy: 0.7584\n","Epoch 3/10\n","72/72 [==============================] - 10s 133ms/step - loss: 0.4862 - accuracy: 0.7741 - val_loss: 0.5528 - val_accuracy: 0.7553\n","Epoch 4/10\n","72/72 [==============================] - 7s 103ms/step - loss: 0.3907 - accuracy: 0.8255 - val_loss: 0.6357 - val_accuracy: 0.7136\n","Epoch 5/10\n","72/72 [==============================] - 7s 98ms/step - loss: 0.2866 - accuracy: 0.8818 - val_loss: 0.7316 - val_accuracy: 0.7223\n","Epoch 6/10\n","72/72 [==============================] - 7s 97ms/step - loss: 0.2007 - accuracy: 0.9222 - val_loss: 0.8419 - val_accuracy: 0.6984\n","Epoch 7/10\n","72/72 [==============================] - 7s 97ms/step - loss: 0.1439 - accuracy: 0.9460 - val_loss: 0.9749 - val_accuracy: 0.6832\n","Epoch 8/10\n","72/72 [==============================] - 6s 88ms/step - loss: 0.1125 - accuracy: 0.9564 - val_loss: 1.1111 - val_accuracy: 0.6993\n","Epoch 9/10\n","72/72 [==============================] - 6s 83ms/step - loss: 0.0936 - accuracy: 0.9628 - val_loss: 1.2135 - val_accuracy: 0.7036\n","Epoch 10/10\n","72/72 [==============================] - 6s 85ms/step - loss: 0.0753 - accuracy: 0.9703 - val_loss: 1.2975 - val_accuracy: 0.6893\n","72/72 [==============================] - 2s 12ms/step\n","Accuracy: 0.6892655367231638\n","Precision: 0.3129411764705882\n","Recall: 0.2392086330935252\n","F1-Score: 0.2711518858307849\n"]}]},{"cell_type":"markdown","source":["# GLOVE HYBRID\n"],"metadata":{"id":"uYnJCHPuE-pd"}},{"cell_type":"code","source":["import numpy as np\n","import pandas as pd\n","from keras.preprocessing.text import Tokenizer\n","from keras_preprocessing.sequence import pad_sequences\n","from keras.layers import Embedding, Conv1D, MaxPooling1D, Bidirectional, LSTM, Dense, Dropout\n","from keras.models import Sequential\n","from sklearn.model_selection import train_test_split\n","from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n","\n","# Load the data from CSV file\n","df = pd.read_csv('/content/drive/MyDrive/New/Clean/LIAR_clean.csv')\n","\n","MAX_NB_WORDS = 50000  # Maximum number of words to be used in the tokenizer\n","VALIDATION_SPLIT = 0.2\n","\n","# Split data into statements and labels\n","statements = df['statement'].astype(str)\n","labels = df['label']\n","\n","# Tokenize statements\n","tokenizer = Tokenizer()\n","tokenizer.fit_on_texts(statements)\n","word_index = tokenizer.word_index\n","sequences = tokenizer.texts_to_sequences(statements)\n","\n","# Pad sequences\n","Max_Len = max([len(x) for x in statements])\n","padded_sequences = pad_sequences(sequences, maxlen=Max_Len)\n","\n","print('Shape of data tensor:', padded_sequences.shape)\n","\n","# Define the maximum number of words to keep in the vocabulary\n","MAX_NUM_WORDS = 20000\n","\n","# Convert the labels to one-hot encoding\n","y = df['label']\n","print('Shape of label tensor:', y.shape)\n","\n","# Split data into training and testing sets\n","X_train, X_test, y_train, y_test = train_test_split(padded_sequences, df['label'].values, test_size=0.2)\n","\n","\n","# Load the GloVe embeddings\n","embeddings_index = {}\n","f = open('/content/drive/MyDrive/Colab Notebooks/weights/glove.6B.300d.txt', encoding='utf8')\n","for line in f:\n","    values = line.split()\n","    word = values[0]\n","    coefs = np.asarray(values[1:], dtype='float32')\n","    embeddings_index[word] = coefs\n","f.close()\n","\n","# Create an embedding matrix\n","embedding_dim = 300\n","word_index = tokenizer.word_index\n","num_words = min(MAX_NUM_WORDS, len(word_index) + 1)\n","embedding_matrix = np.zeros((num_words, embedding_dim))\n","for word, i in word_index.items():\n","    if i >= MAX_NUM_WORDS:\n","        continue\n","    embedding_vector = embeddings_index.get(word)\n","    if embedding_vector is not None:\n","        embedding_matrix[i] = embedding_vector\n","\n","# Build the model\n","model = Sequential()\n","model.add(Embedding(input_dim=num_words, output_dim=embedding_dim, input_length=Max_Len, weights=[embedding_matrix], trainable=False))\n","model.add(Conv1D(128, 5, activation='relu'))\n","model.add(Dropout(0.3))\n","model.add(MaxPooling1D(2))\n","model.add(Conv1D(128, 5, activation='relu'))\n","model.add(MaxPooling1D(2))\n","model.add(Bidirectional(LSTM(32)))\n","\n","model.add(Dense(1, activation='sigmoid'))\n","\n","# Compile the model\n","model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n","print(model.summary())\n","\n","# Train the model\n","history = model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=10, batch_size=128)\n","\n","# Evaluate the model\n","y_pred = model.predict(X_test)\n","y_pred = (y_pred > 0.5).astype(int)\n","\n","# Calculate the evaluation metrics\n","accuracy = accuracy_score(y_test, y_pred)\n","precision = precision_score(y_test, y_pred)\n","recall = recall_score(y_test, y_pred)\n","f1 = f1_score(y_test, y_pred)\n","\n","print(\"Accuracy:\", accuracy)\n","print(\"Precision:\", precision)\n","print(\"Recall:\", recall)\n","print(\"F1-Score:\", f1)\n","\n","# Save the model\n","model.save('/content/drive/MyDrive/Colab Notebooks/weights/LIAR/glove6b300_fnd.h5')\n","\n","# Load the saved model\n","# loaded_model = tf.keras.models.load_model('lstm_fn.h5')\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"LMhvemkK9_Oo","executionInfo":{"status":"ok","timestamp":1680282912720,"user_tz":-330,"elapsed":112395,"user":{"displayName":"Ramacharan Reddy Kasireddy","userId":"17080325997413282440"}},"outputId":"59e84eaa-7c6f-4094-c9d3-2fb91c0526df"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Shape of data tensor: (11505, 1398)\n","Shape of label tensor: (11505,)\n","Model: \"sequential_5\"\n","_________________________________________________________________\n"," Layer (type)                Output Shape              Param #   \n","=================================================================\n"," embedding_5 (Embedding)     (None, 1398, 300)         1889100   \n","                                                                 \n"," conv1d_6 (Conv1D)           (None, 1394, 128)         192128    \n","                                                                 \n"," dropout_3 (Dropout)         (None, 1394, 128)         0         \n","                                                                 \n"," max_pooling1d_6 (MaxPooling  (None, 697, 128)         0         \n"," 1D)                                                             \n","                                                                 \n"," conv1d_7 (Conv1D)           (None, 693, 128)          82048     \n","                                                                 \n"," max_pooling1d_7 (MaxPooling  (None, 346, 128)         0         \n"," 1D)                                                             \n","                                                                 \n"," bidirectional_5 (Bidirectio  (None, 64)               41216     \n"," nal)                                                            \n","                                                                 \n"," dense_5 (Dense)             (None, 1)                 65        \n","                                                                 \n","=================================================================\n","Total params: 2,204,557\n","Trainable params: 315,457\n","Non-trainable params: 1,889,100\n","_________________________________________________________________\n","None\n","Epoch 1/10\n","72/72 [==============================] - 14s 130ms/step - loss: 0.5594 - accuracy: 0.7574 - val_loss: 0.5575 - val_accuracy: 0.7518\n","Epoch 2/10\n","72/72 [==============================] - 8s 108ms/step - loss: 0.5435 - accuracy: 0.7581 - val_loss: 0.5492 - val_accuracy: 0.7518\n","Epoch 3/10\n","72/72 [==============================] - 8s 109ms/step - loss: 0.5240 - accuracy: 0.7581 - val_loss: 0.5654 - val_accuracy: 0.7518\n","Epoch 4/10\n","72/72 [==============================] - 8s 110ms/step - loss: 0.4897 - accuracy: 0.7687 - val_loss: 0.5965 - val_accuracy: 0.7453\n","Epoch 5/10\n","72/72 [==============================] - 8s 109ms/step - loss: 0.4096 - accuracy: 0.8183 - val_loss: 0.6732 - val_accuracy: 0.7336\n","Epoch 6/10\n","72/72 [==============================] - 8s 108ms/step - loss: 0.3175 - accuracy: 0.8676 - val_loss: 0.7057 - val_accuracy: 0.6875\n","Epoch 7/10\n","72/72 [==============================] - 8s 108ms/step - loss: 0.2375 - accuracy: 0.9081 - val_loss: 0.7963 - val_accuracy: 0.6845\n","Epoch 8/10\n","72/72 [==============================] - 8s 107ms/step - loss: 0.1997 - accuracy: 0.9218 - val_loss: 0.8695 - val_accuracy: 0.6919\n","Epoch 9/10\n","72/72 [==============================] - 8s 107ms/step - loss: 0.1655 - accuracy: 0.9374 - val_loss: 0.9685 - val_accuracy: 0.6893\n","Epoch 10/10\n","72/72 [==============================] - 8s 108ms/step - loss: 0.1516 - accuracy: 0.9419 - val_loss: 1.0062 - val_accuracy: 0.6658\n","72/72 [==============================] - 2s 15ms/step\n","Accuracy: 0.6657974793568013\n","Precision: 0.28936170212765955\n","Recall: 0.2381786339754816\n","F1-Score: 0.2612872238232469\n"]}]},{"cell_type":"code","source":["import numpy as np\n","import pandas as pd\n","from keras.preprocessing.text import Tokenizer\n","from keras_preprocessing.sequence import pad_sequences\n","from keras.layers import Embedding, Conv1D, MaxPooling1D, Bidirectional, LSTM, Dense, Dropout\n","from keras.models import Sequential\n","from sklearn.model_selection import train_test_split\n","from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n","\n","# Load the data from CSV file\n","dadfta = pd.read_csv('/content/drive/MyDrive/New/Clean/LIAR_clean.csv')\n","\n","MAX_NB_WORDS = 50000  # Maximum number of words to be used in the tokenizer\n","VALIDATION_SPLIT = 0.2\n","\n","# Split data into statements and labels\n","statements = df['statement'].astype(str)\n","labels = df['label']\n","\n","# Tokenize statements\n","tokenizer = Tokenizer()\n","tokenizer.fit_on_texts(statements)\n","word_index = tokenizer.word_index\n","sequences = tokenizer.texts_to_sequences(statements)\n","\n","# Pad sequences\n","Max_Len = max([len(x) for x in statements])\n","padded_sequences = pad_sequences(sequences, maxlen=Max_Len)\n","\n","print('Shape of data tensor:', padded_sequences.shape)\n","\n","# Define the maximum number of words to keep in the vocabulary\n","MAX_NUM_WORDS = 20000\n","\n","# Convert the labels to one-hot encoding\n","y = df['label']\n","print('Shape of label tensor:', y.shape)\n","\n","# Split data into training and testing sets\n","X_train, X_test, y_train, y_test = train_test_split(padded_sequences, df['label'].values, test_size=0.2)\n","\n","# Load the GloVe embeddings\n","embeddings_index = {}\n","f = open('/content/drive/MyDrive/Colab Notebooks/weights/glove.42B.300d.txt', encoding='utf8')\n","for line in f:\n","    values = line.split()\n","    word = values[0]\n","    coefs = np.asarray(values[1:], dtype='float32')\n","    embeddings_index[word] = coefs\n","f.close()\n","\n","# Create an embedding matrix\n","embedding_dim = 300\n","word_index = tokenizer.word_index\n","num_words = min(MAX_NUM_WORDS, len(word_index) + 1)\n","embedding_matrix = np.zeros((num_words, embedding_dim))\n","for word, i in word_index.items():\n","    if i >= MAX_NUM_WORDS:\n","        continue\n","    embedding_vector = embeddings_index.get(word)\n","    if embedding_vector is not None:\n","        embedding_matrix[i] = embedding_vector\n","\n","# Build the model\n","model = Sequential()\n","model.add(Embedding(input_dim=num_words, output_dim=embedding_dim, input_length=Max_Len, weights=[embedding_matrix], trainable=False))\n","model.add(Conv1D(128, 5, activation='relu'))\n","model.add(Dropout(0.3))\n","model.add(MaxPooling1D(2))\n","model.add(Conv1D(128, 5, activation='relu'))\n","model.add(MaxPooling1D(2))\n","model.add(Bidirectional(LSTM(32)))\n","\n","model.add(Dense(1, activation='sigmoid'))\n","\n","# Compile the model\n","model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n","print(model.summary())\n","\n","# Train the model\n","history = model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=10, batch_size=128)\n","\n","# Evaluate the model\n","y_pred = model.predict(X_test)\n","y_pred = (y_pred > 0.5).astype(int)\n","\n","# Calculate the evaluation metrics\n","accuracy = accuracy_score(y_test, y_pred)\n","precision = precision_score(y_test, y_pred)\n","recall = recall_score(y_test, y_pred)\n","f1 = f1_score(y_test, y_pred)\n","\n","print(\"Accuracy:\", accuracy)\n","print(\"Precision:\", precision)\n","print(\"Recall:\", recall)\n","print(\"F1-Score:\", f1)\n","\n","# Save the model\n","model.save('/content/drive/MyDrive/Colab Notebooks/weights/LIAR/glove42b_fnd.h5')\n","\n","# Load the saved model\n","# loaded_model = tf.keras.models.load_model('lstm_fn.h5')\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"kxxNJW2M9CyN","executionInfo":{"status":"ok","timestamp":1680283162266,"user_tz":-330,"elapsed":200620,"user":{"displayName":"Ramacharan Reddy Kasireddy","userId":"17080325997413282440"}},"outputId":"5eb27e37-79b8-4b30-e9de-d59b924a4948"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Shape of data tensor: (11505, 1398)\n","Shape of label tensor: (11505,)\n","Model: \"sequential_6\"\n","_________________________________________________________________\n"," Layer (type)                Output Shape              Param #   \n","=================================================================\n"," embedding_6 (Embedding)     (None, 1398, 300)         1889100   \n","                                                                 \n"," conv1d_8 (Conv1D)           (None, 1394, 128)         192128    \n","                                                                 \n"," dropout_4 (Dropout)         (None, 1394, 128)         0         \n","                                                                 \n"," max_pooling1d_8 (MaxPooling  (None, 697, 128)         0         \n"," 1D)                                                             \n","                                                                 \n"," conv1d_9 (Conv1D)           (None, 693, 128)          82048     \n","                                                                 \n"," max_pooling1d_9 (MaxPooling  (None, 346, 128)         0         \n"," 1D)                                                             \n","                                                                 \n"," bidirectional_6 (Bidirectio  (None, 64)               41216     \n"," nal)                                                            \n","                                                                 \n"," dense_6 (Dense)             (None, 1)                 65        \n","                                                                 \n","=================================================================\n","Total params: 2,204,557\n","Trainable params: 315,457\n","Non-trainable params: 1,889,100\n","_________________________________________________________________\n","None\n","Epoch 1/10\n","72/72 [==============================] - 13s 119ms/step - loss: 0.5577 - accuracy: 0.7513 - val_loss: 0.5504 - val_accuracy: 0.7571\n","Epoch 2/10\n","72/72 [==============================] - 8s 108ms/step - loss: 0.5387 - accuracy: 0.7568 - val_loss: 0.5498 - val_accuracy: 0.7571\n","Epoch 3/10\n","72/72 [==============================] - 8s 109ms/step - loss: 0.5195 - accuracy: 0.7577 - val_loss: 0.5510 - val_accuracy: 0.7553\n","Epoch 4/10\n","72/72 [==============================] - 8s 110ms/step - loss: 0.4711 - accuracy: 0.7805 - val_loss: 0.5799 - val_accuracy: 0.7479\n","Epoch 5/10\n","72/72 [==============================] - 8s 112ms/step - loss: 0.3778 - accuracy: 0.8308 - val_loss: 0.6290 - val_accuracy: 0.7227\n","Epoch 6/10\n","72/72 [==============================] - 8s 108ms/step - loss: 0.2761 - accuracy: 0.8877 - val_loss: 0.7444 - val_accuracy: 0.7153\n","Epoch 7/10\n","72/72 [==============================] - 8s 108ms/step - loss: 0.2206 - accuracy: 0.9110 - val_loss: 0.8000 - val_accuracy: 0.6893\n","Epoch 8/10\n","72/72 [==============================] - 8s 110ms/step - loss: 0.1755 - accuracy: 0.9295 - val_loss: 0.8844 - val_accuracy: 0.6671\n","Epoch 9/10\n","72/72 [==============================] - 8s 109ms/step - loss: 0.1624 - accuracy: 0.9350 - val_loss: 0.9248 - val_accuracy: 0.6762\n","Epoch 10/10\n","72/72 [==============================] - 8s 115ms/step - loss: 0.1337 - accuracy: 0.9481 - val_loss: 1.0130 - val_accuracy: 0.7062\n","72/72 [==============================] - 2s 14ms/step\n","Accuracy: 0.7062146892655368\n","Precision: 0.2857142857142857\n","Recall: 0.13953488372093023\n","F1-Score: 0.18749999999999997\n"]}]},{"cell_type":"markdown","source":["# Word2Vec"],"metadata":{"id":"h3rzst1Vljek"}},{"cell_type":"code","source":["import numpy as np\n","import pandas as pd\n","from tensorflow.keras.preprocessing.text import Tokenizer\n","from tensorflow.keras.preprocessing.sequence import pad_sequences\n","from keras.layers import Embedding, Conv1D, MaxPooling1D, Bidirectional, LSTM, Dense, Dropout\n","from keras.models import Sequential\n","from sklearn.model_selection import train_test_split\n","from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n","\n","\n","# Load the data from CSV file\n","df = pd.read_csv('/content/drive/MyDrive/New/Clean/LIAR_clean.csv')\n","\n","MAX_NB_WORDS = 50000  # Maximum number of words to be used in the tokenizer\n","VALIDATION_SPLIT = 0.2\n","\n","# Split data into statements and labels\n","statements = df['statement'].astype(str)\n","labels = df['label']\n","\n","# Tokenize statements\n","tokenizer = Tokenizer()\n","tokenizer.fit_on_texts(statements)\n","word_index = tokenizer.word_index\n","sequences = tokenizer.texts_to_sequences(statements)\n","\n","# Pad sequences\n","Max_Len = max([len(x) for x in statements])\n","padded_sequences = pad_sequences(sequences, maxlen=Max_Len)\n","\n","print('Shape of data tensor:', padded_sequences.shape)\n","\n","# Define the maximum number of words to keep in the vocabulary\n","MAX_NUM_WORDS = 20000\n","\n","# Convert the labels to one-hot encoding\n","y = df['label']\n","print('Shape of label tensor:', y.shape)\n","\n","# Split data into training and testing sets\n","X_train, X_test, y_train, y_test = train_test_split(padded_sequences, df['label'].values, test_size=0.2)\n","\n","# Load the Word2Vec embeddings\n","from gensim.models.word2vec import Word2Vec\n","import gensim.downloader as api\n","\n","# Load the pre-trained Word2Vec model\n","w2v_model = api.load('word2vec-google-news-300')\n","\n","# Create an embedding matrix\n","embedding_dim = 300\n","word_index = tokenizer.word_index\n","num_words = min(MAX_NUM_WORDS, len(word_index) + 1)\n","embedding_matrix = np.zeros((num_words, embedding_dim))\n","for word, i in word_index.items():\n","    if i >= MAX_NUM_WORDS:\n","        continue\n","    if word in w2v_model:\n","        embedding_vector = w2v_model[word]\n","        embedding_matrix[i] = embedding_vector\n","\n","# Build the model\n","model = Sequential()\n","model.add(Embedding(input_dim=num_words, output_dim=embedding_dim, input_length=Max_Len, weights=[embedding_matrix], trainable=False))\n","model.add(Conv1D(128, 5, activation='relu'))\n","model.add(Dropout(0.3))\n","model.add(MaxPooling1D(2))\n","model.add(Conv1D(128, 5, activation='relu'))\n","model.add(MaxPooling1D(2))\n","model.add(Bidirectional(LSTM(32)))\n","\n","model.add(Dense(1, activation='sigmoid'))\n","\n","# Compile the model\n","model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n","print(model.summary())\n","\n","# Train the model\n","history = model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=10, batch_size=128)\n","\n","# Evaluate the model\n","y_pred = model.predict(X_test)\n","y_pred = (y_pred > 0.5).astype(int)\n","\n","# Calculate the evaluation metrics\n","accuracy = accuracy_score(y_test, y_pred)\n","precision = precision_score(y_test, y_pred)\n","recall = recall_score(y_test, y_pred)\n","f1 = f1_score(y_test, y_pred)\n","\n","print(\"Accuracy:\", accuracy)\n","print(\"Precision:\", precision)\n","print(\"Recall:\", recall)\n","print(\"F1-Score:\", f1)\n","\n","# Save the model\n","model.save('/content/drive/MyDrive/Colab Notebooks/weights/LIAR/word2vec_fnd.h5')\n","\n","# Load the saved model\n","# loaded_model = tf.keras.models.load_model('lstm_fn.h5')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"PY5CiUXzly92","executionInfo":{"status":"ok","timestamp":1680283679415,"user_tz":-330,"elapsed":486574,"user":{"displayName":"Ramacharan Reddy Kasireddy","userId":"17080325997413282440"}},"outputId":"46e81cbb-6a99-402e-e2a6-34cc166610a7"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Shape of data tensor: (11505, 1398)\n","Shape of label tensor: (11505,)\n","[==================================================] 100.0% 1662.8/1662.8MB downloaded\n","Model: \"sequential_7\"\n","_________________________________________________________________\n"," Layer (type)                Output Shape              Param #   \n","=================================================================\n"," embedding_7 (Embedding)     (None, 1398, 300)         1889100   \n","                                                                 \n"," conv1d_10 (Conv1D)          (None, 1394, 128)         192128    \n","                                                                 \n"," dropout_5 (Dropout)         (None, 1394, 128)         0         \n","                                                                 \n"," max_pooling1d_10 (MaxPoolin  (None, 697, 128)         0         \n"," g1D)                                                            \n","                                                                 \n"," conv1d_11 (Conv1D)          (None, 693, 128)          82048     \n","                                                                 \n"," max_pooling1d_11 (MaxPoolin  (None, 346, 128)         0         \n"," g1D)                                                            \n","                                                                 \n"," bidirectional_7 (Bidirectio  (None, 64)               41216     \n"," nal)                                                            \n","                                                                 \n"," dense_7 (Dense)             (None, 1)                 65        \n","                                                                 \n","=================================================================\n","Total params: 2,204,557\n","Trainable params: 315,457\n","Non-trainable params: 1,889,100\n","_________________________________________________________________\n","None\n","Epoch 1/10\n","72/72 [==============================] - 12s 118ms/step - loss: 0.5663 - accuracy: 0.7515 - val_loss: 0.5632 - val_accuracy: 0.7514\n","Epoch 2/10\n","72/72 [==============================] - 8s 107ms/step - loss: 0.5444 - accuracy: 0.7583 - val_loss: 0.5635 - val_accuracy: 0.7514\n","Epoch 3/10\n","72/72 [==============================] - 8s 108ms/step - loss: 0.5291 - accuracy: 0.7581 - val_loss: 0.5503 - val_accuracy: 0.7514\n","Epoch 4/10\n","72/72 [==============================] - 8s 109ms/step - loss: 0.4896 - accuracy: 0.7692 - val_loss: 0.5988 - val_accuracy: 0.7492\n","Epoch 5/10\n","72/72 [==============================] - 8s 110ms/step - loss: 0.3950 - accuracy: 0.8215 - val_loss: 0.6497 - val_accuracy: 0.7358\n","Epoch 6/10\n","72/72 [==============================] - 8s 109ms/step - loss: 0.2840 - accuracy: 0.8848 - val_loss: 0.7368 - val_accuracy: 0.6897\n","Epoch 7/10\n","72/72 [==============================] - 8s 108ms/step - loss: 0.2136 - accuracy: 0.9167 - val_loss: 0.8381 - val_accuracy: 0.7014\n","Epoch 8/10\n","72/72 [==============================] - 8s 107ms/step - loss: 0.1722 - accuracy: 0.9313 - val_loss: 0.9093 - val_accuracy: 0.6606\n","Epoch 9/10\n","72/72 [==============================] - 8s 107ms/step - loss: 0.1453 - accuracy: 0.9421 - val_loss: 1.0331 - val_accuracy: 0.7032\n","Epoch 10/10\n","72/72 [==============================] - 8s 107ms/step - loss: 0.1286 - accuracy: 0.9487 - val_loss: 1.0629 - val_accuracy: 0.6806\n","72/72 [==============================] - 2s 14ms/step\n","Accuracy: 0.6805736636245111\n","Precision: 0.31685393258426964\n","Recall: 0.2465034965034965\n","F1-Score: 0.27728613569321536\n"]}]},{"cell_type":"markdown","source":["# Fast Text"],"metadata":{"id":"Ztj3gVVXTxUD"}},{"cell_type":"code","source":["import numpy as np\n","import pandas as pd\n","from tensorflow.keras.preprocessing.text import Tokenizer\n","from tensorflow.keras.preprocessing.sequence import pad_sequences\n","from keras.layers import Embedding, Conv1D, MaxPooling1D, Bidirectional, LSTM, Dense, Dropout\n","from keras.models import Sequential\n","from sklearn.model_selection import train_test_split\n","from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n","\n","# Load the data from CSV file\n","df = pd.read_csv('/content/drive/MyDrive/New/Clean/LIAR_clean.csv')\n","\n","MAX_NB_WORDS = 50000  # Maximum number of words to be used in the tokenizer\n","VALIDATION_SPLIT = 0.2\n","\n","# Split data into statements and labels\n","statements = df['statement'].astype(str)\n","labels = df['label']\n","\n","# Tokenize statements\n","tokenizer = Tokenizer()\n","tokenizer.fit_on_texts(statements)\n","word_index = tokenizer.word_index\n","sequences = tokenizer.texts_to_sequences(statements)\n","\n","# Pad sequences\n","Max_Len = max([len(x) for x in statements])\n","padded_sequences = pad_sequences(sequences, maxlen=Max_Len)\n","\n","print('Shape of data tensor:', padded_sequences.shape)\n","\n","# Define the maximum number of words to keep in the vocabulary\n","MAX_NUM_WORDS = 20000\n","\n","# Convert the labels to one-hot encoding\n","y = df['label']\n","print('Shape of label tensor:', y.shape)\n","\n","# Split data into training and testing sets\n","X_train, X_test, y_train, y_test = train_test_split(padded_sequences, df['label'].values, test_size=0.2)\n","\n","# Load the Word2Vec embeddings\n","from gensim.models.word2vec import Word2Vec\n","import gensim.downloader as api\n","\n","# Load the pre-trained Word2Vec model\n","ft_model = api.load('fasttext-wiki-news-subwords-300')\n","\n","# Create an embedding matrix\n","embedding_dim = 300\n","word_index = tokenizer.word_index\n","num_words = min(MAX_NUM_WORDS, len(word_index) + 1)\n","embedding_matrix = np.zeros((num_words, embedding_dim))\n","for word, i in word_index.items():\n","    if i >= MAX_NUM_WORDS:\n","        continue\n","    if word in ft_model:\n","        embedding_vector = ft_model[word]\n","        embedding_matrix[i] = embedding_vector\n","\n","# Build the model\n","model = Sequential()\n","model.add(Embedding(input_dim=num_words, output_dim=embedding_dim, input_length=Max_Len, weights=[embedding_matrix], trainable=False))\n","model.add(Conv1D(128, 5, activation='relu'))\n","model.add(Dropout(0.3))\n","model.add(MaxPooling1D(2))\n","model.add(Conv1D(128, 5, activation='relu'))\n","model.add(MaxPooling1D(2))\n","model.add(Bidirectional(LSTM(32)))\n","model.add(Dense(1, activation='sigmoid'))\n","\n","# Compile the model\n","model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n","print(model.summary())\n","\n","# Train the model\n","history = model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=10, batch_size=128)\n","\n","# Evaluate the model\n","y_pred = model.predict(X_test)\n","y_pred = (y_pred > 0.5).astype(int)\n","\n","# Calculate the evaluation metrics\n","accuracy = accuracy_score(y_test, y_pred)\n","precision = precision_score(y_test, y_pred)\n","recall = recall_score(y_test, y_pred)\n","f1 = f1_score(y_test, y_pred)\n","\n","print(\"Accuracy:\", accuracy)\n","print(\"Precision:\", precision)\n","print(\"Recall:\", recall)\n","print(\"F1-Score:\", f1)\n","\n","# Save the model\n","model.save('/content/drive/MyDrive/Colab Notebooks/weights/LIAR/ft_LIAR.h5')\n","\n","# Load the saved model\n","# loaded_model = tf.keras.models.load_model('lstm_fn.h5')\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"bzxuJQ3GKFyD","executionInfo":{"status":"ok","timestamp":1680284133549,"user_tz":-330,"elapsed":454138,"user":{"displayName":"Ramacharan Reddy Kasireddy","userId":"17080325997413282440"}},"outputId":"ba98dab8-a102-45f5-f7a6-7078945443fc"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Shape of data tensor: (11505, 1398)\n","Shape of label tensor: (11505,)\n","[==================================================] 100.0% 958.5/958.4MB downloaded\n","Model: \"sequential_8\"\n","_________________________________________________________________\n"," Layer (type)                Output Shape              Param #   \n","=================================================================\n"," embedding_8 (Embedding)     (None, 1398, 300)         1889100   \n","                                                                 \n"," conv1d_12 (Conv1D)          (None, 1394, 128)         192128    \n","                                                                 \n"," dropout_6 (Dropout)         (None, 1394, 128)         0         \n","                                                                 \n"," max_pooling1d_12 (MaxPoolin  (None, 697, 128)         0         \n"," g1D)                                                            \n","                                                                 \n"," conv1d_13 (Conv1D)          (None, 693, 128)          82048     \n","                                                                 \n"," max_pooling1d_13 (MaxPoolin  (None, 346, 128)         0         \n"," g1D)                                                            \n","                                                                 \n"," bidirectional_8 (Bidirectio  (None, 64)               41216     \n"," nal)                                                            \n","                                                                 \n"," dense_8 (Dense)             (None, 1)                 65        \n","                                                                 \n","=================================================================\n","Total params: 2,204,557\n","Trainable params: 315,457\n","Non-trainable params: 1,889,100\n","_________________________________________________________________\n","None\n","Epoch 1/10\n","72/72 [==============================] - 13s 119ms/step - loss: 0.5639 - accuracy: 0.7586 - val_loss: 0.5632 - val_accuracy: 0.7492\n","Epoch 2/10\n","72/72 [==============================] - 8s 108ms/step - loss: 0.5498 - accuracy: 0.7588 - val_loss: 0.5536 - val_accuracy: 0.7492\n","Epoch 3/10\n","72/72 [==============================] - 8s 109ms/step - loss: 0.5430 - accuracy: 0.7588 - val_loss: 0.5538 - val_accuracy: 0.7492\n","Epoch 4/10\n","72/72 [==============================] - 8s 109ms/step - loss: 0.5332 - accuracy: 0.7588 - val_loss: 0.5543 - val_accuracy: 0.7492\n","Epoch 5/10\n","72/72 [==============================] - 8s 110ms/step - loss: 0.5158 - accuracy: 0.7590 - val_loss: 0.5636 - val_accuracy: 0.7419\n","Epoch 6/10\n","72/72 [==============================] - 8s 109ms/step - loss: 0.4817 - accuracy: 0.7744 - val_loss: 0.5763 - val_accuracy: 0.7279\n","Epoch 7/10\n","72/72 [==============================] - 8s 108ms/step - loss: 0.4277 - accuracy: 0.8047 - val_loss: 0.6365 - val_accuracy: 0.7297\n","Epoch 8/10\n","72/72 [==============================] - 8s 108ms/step - loss: 0.3624 - accuracy: 0.8397 - val_loss: 0.6812 - val_accuracy: 0.7036\n","Epoch 9/10\n","72/72 [==============================] - 8s 107ms/step - loss: 0.2973 - accuracy: 0.8747 - val_loss: 0.7290 - val_accuracy: 0.6827\n","Epoch 10/10\n","72/72 [==============================] - 8s 107ms/step - loss: 0.2379 - accuracy: 0.9058 - val_loss: 0.8333 - val_accuracy: 0.6906\n","72/72 [==============================] - 2s 15ms/step\n","Accuracy: 0.6905693176879617\n","Precision: 0.30985915492957744\n","Recall: 0.19064124783362218\n","F1-Score: 0.23605150214592274\n"]}]}],"metadata":{"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"},"vscode":{"interpreter":{"hash":"aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"}},"colab":{"provenance":[{"file_id":"195VpL2hxfaVbGUg5vKzJWOjcrZszboc6","timestamp":1680281787793}],"collapsed_sections":["PmF1iUhDEey3"],"machine_shape":"hm"},"gpuClass":"standard","accelerator":"GPU"},"nbformat":4,"nbformat_minor":0}